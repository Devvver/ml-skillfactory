{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGeEoDsuMkRW"
   },
   "source": [
    "## Рекуррентные нейронные сети. Генерация текстов. \n",
    "В этом домашнем задании вам предстоит применить на практике знания о рекуррентых сетях (в нашем случае - Long-Short Term Memory Network) в генеративном контексте. Это значит, что мы будем создавать модель, способную генерировать текст. \n",
    "Для обучения мы будем использовать роман «Преступление и наказание», который, благодаря своему немалому размеру, хорошо подойдет для обучения. \n",
    "Так же, это довольно интересно, насколько хорошо модель сможет заговорить языком великого писателя :)\n",
    "\n",
    "Стоит заметить, что для обучения нейронных сетей требуются хорошие вычислительные мощности. Если вдруг вы сомневаетесь в своей GPU, то стоит обратить внимание на сервис Colaboratory (colab.research.google.com)  от Google. Этот сервис предоставляет мощную GPU на целых 12 часов бесплатно, при этом, если перезапускать ноутбук, то лимит на 12 часов каждый раз будет обновляться. Так же удобен тем, что не нужно никаких дополнительных настроек - просто загружаете свой ноутбук и начинаете работу. Во время разработки домашнего задания мы пользовались Colaboratory - и остались довольны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBjJEzQK3lcQ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HvPxdVosMQVc",
    "outputId": "723df1dd-7830-4615-c7bc-0bcdd437cb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество символов в корпусе: 1116083\n"
     ]
    }
   ],
   "source": [
    "with io.open('book.txt', encoding='utf-8') as f:\n",
    "    text = f.read().lower().replace('\\xa0', ' ')\n",
    "print('количество символов в корпусе:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqBEZmFYFlbQ"
   },
   "source": [
    "Посмотрим на первые символы нашего корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "iNdEotbQ3vmq",
    "outputId": "3af90942-6125-4262-ef57-20cd368dfc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'роман ф. м. достоевского «преступление и наказание» принадлежит к числу тех великих произведений мировой литературы, ценность которых со временем не умаляется, но возрастает для каждого следующего пок'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__X9TZQVFgOQ"
   },
   "source": [
    "### Maximum Likelihood Character Level Language Model\n",
    "Прежде чем перейти к нейронным сетям, рассмотрим более простую генеративную языковую модель, которую чаще всего называют  Maximum Likelihood Character Level Language Model. \n",
    "\n",
    "Ее идея предельно проста: считаем условные вероятности для каждой буквы на обучающем множестве. Например: для буквы \"д\" считаем сколько раз встретились другие буквы после \"д\" в нашем датасете. Считаем сумму частот, делим на нее, частоту каждой буквы, получаем некоторое приближение вероятности. \n",
    "\n",
    "Реализуем данный подход!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9ywhn0hFfsQ"
   },
   "outputs": [],
   "source": [
    "class SimpleMaximumLikelihoodModel():\n",
    "    def __init__(self, order=1):\n",
    "        self.order = order\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, text):\n",
    "        buffer = defaultdict(Counter)\n",
    "        for i in range(len(text) - self.order):\n",
    "            history, char = text[i:i + self.order], text[i + self.order]\n",
    "            buffer[history][char] += 1\n",
    "        self.model = {hist: self.normalize(chars) for hist, chars in buffer.items()}        \n",
    "        \n",
    "    def predict(self, symbol):\n",
    "        if model is not None:\n",
    "            return self.model[symbol]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "    def normalize(self, counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c, cnt / s) for c, cnt in counter.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euJGB5htBqmQ"
   },
   "source": [
    "Построим модель 1-го порядка, т.е. считающую вероятность последующей буквы для каждой буквы. Модель 2-го порядка будет считать вероятность следующей буквы для пары букв, и так далее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvqKwo0OGg7g"
   },
   "outputs": [],
   "source": [
    "order = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKI-uD_KFfmk"
   },
   "outputs": [],
   "source": [
    "model = SimpleMaximumLikelihoodModel(order=order)\n",
    "model.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x52mmB6PCOCd"
   },
   "source": [
    "Посмотрим, какой наиболее вероятный символ после буквы \"а\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "JE6pauA_FfTH",
    "outputId": "c5ef5e70-2daa-4b44-e270-12bbcb2a0b1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('н', 0.05035312581742087),\n",
       " ('к', 0.08249774755137036),\n",
       " ('з', 0.051820850408347136),\n",
       " ('д', 0.02716017089545732),\n",
       " ('т', 0.0636643706222571),\n",
       " ('л', 0.10385967971633679),\n",
       " ('с', 0.06952073705931933),\n",
       " ('е', 0.019138547388612783),\n",
       " ('ж', 0.02374516813439126),\n",
       " ('м', 0.05054204086377772),\n",
       " ('у', 0.000988171011712733),\n",
       " ('р', 0.02621559566367309),\n",
       " ('в', 0.04099456505943558),\n",
       " (' ', 0.1907315371872003),\n",
       " ('.', 0.012570116546051675),\n",
       " ('х', 0.013543755631121574),\n",
       " ('п', 0.01153834975441044),\n",
       " (')', 0.0005376812857848693),\n",
       " ('б', 0.007367686807916994),\n",
       " (',', 0.03509460284244485),\n",
       " ('й', 0.013311244804836225),\n",
       " ('ч', 0.013718138750835586),\n",
       " ('я', 0.02915104484552562),\n",
       " ('и', 0.0017292992704972825),\n",
       " ('щ', 0.0037201732205655825),\n",
       " ('ф', 0.000755660185427384),\n",
       " ('ю', 0.013369372511407563),\n",
       " ('»', 0.0009736390850698986),\n",
       " ('ш', 0.013078733978550876),\n",
       " ('г', 0.009896242043770162),\n",
       " ('ц', 0.000755660185427384),\n",
       " ('…', 0.002979044961781033),\n",
       " (':', 0.0010753625715697387),\n",
       " (';', 0.0017874269770686198),\n",
       " ('о', 0.000188915046356846),\n",
       " ('-', 0.0039817479001366),\n",
       " ('?', 0.00293544918185253),\n",
       " ('!', 0.004533961112564304),\n",
       " ('\\n', 1.4531926642834307e-05),\n",
       " ('[', 2.9063853285668613e-05),\n",
       " ('ё', 1.4531926642834307e-05),\n",
       " ('“', 2.9063853285668613e-05),\n",
       " ('э', 5.812770657133723e-05),\n",
       " ('а', 2.9063853285668613e-05)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('а')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ifj6sE8SCxLp"
   },
   "source": [
    "Попробуем что-нибудь сгенерировать с помощью модели первого порядка. \n",
    "\n",
    "Важный момент: при каждом запуске ячейки будет генерироваться новый набор символов, потому что мы не просто выбираем для каждой буквы последующую с максимальной вероятностью, а создаем вероятностное распределение, из которого генерируем новую букву.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0oUUuo79C6D3",
    "outputId": "92b47c9b-a231-4d62-f6c6-d165ad601210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "делов сяко рупрослстнакоришло? н в т азла льконакадаза, дорут чтахоська … брамем, вегок эть, омобя ра ро инарагам тето ны содово. вня вск. я визосвлой, жудези итех — за в.\n",
      "ан диаедралейлю, зупралено обедут\n"
     ]
    }
   ],
   "source": [
    "# количество букв в start_symbol должно быть больше или равно order\n",
    "start_symbol = 'делов'\n",
    "n_iter = 200\n",
    "generated_string = start_symbol\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    predictions = model.predict(generated_string[-order:])\n",
    "    probabilities = np.random.multinomial(1, [x[1] for x in predictions], 1)\n",
    "    sampled_letter = predictions[np.argmax(probabilities)][0]\n",
    "    generated_string += sampled_letter\n",
    "    \n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lNfyBjrJGpOC"
   },
   "source": [
    "Видно, что если строить модель по условной вероятности для одной буквы, то качество получается не очень хорошим. Попробуйте увеличить порядок (например до 5 или 7), и посмотрите, как изменится качество генерируемого текста. Модель уже способна генерировать осмыленные слова, и даже улавливать некоторую структуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPoEzUBHGrwY"
   },
   "source": [
    "***Задание №1:*** какой символ имеет наибольшую вероятность для строки \"здоро\", если обучить модель порядка 5?\n",
    "В EdX отправьте символ в нижнем регистре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHgHUAvOFfH5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('в', 1.0)]\n",
      "здоровы…\n",
      "\n",
      "— не имея, так чтоб опасался он к свидеть образом составить, — показать близких и так, приобретается как-нибудь; приду, ну, сажая его глаз, ведь этой злобная и, так и вчера, вчера я, может быть о\n"
     ]
    }
   ],
   "source": [
    "# ваше решение\n",
    "order = 5\n",
    "model = SimpleMaximumLikelihoodModel(order=order)\n",
    "model.fit(text)\n",
    "print(model.predict('здоро'))\n",
    "\n",
    "# генерируем текст\n",
    "start_symbol = 'здоро'\n",
    "n_iter = 200\n",
    "generated_string = start_symbol\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    predictions = model.predict(generated_string[-order:])\n",
    "    probabilities = np.random.multinomial(1, [x[1] for x in predictions], 1)\n",
    "    sampled_letter = predictions[np.argmax(probabilities)][0]\n",
    "    generated_string += sampled_letter\n",
    "    \n",
    "print(generated_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMCNGfqaCV5i"
   },
   "source": [
    "После того, как вы ознакомились с более простой языковой моделью, которая, как вы заметили, работает не так уж плохо, время переходить к нейронным сетям. Начнем с важного шага - предобработки данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSm1BD3FcpVF"
   },
   "source": [
    "#### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P5jLz6jY3xZE",
    "outputId": "364d5bb5-5244-4a1d-a909-e1ebb0d5014d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество уникальных символов в тексте: 95\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('количество уникальных символов в тексте:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnnkPn7g32pF"
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MH7uTCfqbOZm"
   },
   "source": [
    "Сформируем выборку для обучения модели. В качестве Х мы будем использовать часть текста (т.е. набор символов), ограниченную длиной max_length, а в качестве Y - следующий символ. Затем закодируем каждый символ символ в Х с помощью бинарного вектора. \n",
    "\n",
    "step - это шаг, с которым мы идем по нашему корпусу, разбивая его на элементы обучающей выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ij5nyZAl34ZG",
    "outputId": "fb0b82e2-0e55-4cfd-b2ff-14a10097b36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество последовательностей: 223203\n"
     ]
    }
   ],
   "source": [
    "max_length = 70\n",
    "step = 5\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - max_length, step):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "    \n",
    "print('количество последовательностей:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sd4BDiqSb7Es",
    "outputId": "ddde31ff-fb46-4ccb-edbd-1685fa78cfa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пример одного элемента обучающей выборки (X и y):\n",
      " орые полагали, что страдания и нищета неизбежны во всяком обществе, чт о\n"
     ]
    }
   ],
   "source": [
    "print('пример одного элемента обучающей выборки (X и y):\\n', sentences[700], next_chars[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1ij0y-mLqFM"
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), max_length, len(chars)), dtype=np.int8)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.int8)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_5gR2BGcghI"
   },
   "source": [
    "#### Архитектура модели, выбор алгоритма оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkdC8s8nc4Qb"
   },
   "source": [
    "Создаем многослойную рекуррентную сеть с двумя LSTM-слоями. Обратите внимание на входную размерность (input_shape): на вход мы принимаем кусок текста, длина которого равна max_length, каждый элемент которой имеет размерность len(chars) - это наш бинарный вектор, говорящий о том, какая это буква. \n",
    "\n",
    "Далее добавляем полносвязный слой (Dense), которой в этом случае будет выходом сети, с размерностью len(chars), так как мы хотим предсказать следующий символ, а так же с функцией активации Softmax - которая преобразует вектор так, что сумма всех его элементов равна 1 и имеет вероятностную интерпретацию.  \n",
    "\n",
    "**Задание**: Добавьте второй LSTM-слой со 128 скрытыми параметрами. Добавьте Dropout слой с вероятностью отключения нейрона 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkK0qn6F4FQy"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(max_length, len(chars))))\n",
    "model.add(LSTM(128)) # ваш код\n",
    "model.add(Dropout(0.5)) # ваш код\n",
    "model.add(Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_OtqmvJfGHc"
   },
   "source": [
    "В качестве алгоритма подбора параметров модели выберем Adam ([Документация](https://keras.io/optimizers/), [Оригинал статьи](https://arxiv.org/abs/1412.6980v8)), немного изменив его стандартные параметры, а в качестве функции ошибки возьмем многоклассовую энтропию, т.к. предсказываем мы следующую букву, и должны делать это как можно точнее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdfgRUpD4LTZ"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.99, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjfUUkk7cGsI"
   },
   "source": [
    "#### Реализуем вспомогательные функции для генерации текста с помощью модели\n",
    "Ниже представлен код функций, которые упрощают генерацию текста с помощью модели. Генерировать слова и символы можно с помощью нескольких основных стратегий:\n",
    "1. Greedy - выбираем символ с максимальной вероятностью из выхода сети (softmax)\n",
    "2. Sampling - генерируем символ из вероятностного распределения на выходе сети (softmax)\n",
    "3. Beam Search - более сложный алгоритм, учитывающий условные вероятности, позволяющий выбрать наиболее вероятную последовательность символов. \n",
    "\n",
    "В данном случае мы будем использовать стратегию №2, сэмплируя символ из распределения, которое получается на выходе из слоя Softmax. Результат Softmax'a можно интерпретировать вероятностно, т.к. после применения этой функции к вектору все его компоненты суммируются в единицу. Получив результат Softmax, мы создаем мультиномиальное распределение, из которого и генерируем символ. Важное значение имеет параметр temperature - чем он больше, тем более равномерным становится наше распределение. \n",
    "Вы можете увидеть, как меняются сгенерированные предложения с изменением этого параметра. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZ3VDD9k4Mzy"
   },
   "outputs": [],
   "source": [
    "def sample(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Функция для генерация слова из распределения; аргумент temperature позволяет\n",
    "    сглаживать распределение. Чем его значение больше, тем более сглаженным\n",
    "    получается распределение над символами. \n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    \"\"\"\n",
    "    Данная функция вызывается на каждой эпохе обучения сети. Ее задача - показать\n",
    "    результаты модели. \n",
    "    \"\"\"\n",
    "    print()\n",
    "    print('Генерируем текст после эпохи #%d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - max_length - 1)\n",
    "\n",
    "    sentence = text[start_index: start_index + max_length]\n",
    "    print('Предложение, на основании которого мы генерируем: ', sentence)\n",
    "\n",
    "    for temperature in [0.05, 0.2, 0.5]:\n",
    "        print('temperature = ', temperature)\n",
    "\n",
    "        sentence = text[start_index: start_index + max_length]\n",
    "        generated = sentence\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(200):\n",
    "            x_pred = np.zeros((1, max_length, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            predictions = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(predictions, temperature)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Peq7uctM_5K"
   },
   "source": [
    "Аргумент callbacks принимает на вход функции, которые срабатывают в конце каждой эпохи. В нашем случае таких функций две:\n",
    "\n",
    "**LambdaCallback**, который вызывает on_epoch_end, позволяющий генерировать нам предложение на каждой эпохе и наблюдать за поведением модели. \n",
    "\n",
    "**ModelCheckpoint** - сохраняет копию модели после каждой итерации на жесткий диск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "CFr_5TlmXYx-",
    "outputId": "45884892-12eb-4f01-c70d-ca4cc1db5005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223104/223203 [============================>.] - ETA: 0s - loss: 2.1809 - acc: 0.3600\n",
      "Генерируем текст после эпохи #0\n",
      "Предложение, на основании которого мы генерируем:  ая, неуклюжая, робкая и смиренная девка, чуть не идиотка, тридцати пят\n",
      "temperature =  0.05\n",
      "ая, неуклюжая, робкая и смиренная девка, чуть не идиотка, тридцати пять не соня и водел в соненно в соненно в соненно в столоников подомонить и не подомонить и всё подомал он подоловал он не соня стороников подомонить и подомонить и подомонить и всем не подомить и всё п\n",
      "temperature =  0.2\n",
      "ая, неуклюжая, робкая и смиренная девка, чуть не идиотка, тридцати пять не вого в соворять на столоников в соненно в соленить с продом столоников подомой подомил он подоление стал и подном подомонов сонение и всё сомнать и всё поделать полодать и водал на домоно не подо\n",
      "temperature =  0.5\n",
      "ая, неуклюжая, робкая и смиренная девка, чуть не идиотка, тридцати пять и ов раду колько не не семенной только на день говор! с навал даме вавуе и вакал и подрумить и согоровну, и вы мисать не что стало, — заменовить комание было в него что долоников он про поворринался\n",
      "223203/223203 [==============================] - 840s 4ms/sample - loss: 2.1809 - acc: 0.3601\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x, y,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    callbacks=[\n",
    "        LambdaCallback(on_epoch_end=on_epoch_end),\n",
    "        ModelCheckpoint('2layers-weights-{epoch:02d}-{loss:.3f}.hdf5', monitor='loss')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxvSgn3q4slV"
   },
   "source": [
    "#### Применение LSTM для генерации символов в тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6p0lwRF6h0D"
   },
   "source": [
    "Мы подготовили для вас набор небольших предложений, в котором необходимо предсказать следующий символ для каждого предложения. Храниться этот набор в файле test_X.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTqPtq0F9rkk"
   },
   "outputs": [],
   "source": [
    "with open('test_X.pickle', 'rb') as f:\n",
    "    test_X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73nr1tk16wtT"
   },
   "source": [
    "Код для предсказания следующего символа для каждого предложения с помощью вашей модели. Попробуйте варьировать параметр temperature, и то, как изменяется результат от него. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMn7Z4ox9rd_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "temperature = 0.5\n",
    "\n",
    "for test_x in test_X:\n",
    "    test_x_matrix = np.zeros((1, len(test_x), len(chars)))\n",
    "    for t, char in enumerate(test_x):\n",
    "        test_x_matrix[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    predictions = model.predict(test_x_matrix, verbose=0)[0]\n",
    "    next_index = sample(predictions, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "    answers.append(next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfxlXOfx6b7I"
   },
   "source": [
    "**Задание №2:** Данную строку необходимо отправить в качестве ответа в EdX. Ваша задача - обучить модель так, чтобы метрика accuracy превысила 0.35. Чтобы получить полный балл - необходимо получить хотя бы 0.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT-tRlU-9rVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,д,ь,а,д,р,а,и,ж,п,о, ,и,о, ,о,т,с,о,р,ч,т,л,с,о,о,в,о,в,с,ч,о,б,е,е,о,т,о,в,с,н,и,о,—,а,л, ,с,т, ,г,ж,е,н,т,в,в,д,е,п,л,с,е, ,е,к,л,о,д,т,ы,о,а,о,е,а,и,а,и,л,а,т,о,м,п,т,е,ь,е,в,у,и,о,л,ь,п,с,ь,е,е\n"
     ]
    }
   ],
   "source": [
    "answer_string = ','.join(answers)\n",
    "print(answer_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88a_p2ihl9cx"
   },
   "source": [
    "## Генерация предложений с помощью обученной модели\n",
    "В этом разделе мы загрузим предобученную модель для генерации текста, и посмотрим, как она работает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKIkpjMkmW0k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kalan/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/kalan/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(max_length, len(chars))))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0kKUXiGmZyB"
   },
   "outputs": [],
   "source": [
    "model.load_weights('checkpoint.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE0ViBiOmf2L"
   },
   "source": [
    "Вы можете изменять переменную sentence, и наблюдать, какой текст получается на выходе. Заметим, что модель, обученная на уровне символов, научилась генерировать целые осмысленные слова, улавливая долгосрочные взаимосвязи. \n",
    "\n",
    "Можете попробовать подвигать значение температуры, и посмотреть, как меняются в зависмости от нее результаты модели. Так же можно менять исходное предложение, но важно помнить, что его длина должна быть равна max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ZB18BVFd4cpG",
    "outputId": "318cd8d2-151e-49c5-ac51-763410c888f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "этот факт был тщательно расследован и довольно хорошо засвидетельствовала. тол"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ько не смешал и скорее его поверение, который дом. выходите, по праве. подостовали его полезывать к тему дело оч"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "\n",
    "sentence = 'этот факт был тщательно расследован и довольно хорошо засвидетельствов'\n",
    "generated = sentence\n",
    "sys.stdout.write(generated)\n",
    "\n",
    "assert len(sentence) == max_length, 'Длина sentence должна быть равна max_length'\n",
    "\n",
    "for i in range(120):\n",
    "    x_pred = np.zeros((1, max_length, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    predictions = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(predictions, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEZEgB81iYim"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grQl6zunwHol"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPW7-QW3gkAb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm-text-generation-solution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
