{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark DataFrame API \n",
    "Метод работы со spark, похожий на привычный нам pandas. За исключением того, что все происходит на кластере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с api нам понадобится переменная **spark** которая содержит метаинформацию о подключении к spark. Она чем-то похожа на знакомую нам переменную **sc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных из csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"crimes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Latitude\", \"Longitude\"]].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[['Arrest', 'Primary Type', 'Longitude', 'Latitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразуем строковые значения в числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df_short.withColumn('LongitudeFloat', df_short[\"Longitude\"].cast(\"float\"))\n",
    "df_short = df_short.withColumn('LatitudeFloat', df_short[\"Latitude\"].cast(\"float\"))\n",
    "df_short = df_short.withColumn('ArrestInt', df_short[\"Arrest\"].cast(\"boolean\").cast(\"int\"))\n",
    "df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ArrestInt', 'Primary Type', 'LongitudeFloat', 'LatitudeFloat']\n",
    "df_short = df_short[features]\n",
    "df_short = df_short.withColumnRenamed('LongitudeFloat', 'Longitude')\\\n",
    "                   .withColumnRenamed('LatitudeFloat', 'Latitude')\\\n",
    "                   .withColumnRenamed('ArrestInt', 'Arrest')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Есть много функций для агрегации данных, например mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df_short.select(mean(df_short[\"Latitude\"])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно зарегестрировать таблицу как SQL и работать с ней: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short.createOrReplaceTempView(\"crimes_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"select * from crimes_table where Latitude > 41.89139 and Latitude < 41.89140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+----------+---------+\n",
      "|Arrest|       Primary Type| Longitude| Latitude|\n",
      "+------+-------------------+----------+---------+\n",
      "|     0|            BATTERY|-87.744385|  41.8914|\n",
      "|     0|      OTHER OFFENSE|-87.744316|  41.8914|\n",
      "|     0|           BURGLARY| -87.74448|  41.8914|\n",
      "|     0|           BURGLARY|-87.669586|41.891396|\n",
      "|     0|              THEFT| -87.74429|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74448|  41.8914|\n",
      "|     0|            BATTERY| -87.74448|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74443|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74443|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74448|  41.8914|\n",
      "|     0|              THEFT| -87.66228| 41.89139|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74443|  41.8914|\n",
      "|     0|           BURGLARY| -87.74448|  41.8914|\n",
      "|     0|            BATTERY| -87.74429|  41.8914|\n",
      "|     0|MOTOR VEHICLE THEFT| -87.74425|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74443|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.67123|  41.8914|\n",
      "|     0|            ASSAULT| -87.74429|  41.8914|\n",
      "|     0|    CRIMINAL DAMAGE| -87.74448|  41.8914|\n",
      "|     0|              THEFT|-87.669586|41.891396|\n",
      "+------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_indexer = StringIndexer(inputCol=\"Primary Type\", outputCol=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = type_indexer.fit(df_short).transform(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------+---------+----+\n",
      "|Arrest| Primary Type| Longitude| Latitude|type|\n",
      "+------+-------------+----------+---------+----+\n",
      "|     0|      BATTERY|-87.744385|  41.8914| 1.0|\n",
      "|     1|OTHER OFFENSE| -87.66532|41.773373| 5.0|\n",
      "|     0|      BATTERY| -87.59664| 41.81386| 1.0|\n",
      "|     0|      BATTERY| -87.62262|41.800804| 1.0|\n",
      "+------+-------------+----------+---------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df_short[\"Arrest\", \"Longitude\", \"Latitude\", \"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+----+\n",
      "|Arrest| Longitude| Latitude|type|\n",
      "+------+----------+---------+----+\n",
      "|     0|-87.744385|  41.8914| 1.0|\n",
      "|     1| -87.66532|41.773373| 5.0|\n",
      "|     0| -87.59664| 41.81386| 1.0|\n",
      "|     0| -87.62262|41.800804| 1.0|\n",
      "+------+----------+---------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoderEstimator(inputCols=['type'], outputCols=['type_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder.fit(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = model.transform(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+----+--------------+\n",
      "|Arrest| Longitude| Latitude|type|      type_vec|\n",
      "+------+----------+---------+----+--------------+\n",
      "|     0|-87.744385|  41.8914| 1.0|(33,[1],[1.0])|\n",
      "|     1| -87.66532|41.773373| 5.0|(33,[5],[1.0])|\n",
      "|     0| -87.59664| 41.81386| 1.0|(33,[1],[1.0])|\n",
      "|     0| -87.62262|41.800804| 1.0|(33,[1],[1.0])|\n",
      "|     0|-87.743355|41.878063| 9.0|(33,[9],[1.0])|\n",
      "+------+----------+---------+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['Longitude', 'Latitude', 'type_vec'], outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "df_short = assembler.transform(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+----+--------------+--------------------+\n",
      "|Arrest| Longitude| Latitude|type|      type_vec|            features|\n",
      "+------+----------+---------+----+--------------+--------------------+\n",
      "|     0|-87.744385|  41.8914| 1.0|(33,[1],[1.0])|(35,[0,1,3],[-87....|\n",
      "|     1| -87.66532|41.773373| 5.0|(33,[5],[1.0])|(35,[0,1,7],[-87....|\n",
      "|     0| -87.59664| 41.81386| 1.0|(33,[1],[1.0])|(35,[0,1,3],[-87....|\n",
      "|     0| -87.62262|41.800804| 1.0|(33,[1],[1.0])|(35,[0,1,3],[-87....|\n",
      "|     0|-87.743355|41.878063| 9.0|(33,[9],[1.0])|(35,[0,1,11],[-87...|\n",
      "+------+----------+---------+----+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Arrest|            features|\n",
      "+------+--------------------+\n",
      "|     0|(35,[0,1,3],[-87....|\n",
      "|     1|(35,[0,1,7],[-87....|\n",
      "|     0|(35,[0,1,3],[-87....|\n",
      "|     0|(35,[0,1,3],[-87....|\n",
      "+------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_short = df_short[[\"Arrest\", \"features\"]]\n",
    "df_short.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML на больших данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = df_short.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Arrest|            features|\n",
      "+------+--------------------+\n",
      "|     0|(35,[0,1,2],[-87....|\n",
      "|     0|(35,[0,1,2],[-87....|\n",
      "|     0|(35,[0,1,2],[-87....|\n",
      "|     0|(35,[0,1,2],[-87....|\n",
      "|     0|(35,[0,1,2],[-87....|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Arrest\", featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|Arrest|            features|       rawPrediction|         probability|prediction|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "|     0|(35,[0,1,2],[-87....|[8.29124610884311...|[0.82912461088431...|       0.0|\n",
      "|     0|(35,[0,1,2],[-87....|[8.29124610884311...|[0.82912461088431...|       0.0|\n",
      "|     0|(35,[0,1,2],[-87....|[8.29124610884311...|[0.82912461088431...|       0.0|\n",
      "|     0|(35,[0,1,2],[-87....|[8.29124610884311...|[0.82912461088431...|       0.0|\n",
      "|     0|(35,[0,1,2],[-87....|[8.29124610884311...|[0.82912461088431...|       0.0|\n",
      "+------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7959037153469779"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"Arrest\")\n",
    "evaluator.setMetricName(\"areaUnderROC\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
