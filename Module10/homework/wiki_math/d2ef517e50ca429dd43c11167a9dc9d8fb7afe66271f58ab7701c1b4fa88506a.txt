Алгоритм Гаусса — Ньютона  Приближение кривой со случайным шумом асимметричной моделью пика используя алгоритм Гаусса — Ньютона с переменным коэффициентом затухания α.  Сверху: необработанные данные и модель. Снизу: ход нормализованной суммы квадратов ошибок. Алгоритм Гаусса — Ньютона используется для решения задач нелинейным методом наименьших квадратов[en]. Алгоритм  является модификацией метода Ньютона для нахождения минимума функции. В отличие от метода Ньютона, алгоритм Гаусса — Ньютона может быть использован только для минимизации суммы квадратов, но его преимущество в том, что метод не требует вычисления вторых производных, что может оказаться существенной трудностью. Задачи, для которых применяется нелинейный метод наименьших квадратов, возникают, например, при нелинейной регрессии, в которой ищутся параметры модели, которые наиболее соответствуют наблюдаемым величинам. Метод назван именами математиков Карла Фридриха Гаусса и Исаака Ньютона.  Содержание  1 Описание 2 Замечания 3 Пример 4 Сходимость 5 Алгоритм на основе метода Ньютона 6 Улучшенные версии 7 Оптимизация больших задач 8 Связанные алгоритмы 9 Примечания 10 Литература 11 Ссылки  11.1 Реализации     Описание[править | править код] Если задано m функций r = (r1, …, rm) (часто называемых невязками) от  n переменных β = (β1, …, βn), при m ≥ n. Алгоритм Гаусса — Ньютона итеративно находит значения переменных, которые минимизируют сумму квадратов[1]      S (  β  ) =  ∑  i = 1   m    r  i   2   (  β  ) .   {\displaystyle S({\boldsymbol {\beta }})=\sum _{i=1}^{m}r_{i}^{2}({\boldsymbol {\beta }}).}   Начав с некоторого начального приближения       β   ( 0 )     {\displaystyle {\boldsymbol {\beta }}^{(0)}}  , метод осуществляет итерации        β   ( s + 1 )   =   β   ( s )   −   (     J  r      T      J  r     )   − 1      J  r      T     r  (   β   ( s )   )   {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\left(\mathbf {J_{r}} ^{\mathsf {T}}\mathbf {J_{r}} \right)^{-1}\mathbf {J_{r}} ^{\mathsf {T}}\mathbf {r} ({\boldsymbol {\beta }}^{(s)})}   Здесь, если рассматривать r и β как вектор-столбцы, элементы матрицы Якоби равны      (   J  r     )  i j   =    ∂  r  i   (   β   ( s )   )   ∂  β  j        {\displaystyle (\mathbf {J_{r}} )_{ij}={\frac {\partial r_{i}({\boldsymbol {\beta }}^{(s)})}{\partial \beta _{j}}}}   а символ         T      {\displaystyle ^{\mathsf {T}}}   означает транспонирование матрицы. Если m = n, итерации упрощаются до        β   ( s + 1 )   =   β   ( s )   −   (   J  r    )   − 1    r  (   β   ( s )   )   {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\left(\mathbf {J_{r}} \right)^{-1}\mathbf {r} ({\boldsymbol {\beta }}^{(s)})}  , что является прямым обобщением одномерного метода Ньютона. При аппроксимации данных, где целью является поиск параметров β, таких, что заданная модель функций y = f(x, β) наилучшим образом приближает точки данных (xi, yi), функции ri являются остаточными ошибками[en]       r  i   (  β  ) =  y  i   − f (  x  i   ,  β  ) .   {\displaystyle r_{i}({\boldsymbol {\beta }})=y_{i}-f(x_{i},{\boldsymbol {\beta }}).}   Тогда метод Гаусса — Ньютона можно выразить в терминах якобиана Jf функции f        β   ( s + 1 )   =   β   ( s )   −   (     J  f      T      J  f     )   − 1      J  f      T     r  (   β   ( s )   ) .   {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\left(\mathbf {J_{f}} ^{\mathsf {T}}\mathbf {J_{f}} \right)^{-1}\mathbf {J_{f}} ^{\mathsf {T}}\mathbf {r} ({\boldsymbol {\beta }}^{(s)}).}   Заметим, что       (     J  f      T      J  f     )   − 1      J  f      T      {\displaystyle \left(\mathbf {J_{f}} ^{\mathsf {T}}\mathbf {J_{f}} \right)^{-1}\mathbf {J_{f}} ^{\mathsf {T}}}   является псевдообратной матрицей к       J  f      {\displaystyle \mathbf {J_{f}} }  .  Замечания[править | править код] Требование m ≥ n в алгоритме необходимо, поскольку в другом случае матрица JrTJr не имеет обратной и нормальные уравнения нельзя решить (по меньшей мере однозначно). Алгоритм Гаусса — Ньютона можно получить с помощью линейного приближения вектора функций ri. Используя теорему Тейлора, мы можем для каждой итерации записать:       r  (  β  ) ≈  r  (   β   s   ) +   J  r    (   β   s   ) Δ   {\displaystyle \mathbf {r} ({\boldsymbol {\beta }})\approx \mathbf {r} ({\boldsymbol {\beta }}^{s})+\mathbf {J_{r}} ({\boldsymbol {\beta }}^{s})\Delta }  , где     Δ =  β  −   β   s     {\displaystyle \Delta ={\boldsymbol {\beta }}-{\boldsymbol {\beta }}^{s}}  . Задача нахождения Δ, минимизирующего сумму квадратов в правой части, т.е.       m i n  ‖  r  (   β   s   ) +   J  r    (   β   s   ) Δ  ‖  2   2     {\displaystyle \mathbf {min} \|\mathbf {r} ({\boldsymbol {\beta }}^{s})+\mathbf {J_{r}} ({\boldsymbol {\beta }}^{s})\Delta \|_{2}^{2}}  , является линейной задачей нахождения наименьших квадратов, которую можно решить явно, что даёт нормальные уравнения. Нормальные уравнения — это m линейных уравнений по неизвестным приращениям Δ. Уравнения могут быть решены за один шаг, если использовать разложение Холецкого, или, лучше, QR-разложение матрицы Jr. Для больших систем итеративный метод может оказаться более эффективным, если используются такие методы, как метод сопряжённых градиентов. Если имеется линейная зависимость столбцов матрицы Jr, метод итераций завершается неудачей, поскольку JrTJr становится вырожденной.  Пример[править | править код]  Вычисленная кривая с         β ^     1   = 0.362   {\displaystyle {\hat {\beta }}_{1}=0.362}   и         β ^     2   = 0.556   {\displaystyle {\hat {\beta }}_{2}=0.556}   (синим цветом) и наблюдаемые данные (красным цветом). В этом примере используется алгоритм Гаусса — Ньютона для построения модели данных путём минимизации суммы квадратов отклонений данных и модели. В экспериментальной биологии изучение связей между концентрацией субстрата [S] и скоростью реакции в реакции энзимомодуляции, были получены следующие данные.     i  1 2 3 4 5 6 7   [S]  0.038 0.194 0.425 0.626 1.253 2.500 3.740   скорость  0.050 0.127 0.094 0.2122 0.2729 0.2665 0.3317  Нужно найти кривую (функцию-модель) вида  скорость     =     V  max   [ S ]    K  M   + [ S ]      {\displaystyle ={\frac {V_{\text{max}}[S]}{K_{M}+[S]}}}  , которая приближает наилучшим образом данные в смысле наименьших квадратов с параметрами      V  max     {\displaystyle V_{\text{max}}}   и      K  M     {\displaystyle K_{M}}  , которые следует найти. Обозначим через      x  i     {\displaystyle x_{i}}   и      y  i     {\displaystyle y_{i}}   значения [S] и скорость из таблицы,     i = 1 , … , 7   {\displaystyle i=1,\dots ,7}  . Пусть      β  1   =  V  max     {\displaystyle \beta _{1}=V_{\text{max}}}   и      β  2   =  K  M     {\displaystyle \beta _{2}=K_{M}}  . Мы будем искать      β  1     {\displaystyle \beta _{1}}   и      β  2     {\displaystyle \beta _{2}}  , такие, что сумма квадратов отклонений       r  i   =  y  i   −     β  1    x  i      β  2   +  x  i       ( i = 1 , … , 7 )   {\displaystyle r_{i}=y_{i}-{\frac {\beta _{1}x_{i}}{\beta _{2}+x_{i}}}\;(i=1,\dots ,7)}   минимальна. Якобиан       J  r      {\displaystyle \mathbf {J_{r}} }   вектора остатков      r  i     {\displaystyle r_{i}}   по неизвестным      β  j     {\displaystyle \beta _{j}}   — это     7 × 2   {\displaystyle 7\times 2}   матрица с     i   {\displaystyle i}  -ой строкой, имеющей элементы         ∂  r  i     ∂  β  1      = −    x  i     β  2   +  x  i      ,      ∂  r  i     ∂  β  2      =     β  1    x  i      (   β  2   +  x  i    )   2     .   {\displaystyle {\frac {\partial r_{i}}{\partial \beta _{1}}}=-{\frac {x_{i}}{\beta _{2}+x_{i}}},\ {\frac {\partial r_{i}}{\partial \beta _{2}}}={\frac {\beta _{1}x_{i}}{\left(\beta _{2}+x_{i}\right)^{2}}}.}   Начав с начального приближения      β  1   = 0.9   {\displaystyle \beta _{1}=0.9}   и      β  2   = 0.2   {\displaystyle \beta _{2}=0.2}   после пяти итераций алгоритм Гаусса — Ньютона даёт оптимальные значения         β ^     1   = 0.362   {\displaystyle {\hat {\beta }}_{1}=0.362}   и         β ^     2   = 0.556   {\displaystyle {\hat {\beta }}_{2}=0.556}  . Сумма квадратов остатков уменьшается от начального значения 1.445 до 0.00784 к пятой итерации. График справа показывает кривую с оптимальными параметрами.  Сходимость[править | править код] Можно показать[2], что направление увеличения Δ является направлением спуска[en] для S, и, если алгоритм сходится, пределом будет стационарная точка для S. Однако сходимость не гарантируется, даже когда начальная точка близка к решению[en], что происходит в методе Ньютона[en] или методе BFGS при обычных условиях Фольфе[3]. Скорость сходимости алгоритма Гаусса — Ньютона близка к квадратичной[4]. Алгоритм может сходиться медленнее или не сходиться совсем, если начальное приближение далеко от минимального или если матрица       J  r    T     J  r      {\displaystyle \mathbf {J_{r}^{\mathsf {T}}J_{r}} }   плохо обусловлена.  Например, представим задачу с     m = 2   {\displaystyle m=2}   уравнениями и     n = 1   {\displaystyle n=1}   переменной           r  1   ( β )    = β + 1      r  2   ( β )    = λ  β  2   + β − 1.       {\displaystyle {\begin{aligned}r_{1}(\beta )&=\beta +1\\r_{2}(\beta )&=\lambda \beta ^{2}+\beta -1.\end{aligned}}}   Полученное оптимальное решение —     β = 0   {\displaystyle \beta =0}  . (Настоящий оптимум —     β = − 1   {\displaystyle \beta =-1}   для     λ = 2   {\displaystyle \lambda =2}  , поскольку     S ( 0 ) =  1  2   + ( − 1  )  2   = 2   {\displaystyle S(0)=1^{2}+(-1)^{2}=2}  , в то время как     S ( − 1 ) = 0   {\displaystyle S(-1)=0}  .) Если     λ = 0   {\displaystyle \lambda =0}  , то задача, фактически, линейна и метод находит решение за одну итерацию. Если |λ| < 1, то метод сходится линейно и ошибка убывает со скоростью |λ| на каждой итерации. Однако, если |λ| > 1, то метод не сходится даже локально[5].  Алгоритм на основе метода Ньютона[править | править код] Далее предполагается, что алгоритм Гаусса — Ньютона основан на методе Ньютона[en] для минимизации функции с помощью аппроксимации. Как следствие, скорость сходимости алгоритма Гаусса — Ньютона может быть квадратична, если выполнены некоторые условия. В общем случае (при более слабых условиях), скорость сходимости может оказаться линейной[6]. Рекуррентное отношение метода Ньютона для минимизации функции S от параметров       β    {\displaystyle {\boldsymbol {\beta }}}          β   ( s + 1 )   =   β   ( s )   −   H   − 1    g     {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}-\mathbf {H} ^{-1}\mathbf {g} \,}   где g означает вектор-градиент функции S, а H обозначает гессиан функции S. Поскольку     S =  ∑  i = 1   m    r  i   2     {\displaystyle S=\sum _{i=1}^{m}r_{i}^{2}}  , градиент задаётся равенством       g  j   = 2  ∑  i = 1   m    r  i      ∂  r  i     ∂  β  j      .   {\displaystyle g_{j}=2\sum _{i=1}^{m}r_{i}{\frac {\partial r_{i}}{\partial \beta _{j}}}.}   Элементы гессиана вычисляются путём дифференцирования элементов градиента      g  j     {\displaystyle g_{j}}   по      β  k     {\displaystyle \beta _{k}}         H  j k   = 2  ∑  i = 1   m    (     ∂  r  i     ∂  β  j         ∂  r  i     ∂  β  k      +  r  i       ∂  2    r  i     ∂  β  j   ∂  β  k       )  .   {\displaystyle H_{jk}=2\sum _{i=1}^{m}\left({\frac {\partial r_{i}}{\partial \beta _{j}}}{\frac {\partial r_{i}}{\partial \beta _{k}}}+r_{i}{\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}\right).}   Метод Гаусса — Ньютона получается путём отбрасывания второй производной (второго члена в выражении). То есть гессиан аппроксимируется        H  j k   ≈ 2  ∑  i = 1   m    J  i j    J  i k     {\displaystyle H_{jk}\approx 2\sum _{i=1}^{m}J_{ij}J_{ik}}  , где      J  i j   =    ∂  r  i     ∂  β  j        {\displaystyle J_{ij}={\frac {\partial r_{i}}{\partial \beta _{j}}}}   — элементы якобиана Jr. Градиент и приближённый гессиан можно записать в матричной нотации       g  = 2   J    r     T     r  ,   H  ≈ 2   J    r     T      J  r    .    {\displaystyle \mathbf {g} =2\mathbf {J} _{\mathbf {r} }^{\mathsf {T}}\mathbf {r} ,\quad \mathbf {H} \approx 2\mathbf {J} _{\mathbf {r} }^{\mathsf {T}}\mathbf {J_{r}} .\,}   Эти выражения подставляются в рекуррентное отношение выше для получения операционных уравнений        β   ( s + 1 )   =   β   ( s )   + Δ ;  Δ = −   (     J  r      T      J  r     )   − 1      J  r      T     r  .   {\displaystyle {\boldsymbol {\beta }}^{(s+1)}={\boldsymbol {\beta }}^{(s)}+\Delta ;\quad \Delta =-\left(\mathbf {J_{r}} ^{\mathsf {T}}\mathbf {J_{r}} \right)^{-1}\mathbf {J_{r}} ^{\mathsf {T}}\mathbf {r} .}   Сходимость метода Гаусса — Ньютона в общем случае не гарантирована. Аппроксимация       |   r  i       ∂  2    r  i     ∂  β  j   ∂  β  k       |  ≪  |     ∂  r  i     ∂  β  j         ∂  r  i     ∂  β  k       |    {\displaystyle \left|r_{i}{\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}\right|\ll \left|{\frac {\partial r_{i}}{\partial \beta _{j}}}{\frac {\partial r_{i}}{\partial \beta _{k}}}\right|}  , которая должна выполняться для возможности отбрасывания членов со второй производной, может быть получена в двух случаях, для которых сходимость ожидается[7]  Значения функции      r  i     {\displaystyle r_{i}}   малы по величине, по меньшей мере, рядом с минимумом. Функции лишь "слегка" нелинейны, то есть         ∂  2    r  i     ∂  β  j   ∂  β  k        {\displaystyle {\frac {\partial ^{2}r_{i}}{\partial \beta _{j}\partial \beta _{k}}}}   относительно малы по величине. Улучшенные версии[править | править код] В методах Гаусса — Ньютона сумма квадратов остатков S может не уменьшаться на каждой итерации. Однако, поскольку Δ направлен в сторону уменьшения функции, если     S (   β   s   )   {\displaystyle S({\boldsymbol {\beta }}^{s})}   не является стационарной точкой, выполняется неравенство     S (   β   s   + α Δ ) < S (   β   s   )   {\displaystyle S({\boldsymbol {\beta }}^{s}+\alpha \Delta )<S({\boldsymbol {\beta }}^{s})}   для достаточно малых     α > 0   {\displaystyle \alpha >0}  . Таким образом, если обнаруживается расходимость, можно использовать долю     α   {\displaystyle \alpha }   вектора приращения Δ в формуле обновления:           β   s + 1   =   β   s   + α   Δ   {\displaystyle {\boldsymbol {\beta }}^{s+1}={\boldsymbol {\beta }}^{s}+\alpha \ \Delta }  . Другими словами, вектор приращения слишком длинный, но он указывает направление «спуска», так что если пройти лишь часть пути, можно уменьшить значение функции S. Оптимальное значение     α   {\displaystyle \alpha }   может быть найдено с помощью алгоритма линейного поиска, то есть величина     α   {\displaystyle \alpha }   определяется путём нахождения значения, минимизирующего S с помощью прямого поиска на интервале     0 < α < 1   {\displaystyle 0<\alpha <1}  . В случаях, когда в направлении вектора приращения оптимальная доля     α   {\displaystyle \alpha }   близка к нулю, альтернативным методом отработки расходимости является использование алгоритма Левенберга — Марквардта, известного также как «метод доверительных областей»[en][1]. Нормальные уравнения, модифицированные таким образом, что вектор спуска поворачивается в направлении наискорейшего спуска,           (   J  T   J + λ D  )  Δ = −   J   T    r    {\displaystyle \left(\mathbf {J^{T}J+\lambda D} \right)\Delta =-\mathbf {J} ^{T}\mathbf {r} }  , где D — положительная диагональная матрица. Заметим, что если D является единичной матрицей E и     λ → + ∞   {\displaystyle \lambda \to +\infty }  , то     λ Δ = λ   (    J  E   J  + λ  E   )   − 1    (  −   J   T    r   )  =  (   E  −   J  T   J   /  λ + ⋯  )   (  −   J   T    r   )  → −   J   T    r    {\displaystyle \lambda \Delta =\lambda \left(\mathbf {J^{E}J} +\lambda \mathbf {E} \right)^{-1}\left(-\mathbf {J} ^{T}\mathbf {r} \right)=\left(\mathbf {E} -\mathbf {J^{T}J} /\lambda +\cdots \right)\left(-\mathbf {J} ^{T}\mathbf {r} \right)\to -\mathbf {J} ^{T}\mathbf {r} }  . Таким образом, направление Δ приближает направление отрицательного градиента     −   J   T    r    {\displaystyle -\mathbf {J} ^{T}\mathbf {r} }  . Так называемый параметр Марквардта     λ   {\displaystyle \lambda }   может также быть оптимизирован путём линейного поиска, но смысла особого нет, поскольку вектор сдвига нужно каждый раз пересчитывать, когда меняется     λ   {\displaystyle \lambda }  . Более эффективная стратегия такая. Если обнаруживается расхождение, увеличиваем параметр Марквардта, пока S убывает. Затем сохраняем значение между итерациями, но уменьшаем его, если возможно, пока не достигнем значения, когда параметр Марквардта не может быть обнулён. Минимизация S тогда становится стандартной минимизацией Гаусса — Ньютона.  Оптимизация больших задач[править | править код] Для оптимизации большого размера метод Гаусса — Ньютона особенно интересен, поскольку часто (хотя, определённо, не всегда) матрица       J    r      {\displaystyle \mathbf {J} _{\mathbf {r} }}   более разрежена, чем приближённый гессиан       J    r     T      J  r      {\displaystyle \mathbf {J} _{\mathbf {r} }^{\mathsf {T}}\mathbf {J_{r}} }  . В таких случаях шаг вычисления сам по себе, обычно, требует применения итеративного приближённого метода, такого как метод сопряжённых градиентов. Чтобы этот подход работал, необходим как минимум эффективный метод вычисления произведения        J    r     T      J  r     p    {\displaystyle \mathbf {J} _{\mathbf {r} }^{\mathsf {T}}\mathbf {J_{r}} \mathbf {p} }   для некоторого вектора p. Для хранения разреженной матрицы практично хранить строки матрицы       J    r      {\displaystyle \mathbf {J} _{\mathbf {r} }}   в сжатом виде (т.е. без нулевых элементов), что делает прямое вычисление приведённого выше произведения (ввиду транспозиции) сложным. Однако, если определить ci как строку i матрицы       J    r      {\displaystyle \mathbf {J} _{\mathbf {r} }}  , выполняется следующее отношение:        J    r     T      J  r     p  =  ∑  i     c   i   (   c   i   ⋅  p  )   {\displaystyle \mathbf {J} _{\mathbf {r} }^{\mathsf {T}}\mathbf {J_{r}} \mathbf {p} =\sum _{i}\mathbf {c} _{i}(\mathbf {c} _{i}\cdot \mathbf {p} )}   так что любая строка делает вклад в произведение аддитивно и независимо. Кроме того, это выражение хорошо изучено для применения параллельных вычислений. Заметим, что любая строка ci является градиентом соответствующей невязки ri. При учёте этого обстоятельства вышеприведённая формула подчёркивает факт, что невязки вносят вклад в результат независимо друг от друга.  Связанные алгоритмы[править | править код] В квазиньютоновских методах, таких как методы Дэвидона, Флетчера и Пауэлла[en] или Бройдена — Флетчера — Голдфарба — Шэнно (метод БФГШ) приближение полного гессиана         ∂  2   S   ∂  β  j   ∂  β  k        {\displaystyle {\frac {\partial ^{2}S}{\partial \beta _{j}\partial \beta _{k}}}}   строится с помощью первых производных        ∂  r  i     ∂  β  j        {\displaystyle {\frac {\partial r_{i}}{\partial \beta _{j}}}}   так, что после n уточнений метод по производительности близок к методу Ньютона. Заметим, что квазиньютоновские методы могут минимизировать вещественные функции общего вида, в то время как методы Гаусса — Ньютона, Левенберга — Марквардта  и т.д. применимы только к нелинейным задачам наименьших квадратов. Другой метод решения задач минимизации, использующий только первые производные, это метод градиентного спуска. Однако этот метод не принимает во внимание вторые производные, даже приблизительные. Как следствие, метод крайне малоэффективен для многих функций, особенно в случае сильного взаимного влияния параметров.  Примечания[править | править код]   ↑ 1 2 Björck, 1996.  ↑ Björck, 1996, с. 260.  ↑ Mascarenhas, 2013, с. 253–276.  ↑ Björck, 1996, с. 341, 342.  ↑ Fletcher, 1987, с. 113.  ↑ Gratton, Lawless, Nichols.  ↑ Nocedal, Wright, 1999, с. 259-262.   Литература[править | править код]  A. Björck. Numerical methods for least squares problems. — Philadelphia: SIAM, 1996. — ISBN 0-89871-360-9. Roger Fletcher. Practical methods of optimization. — 2nd. — New York: John Wiley & Sons, 1987. — ISBN 978-0-471-91547-8. Walter F. Mascarenhas. The divergence of the BFGS and Gauss Newton Methods // Mathematical Programming. — 2013. — Т. 147, вып. 1. — DOI:10.1007/s10107-013-0720-6. S. Gratton, A.S. Lawless, N.K. Nichols. Approximate Gauss-Newton methods for nonlinear least squares problems. NUMERICAL ANALYSIS REPORT 9/04 (англ.) (недоступная ссылка — история).  The University of Reading (January 2007). Проверено 20 июля 2017. Архивировано 4 августа 2016 года. Jorge  Nocedal, Stephen J. Wright. Numerical optimization / Peter Glynn, Stephen M. Robinson. — New York: Springer, 1999. — (Springer Series in Operations Research). — ISBN 0-387-98793-2.  Ссылки[править | править код] Реализации[править | править код] Artelys Knitro. Система для решения нелинейных задач с импленментацией метода Гаусса — Ньютона. Система написана на C и имеет интерфейсы для C++/C#/Java/Python/MATLAB/R. Методы оптимизацииОдномерные Метод золотого сечения Дихотомия Метод парабол Перебор по сетке Метод равномерного блочного поиска Метод Фибоначчи Троичный поиск Метод Пиявского Метод Стронгина Прямые методы Метод Гаусса Метод Нелдера — Мида  Метод Хука — Дживса Метод конфигураций Метод Розенброка Первого порядка Градиентный спуск Метод Зойтендейка Покоординатный спуск Метод сопряжённых градиентов Квазиньютоновские методы Алгоритм Левенберга — Марквардта Второго порядка Метод Ньютона Метод Ньютона — Рафсона Алгоритм Бройдена — Флетчера — Гольдфарба — Шанно (BFGS) Стохастические Метод Монте-Карло Имитация отжига Эволюционные алгоритмы Дифференциальная эволюция Муравьиный алгоритм Метод роя частиц Алгоритм пчелиной колонии Метод случайных блужданий Методы линейногопрограммирования Симплекс-метод Алгоритм Гомори Метод эллипсоидов Метод потенциалов Методы нелинейногопрограммирования Последовательное квадратичное программирование  Для улучшения этой статьи желательно: Проверить качество перевода с иностранного языка.Исправить статью согласно стилистическим правилам Википедии.    