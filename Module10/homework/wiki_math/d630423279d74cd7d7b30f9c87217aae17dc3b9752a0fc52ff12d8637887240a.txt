Байесовский вывод Байесовский вывод — статистический вывод, в котором свидетельство и/или наблюдение используются, чтобы обновить или вновь вывести вероятность того, что гипотеза может быть верной; название байесовский происходит от частого использования в процессе вывода теоремы Байеса, которая была выведена из работ преподобного Томаса Байеса [1].  Содержание  1 Свидетельство и изменение веры 2 Простые примеры байесовского вывода  2.1 Из какой вазы печенье?   3 Литература 4 Примечания   Свидетельство и изменение веры[править | править код] Байесовский вывод использует аспекты научного метода, который вовлекает сбор свидетельств, предназначенных для того, чтобы поддерживать или не поддерживать данную гипотезу. Поскольку свидетельства накапливаются, степень веры в гипотезу должна измениться. С достаточным количеством свидетельств, она должна стать либо очень высокой, либо очень низкой. Таким образом, сторонники байесовского вывода говорят, что он может использоваться, чтобы провести различие между противоречивыми гипотезами: гипотезы с очень высокой поддержкой должны быть приняты как истинные, а с очень низкой поддержкой должны быть отклонены как ложные. Однако, противники говорят, что этот метод вывода может привести к отклонению благодаря исходному верованию, которого каждый придерживается до того, когда какое-либо свидетельство будет собрано (это — форма так называемого индуктивного отклонения(англ. bias)).[1] Байесовский вывод использует числовую оценку степени веры в гипотезу до получения свидетельства, чтобы вычислить числовую оценку степени веры в гипотезу после того, как свидетельство было получено (этот процесс повторяется, когда получено дополнительное свидетельство). В индукционном процессе байесовский вывод обычно опирается на степени веры, или субъективные вероятности, и не обязательно утверждает, что обеспечен объективный метод индукции. Тем не менее, некоторые байесовские статистики полагают, что вероятности могут иметь объективное значение, и поэтому байесовский вывод может обеспечить объективный метод индукции (см. научный метод).[1]  Теорема Байеса подправляет вероятность гипотезы, данную новым свидетельством, следующим образом:      P ( H  |  E ) =    P ( E  |  H )  P ( H )   P ( E )    ,   {\displaystyle P(H|E)={\frac {P(E|H)\;P(H)}{P(E)}},}   где      H   {\displaystyle H}   представляет конкретную гипотезу, которая может быть, а может и не быть некоторой нулевой гипотезой.     P ( H )   {\displaystyle P(H)}   называется априорной вероятностью     H   {\displaystyle H}  ,  которая была выведена прежде, чем новое свидетельство     E   {\displaystyle E}   стало доступным.     P ( E  |  H )   {\displaystyle P(E|H)}   называется условной вероятностью  наблюдения свидетельства     E   {\displaystyle E}  , если гипотеза     H   {\displaystyle H}   оказывается верной; её также называют функцией правдоподобия, когда она рассматривается как функция     H   {\displaystyle H}   для фиксированного     E   {\displaystyle E}  .     P ( E )   {\displaystyle P(E)}   называется маргинальной вероятностью     E   {\displaystyle E}  : априорная вероятность наблюдения нового свидетельства     E   {\displaystyle E}   согласно всем возможным гипотезам; может быть вычислено по формуле полной вероятности:     P ( E ) = ∑ P ( E  |   H  i   ) P (  H  i   )   {\displaystyle P(E)=\sum P(E|H_{i})P(H_{i})}   — как сумма произведений всех вероятностей любого полного набора взаимно исключающих гипотез и соответствующих условных вероятностей.     P ( H  |  E )   {\displaystyle P(H|E)}   называется апостериорной вероятностью     H   {\displaystyle H}   для данного     E   {\displaystyle E}  .[1] Простые примеры байесовского вывода[править | править код] Из какой вазы печенье?[править | править код] Для иллюстрации предположим, что есть две полных вазы печенья. В 1-ой вазе 10 шоколадного и 30 простого печенья, в то время как во 2-ой вазе 20 каждого сорта. Наш друг Фред выбирает вазу наугад, и затем выбирает печенье наугад. Мы можем предположить, что нет никакой причины полагать, что Фред рассматривает одну вазу иначе другой, аналогично и для печенья. Печенье, выбранное Фредом, оказывается простым. Насколько вероятно, что Фред выбрал его из 1-ой вазы? Интуитивно, кажется ясным, что ответ должен быть больше половины, так как есть больше простого печенья в 1-ой вазе. Точный ответ дается теоремой Байеса. Пусть      H  1     {\displaystyle H_{1}}   — выбор вазы 1, а      H  2     {\displaystyle H_{2}}  — выбор вазы 2.  Предполагается, что вазы идентичны с точки зрения Фреда, таким образом     P (  H  1   ) = P (  H  2   )   {\displaystyle P(H_{1})=P(H_{2})}  , а вместе должны составить 1, таким образом обе равны 0.5. Событие     E   {\displaystyle E}   — наблюдение простого печенья. Из содержания ваз, мы знаем что     P ( E  |   H  1   ) = 30  /  40 = 0.75   {\displaystyle P(E|H_{1})=30/40=0.75}   и     P ( E  |   H  2   ) = 20  /  40 = 0.5   {\displaystyle P(E|H_{2})=20/40=0.5}  . Формула Байеса тогда даёт          P (  H  1    |  E )   =      P ( E  |   H  1   ) P (  H  1   )   P ( E  |   H  1   ) P (  H  1   ) + P ( E  |   H  2   ) P (  H  2   )            =      0.75 × 0.5   0.75 × 0.5 + 0.5 × 0.5            =   0.6.       {\displaystyle {\begin{matrix}P(H_{1}|E)&=&{\frac {P(E|H_{1})P(H_{1})}{P(E|H_{1})P(H_{1})+P(E|H_{2})P(H_{2})}}\\\\&=&{\frac {0.75\times 0.5}{0.75\times 0.5+0.5\times 0.5}}\\\\&=&0.6.\end{matrix}}}   До того, как мы наблюдали печенье, вероятность, которую мы назначили для Фреда, выбиравшего 1-ю вазу, была априорной вероятностью     P (  H  1   )   {\displaystyle P(H_{1})}  , равной 0.5. После наблюдения печенья, мы должны пересмотреть вероятность     P (  H  1    |  E )   {\displaystyle P(H_{1}|E)}  , которая теперь равна 0.6.[1]  Литература[править | править код] On-line textbook: Information Theory, Inference, and Learning Algorithms, by David MacKay, has chapters on Bayesian methods, including examples; arguments in favour of Bayesian methods (in the style of Edwin Jaynes); modern Monte Carlo methods, message-passing methods, and variational methods; and examples illustrating the connections between Bayesian inference and data compression. Berger, J.O. (1999) Statistical Decision Theory and Bayesian Analysis. Second Edition. Springer Verlag, New York. ISBN 0-387-96098-8 and also ISBN 3-540-96098-8. Bolstad, William M. (2004) Introduction to Bayesian Statistics, John Wiley ISBN 0-471-27020-2 Bretthorst, G. Larry, 1988, Bayesian Spectrum Analysis and Parameter Estimation in Lecture Notes in Statistics, 48, Springer-Verlag, New York, New York Carlin, B.P. and Louis, T.A. (2008) Bayesian Methods for Data Analysis, Third Edition. Chapman & Hall/CRC. [1] Dawid, A.P. and Mortera, J. (1996) Coherent analysis of forensic identification evidence. Journal of the Royal Statistical Society, Series B, 58,425-443. Foreman, L.A; Smith, A.F.M. and Evett, I.W. (1997). Bayesian analysis of deoxyribonucleic acid profiling data in forensic identification applications (with discussion). Journal of the Royal  Statistical Society, Series A, 160, 429-469. Gardner-Medwin, A. What probability should the jury address?. Significance. Volume 2, Issue 1, March 2005 Gelman, A., Carlin, J., Stern, H., and Rubin, D.B. (2003). Bayesian Data Analysis. Second Edition. Chapman & Hall/CRC, Boca Raton, Florida. [2] (недоступная ссылка) ISBN 1-58488-388-X. Gelman, A. and Meng, X.L. (2004). Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives: an essential journey with Donald Rubin's statistical family. John Wiley & Sons, Chichester, UK. ISBN 0-470-09043-X Giffin, A. and Caticha, A. (2007) Updating Probabilities with Data and Moments Jaynes, E.T. (1998) Probability Theory: The Logic of Science. Lee, Peter M. Bayesian Statistics: An Introduction. Second Edition. (1997). ISBN 0-340-67785-6. Loredo, Thomas J. (1992) "Promise of Bayesian Inference in Astrophysics" in Statistical Challenges in Modern Astronomy, ed. Feigelson & Babu. O'Hagan, A. and Forster, J. (2003) Kendall's Advanced Theory of Statistics, Volume 2B: Bayesian Inference. Arnold, New York. ISBN 0-340-52922-9. Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems, San Mateo, CA: Morgan Kaufmann. Robert, C.P. (2001) The Bayesian Choice. Springer Verlag, New York. Robertson, B. and Vignaux, G.A. (1995) Interpreting Evidence: Evaluating Forensic Science in the Courtroom. John Wiley and Sons. Chichester. Winkler, Robert L, Introduction to Bayesian Inference and Decision, 2nd Edition (2003) Probabilistic. ISBN 0-9647938-4-9 Scientific American essay on Bayesian inference and the probability of God's existence by Chris Wiggins. A nice on-line introductory tutorial to Bayesian probabilityfrom Queen Mary University of London An Intuitive Explanation of Bayesian Reasoning Bayes' Theorem for the curious and bewildered; an excruciatingly gentle introduction by Eliezer Yudkowsky Paul Graham. "A Plan for Spam" (exposition of a popular approach for spam classification)   Примечания[править | править код]   ↑ 1 2 3 4 5  Наука Вики, Байесовский вывод.      