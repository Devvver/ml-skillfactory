Анализ независимых компонент Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning     Анализ независимых компонент (АНК, англ. Independent Component Analysis, ICA) — это вычислительный метод в обработке сигналов для разделения многомерного[en] сигнала на аддитивные подкомпоненты. Это делается при предположении, что подкомпоненты являются негауссовыми сигналами и что они статистически независимы друг от друга. АНК является специальным случаем слепого разделения сигнала. Типичным примером приложения является «Задача о шумной вечеринке»[en], задаче получения разговора одного лица в шумном помещении.  Содержание  1 Введение 2 Определение независимости компонент 3 Математическое определение  3.1 Общее определение 3.2 Генерирующая модель  3.2.1 Линейная АНК без шума 3.2.2 Линейный АНК с шумом 3.2.3 Нелинейный АНК   3.3 Различимость   4 Бинарный анализ независимых компонент 5 Методы слепого разделения сигнала  5.1 Поиск наилучшей проекции 5.2 Основанный на infomax 5.3 На основе оценки максимального правдоподобия   6 История и предпосылки 7 Приложения 8 См. также 9 Примечания 10 Литература 11 Ссылки   Введение[править | править код] Воспроизвести медиафайл АНК на четырёх случайно смешанных видео[1] Анализ независимых компонент пытается разложить множественный сигнал на независимые негауссовы сигналы. Как пример, звук, обычно, является сигналом, который состоит из сложения в каждый момент t сигналов из нескольких источников. Вопрос тогда заключается в том, возможно ли выделить эти источники из общего сигнала. Если допущение статистической независимости верно, слепое разделение независимых компонент смешанного сигнала даёт очень хорошие результаты. Метод также применяется для анализа сигналов, которые не предполагаются смешанными. Простым приложением АНК является «задача о шумной вечеринке»[en], где сигналы разговоров отделяются из общего сигнала, состоящего из разговоров одновременно говорящих людей в помещении. Обычно задача упрощается предположением, что задержка по времени или эхо отсутствует. Заметим, что отфильтрованный и задержанный сигнал является копией зависимой компоненты, а тогда допущение статистической независимости не нарушено. Важно также учитывать, что если представлено     N   {\textstyle N}   источников, нужно по меньшей мере     N   {\textstyle N}   наблюдений (например, микрофонов, если наблюдаемый сигнал — аудио), чтобы обнаружить исходные сигналы. В этом случае матрица квадратна (    J = D   {\textstyle J=D}   , где     D   {\textstyle D}   входная размерность данных, а     J   {\textstyle J}   — размерность модели). Иначе получаем и исследуем недоопределённый (    J > D   {\textstyle J>D}  ) или переопределённый (    J < D   {\textstyle J<D}  ) случаи. То, что АНК разделение смешанных сигналов даёт очень хорошие результаты, базируется на двух допущениях и трёх эффектах источников смешанного сигнала. Двумя допущениями являются:  Источники сигналов независимы друг от друга. Значения каждого источника сигнала имеют негауссово распределение. Тремя эффектами источника смешанного сигнала являются:  Независимость: Как в и допущении 1, источники сигналов независимы, однако, их смесь не является независимой от источников. Это потому, что смесь сигналов имеет те же источники. Нормальность: Согласно центральной предельной теореме, распределение суммы независимых случайных переменных с конечной дисперсией стремится к гауссовому распределению.Грубо говоря, сумма двух независимых случайных переменных обычно имеет распределение более близкое к гауссовому, чем любое из двух исходных случайных переменных. Здесь мы рассматриваем каждый сигнал как случайную переменную. Сложность: Временна́я сложность любой смеси сигналов большее чем сложность более простого из его составляющих. Эти принципы составляют базовые основы АНК. Если сигналы, которые нам удалось извлечь из смеси, независимы подобно исходным сигналам и имеют негауссоые гистограммы или имеют малую сложность подобную сигналу источников, это должно быть сигналы источников[2][3].  Определение независимости компонент[править | править код] АНК находит независимые компоненты (которые называются факторами, скрытыми переменными или источниками) путём максимизации статистической независимости оцениваемых компонент. Мы можем выбрать один из многих путей для определения заменителя независимости и этот выбор определяет форму алгоритма АНК. Два наиболее широких определения независимости АНК  Минимизация взаимной информации Максимизация негауссовости Семейство АНК алгоритмов с минимизацией взаимной информации (англ. Minimization-of-Mutual information, MMI) использует меры, такие как расхождение Кульбака — Лейблера и максимальная энтропия. Семейство алгоритмов АНК с максимизацией негауссовости использует коэффициент эксцесса и негэнтропию. Типичный алгоритмы для АНК использует центрирование (вычитание среднего для создание нулевого среднего для сигнала), избавление от корреляции[en] (обычно с помощью спектрального разложения матрицы) и снижение размерности в качестве препроцессорных шагов для снижения сложности проблемы для действительного итеративного алгоритма. Декорреляция и снижение размерности может быть получено методом главных компонент или сингулярным разложением. Декорреляция обеспечивает, чтобы все размерности трактовались одинаково априори до прогона алгоритма. Хорошо известные алгоритмы для АНК включают infomax[en], FastICA[en], JADE[en] и ядерный независимый компонентный анализ[en] и многие другие. В общем случае АНК не может определить действительное число источников сигналов, единственный правильный порядок или масштаб (включая знак) сигналов. АНК важен для слепого разделения сигнала и имеет много практических приложений. Метод тесно связан с поиском (или даже является частным случаем поиска) факториального кодирования[en] данных, то есть нового векторного представления каждого вектора данных, так чтобы он был однозначно закодирован результирующим кодовым вектором (кодирование без потерь), но компоненты кода статистически независимы.  Математическое определение[править | править код] Линейный анализ независимых компонент может быть разделён на случай без шумов и случай с шумами, где АНК без шумов является частым случаем АНК с шумом. Нелинейный АНК следует считать отдельным случаем.  Общее определение[править | править код] Данные представлены наблюдаемым случайным вектором      x  = (  x  1   , … ,  x  m    )  T     {\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}  , а скрытые компоненты  случайным вектором      s  = (  s  1   , … ,  s  n    )  T     {\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}}  . Задачей является преобразование наблюдаемых данных      x    {\displaystyle {\boldsymbol {x}}}   с помощью статического преобразования      W    {\displaystyle {\boldsymbol {W}}}   в наблюдаемый вектор максимально независимых компонент      s  =  W   x    {\displaystyle {\boldsymbol {s}}={\boldsymbol {W}}{\boldsymbol {x}}}  , измеренных некоторой функцией независимости     F (  s  1   , … ,  s  n   )   {\displaystyle F(s_{1},\ldots ,s_{n})}  .  Генерирующая модель[править | править код] Линейная АНК без шума[править | править код] Компоненты      x  i     {\displaystyle x_{i}}   наблюдаемого случайного вектора      x  = (  x  1   , … ,  x  m    )  T     {\displaystyle {\boldsymbol {x}}=(x_{1},\ldots ,x_{m})^{T}}   генерируются как сумма независимых компонент      s  k     {\displaystyle s_{k}}  ,     k = 1 , … , n   {\displaystyle k=1,\ldots ,n}  :      x  i   =  a  i , 1    s  1   + ⋯ +  a  i , k    s  k   + ⋯ +  a  i , n    s  n     {\displaystyle x_{i}=a_{i,1}s_{1}+\cdots +a_{i,k}s_{k}+\cdots +a_{i,n}s_{n}}   взвешенных весами      a  i , k     {\displaystyle a_{i,k}}  . Та же генерирующая модель может быть записана в векторном виде как      x  =  ∑  k = 1   n     s   k     a   k     {\displaystyle {\boldsymbol {x}}=\sum _{k=1}^{n}{\boldsymbol {s}}_{k}{\boldsymbol {a}}_{k}}  , где наблюдаемый случайный вектор     x   {\displaystyle x}   представлен базисными векторами       a   k   = (   a   1 , k   , … ,   a   m , k    )  T     {\displaystyle {\boldsymbol {a}}_{k}=({\boldsymbol {a}}_{1,k},\ldots ,{\boldsymbol {a}}_{m,k})^{T}}  .  Базисные вектора       a   k     {\displaystyle {\boldsymbol {a}}_{k}}   образуют столбцы матрицы смешивания      A  = (   a   1   , … ,   a   n   )   {\displaystyle {\boldsymbol {A}}=({\boldsymbol {a}}_{1},\ldots ,{\boldsymbol {a}}_{n})}   и  генерирующая формула может быть записана как      x  =  A   s    {\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}}  , где      s  = (  s  1   , … ,  s  n    )  T     {\displaystyle {\boldsymbol {s}}=(s_{1},\ldots ,s_{n})^{T}}  . Если дана модель и реализации      x  1   , … ,  x  N     {\displaystyle x_{1},\ldots ,x_{N}}   случайного вектора      x    {\displaystyle {\boldsymbol {x}}}  , задачей является оценка как матрицы смешивания      A    {\displaystyle {\boldsymbol {A}}}  , так и источников      s    {\displaystyle {\boldsymbol {s}}}  . Это делается путём адаптивного вычисления векторов      w    {\displaystyle {\boldsymbol {w}}}   и установления функции цены, которая либо максимизирует негауссовость вычисленного      s  k   =   w   T    x    {\displaystyle s_{k}={\boldsymbol {w}}^{T}{\boldsymbol {x}}}   или минимизирует взаимную информацию. В некоторых случаях априорное знание распределения вероятности источников может быть использовано в функции цены. Исходные источники      s    {\displaystyle {\boldsymbol {s}}}   могут быть извлечены путём умножения наблюдаемых сигналов      x    {\displaystyle {\boldsymbol {x}}}   на обратную к матрице смешивания      W  =   A   − 1     {\displaystyle {\boldsymbol {W}}={\boldsymbol {A}}^{-1}}  , которая известна также как несмешивающая матрица. Здесь предполагается, что матрица смешивания квадратная (    n = m   {\displaystyle n=m}  ). Если число базисных векторов больше размерности наблюдаемых векторов     n > m   {\displaystyle n>m}  , задача является переопределённой, но остаётся разрешимой с помощью псевдообратной матрицы.  Линейный АНК с шумом[править | править код] С добавочным предположением о нулевом средним и некоррелирующим гауссовым шумом     n ∼ N ( 0 , diag ⁡ ( Σ ) )   {\displaystyle n\sim N(0,\operatorname {diag} (\Sigma ))}  , модель АНК принимает форму      x  =  A   s  + n   {\displaystyle {\boldsymbol {x}}={\boldsymbol {A}}{\boldsymbol {s}}+n}  .  Нелинейный АНК[править | править код] Смесь источников не обязательно должна быть линейной. Используя нелинейную функцию смешивания     f ( ⋅  |  θ )   {\displaystyle f(\cdot |\theta )}   с параметрами     θ   {\displaystyle \theta }   нелинейной моделью АНК будет     x = f ( s  |  θ ) + n   {\displaystyle x=f(s|\theta )+n}  .  Различимость[править | править код] Независимые компоненты различимы с точностью до перестановки и масштабирования источников. Эта различимость требует, чтобы:  Максимум один из источников      s  k     {\displaystyle s_{k}}   был гауссовым, Число наблюдаемых смесей     m   {\displaystyle m}   должно быть не меньше числа компонент     n   {\displaystyle n}  :     m ⩾ n   {\displaystyle m\geqslant n}  . Это эквивалентно высказыванию, что матрица смеси      A    {\displaystyle {\boldsymbol {A}}}   должна иметь полный ранг, чтобы существовала обратная ей. Бинарный анализ независимых компонент[править | править код] Специальным вариантом АНК является Бинарный АНК, в котором как источники сигнала, так и мониторы имеют двоичную форму, а наблюдения от мониторов являются дизъюнктивной смесью бинарных независимых источников. Задача, как было показано, имеет приложения во многих областях, включая медицинскую диагностику, многокластерное назначение, сетевую томографию[en] и управление ресурсами интернета. Пусть       x  1   ,  x  2   , … ,  x  m      {\displaystyle {x_{1},x_{2},\ldots ,x_{m}}}   является набором бинарных переменных из     m   {\displaystyle m}   мониторов и       y  1   ,  y  2   , … ,  y  n      {\displaystyle {y_{1},y_{2},\ldots ,y_{n}}}   является набором бинарных переменных из     n   {\displaystyle n}   источников. Связи источник-монитор представлены (неизвестной) смешанной матрицей      G    {\textstyle {\boldsymbol {G}}}  , где      g  i j   = 1   {\displaystyle g_{ij}=1}   указывает, что сигнал от i-го источника может наблюдаться j-м монитором. Система работает следующим образом: в любое время, если источник     i   {\displaystyle i}   активен (     y  i   = 1   {\displaystyle y_{i}=1}  ) и он связан с монитором     j   {\displaystyle j}   (     g  i j   = 1   {\displaystyle g_{ij}=1}  ) то монитор     j   {\displaystyle j}   будет наблюдать некоторую активность (     x  j   = 1   {\displaystyle x_{j}=1}  ). Формально мы имеем:       x  i   =  ⋁  j = 1   n   (  g  i j   ∧  y  j   ) , i = 1 , 2 , … , m ,   {\displaystyle x_{i}=\bigvee _{j=1}^{n}(g_{ij}\wedge y_{j}),i=1,2,\ldots ,m,}   где     ∧   {\displaystyle \wedge }   является булевым И (англ. AND), а     ∨   {\displaystyle \vee }   является булевым ИЛИ (англ. OR). Заметим, что шум не моделируется явно, а трактуется как независимые источники. Описанная выше проблема может быть эвристически решена[4] при предположении, что переменные непрерывны, путём применения метода FastICA[en] на бинарных наблюдаемых данных для получения смешанной матрицы      G    {\textstyle {\boldsymbol {G}}}   (получаем вещественные значения), затем применяем технику округления на      G    {\textstyle {\boldsymbol {G}}}   для получения бинарных значений. Этот подход, как было показано, даёт крайне неточный результат. Другим методом является использование динамического программирования — рекурсивно разбиваем матрицу наблюдений      X    {\textstyle {\boldsymbol {X}}}   на подматрицы и прогоняем алгоритм вывода на этих подматрицах. Ключевое наблюдение, которое ведёт к этому алгоритму, это то, что подматрица       X   0     {\textstyle {\boldsymbol {X}}^{0}}   матрицы      X    {\textstyle {\boldsymbol {X}}}  , где      x  i j   = 0 ∀ j   {\textstyle x_{ij}=0\forall j}   соответствует несмещённой матрице наблюдений скрытых компонент, которые не имеют связь с     i   {\displaystyle i}  -м монитором. Результаты экспериментов[5] показывают, что этот подход точен при умеренном уровне шумов. Аппарат обобщённой бинарной АНК[6] вводит более широкое описание проблемы, которое не требует какого-либо знания о порождающей модели. Другими словами, этот метод пытается разложить источник на независимые компоненты (на столько, на сколько возможно без потери какой-либо информации) без предварительных допущений о способах, как он был получен. Хотя эта задача достаточно сложная, она может быть точно решена с помощью метода ветвей и границ или точно ограничена сверху умножением матрицы на вектор.  Методы слепого разделения сигнала[править | править код] Поиск наилучшей проекции[править | править код] Смеси сигналов имеют тенденцию к получению гауссовой плотности вероятности, а сигналы источников имеют тенденцию к негауссовой плотности вероятности. Каждый источник сигнала может быть выделен из набора смесей сигналов путём вычисления скалярного произведения вектора весов и той смеси сигналов, на которой это скалярное произведение даёт ортогональную проекцию смеси сигналов. Следующая задача заключается в нахождении вектора весов. Одним из методов сделать это, является поиск наилучшей проекции[2][7]. Поиск наилучшей проекции ищет одну проекцию за шаг, такую что выделенный сигнал будет настолько негауссовым, насколько это возможно. Это контрастирует с АНК, который обычно выделяет M сигналов одновременно из M смесей сигналов, что требует оценки     M × M   {\displaystyle M\times M}   несмешивающей матрицы. Одним из практических преимуществ поиска наилучшей проекции над АНК является то, что может выделяться менее M сигналов, если требуется, где каждый источник сигнала выделяется из смеси M сигналов используя M-элементный вектор весов. Мы можем использовать коэффициент эксцесса для извлечения сигнала с несколькими источниками путём нахождения правильных векторов весов с использованием поиска наилучшей проекции. Коэффициент эксцесса плотности вероятности сигнала, для конечной выборки  вычисляется как      K =    E ⁡ [ (  y  −   y ¯    )  4   ]   ( E ⁡ [ (  y  −   y ¯    )  2   ]  )  2      − 3   {\displaystyle K={\frac {\operatorname {E} [(\mathbf {y} -\mathbf {\overline {y}} )^{4}]}{(\operatorname {E} [(\mathbf {y} -\mathbf {\overline {y}} )^{2}])^{2}}}-3}   где       y ¯     {\displaystyle \mathbf {\overline {y}} }   является выборочным средним      y    {\displaystyle \mathbf {y} }   выделенных сигналов. Константа 3 обеспечивает, чтобы гауссовы сигналы имели нулевой коэффициент эксцесса, супергауссовы сигналы имели положительный коэффициент эксцесса, а субгауссовы сигналы имели отрицательный коэффициент эксцесса. Знаменатель равен дисперсии      y    {\displaystyle \mathbf {y} }   и он обеспечивает, чтобы измеренный коэффициент эксцесса получал дисперсию сигнала. Целью поиска наилучшей проекции является максимизация коэффициента эксцесса и сделать выделенный сигнал настолько ненормальным, насколько возможно. Используя коэффициент эксцесса как меру ненормальности мы можем теперь проверить насколько коэффициент эксцесса сигнала      y  =   w   T    x    {\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {x} }  , извлечённого из наборе M смесей      x  = (  x  1   ,  x  2   , … ,  x  M    )  T     {\displaystyle \mathbf {x} =(x_{1},x_{2},\ldots ,x_{M})^{T}}  , изменяется по мере того, как вектор весов      w    {\displaystyle \mathbf {w} }   вращается вокруг начала координат. Если задано, что каждый источник сигнала      s    {\displaystyle \mathbf {s} }   является супергауссовым, мы можем ожидать  коэффициент эксцесса извлечённого сигнала      y    {\displaystyle \mathbf {y} }   максимален в точности тогда, когда      y  =  s    {\displaystyle \mathbf {y} =\mathbf {s} }  . коэффициент эксцесса извлечённого сигнала      y    {\displaystyle \mathbf {y} }   максимален, когда      w    {\displaystyle \mathbf {w} }   орогонален проекциям осей      S  1     {\displaystyle S_{1}}   или      S  2     {\displaystyle S_{2}}  , поскольку мы знаем, что вектор оптимального веса должен быть ортогонален преобразованным осям      S  1     {\displaystyle S_{1}}   и      S  2     {\displaystyle S_{2}}  . Для смеси сигналов от разных источников мы можем использовать коэффициент эксцесса ортогонализации Грама ― Шмидта (ОГШ) для  извлечения сигналов. Если дан смесь M сигналов в M-мерном пространстве, ОГШ проектирует эти точки данных в (M-1)-мерное пространство с помощью вектора весов. Мы можем гарантировать независимость выделенных сигналов с помощью ОГШ. С целью поиска правильного значения      w    {\displaystyle \mathbf {w} }   мы можем использовать метод градиентного спуска. Прежде всего, мы избавляемся от корреляции и преобразуем      x    {\displaystyle \mathbf {x} }   в новую смесь      z    {\displaystyle \mathbf {z} }  , которая имеет единичную дисперсию и      z  = (  z  1   ,  z  2   , … ,  z  M    )  T     {\displaystyle \mathbf {z} =(z_{1},z_{2},\ldots ,z_{M})^{T}}  . Этот процесс может быть выполнен путём применения сингулярное разложение к      x    {\displaystyle \mathbf {x} }  ,       x  =  U   D    V   T     {\displaystyle \mathbf {x} =\mathbf {U} \mathbf {D} \mathbf {V} ^{T}}   Масштабируем каждый вектор      U  i   =  U  i    /  E ⁡ (  U  i   2   )   {\displaystyle U_{i}=U_{i}/\operatorname {E} (U_{i}^{2})}   и положим      z  =  U    {\displaystyle \mathbf {z} =\mathbf {U} }  . Сигнал, выделенный взвешенным вектором      w    {\displaystyle \mathbf {w} }  , равен      y  =   w   T    z    {\displaystyle \mathbf {y} =\mathbf {w} ^{T}\mathbf {z} }  . Если вектор весов w имеет единичную длину, то есть     E ⁡ [ (   w   T    z   )  2   ] = 1   {\displaystyle \operatorname {E} [(\mathbf {w} ^{T}\mathbf {z} )^{2}]=1}  , тогда коэффициент эксцесса можно переписать как:      K =    E ⁡ [   y   4   ]   ( E ⁡ [   y   2   ]  )  2      − 3 = E ⁡ [ (   w   T    z   )  4   ] − 3.   {\displaystyle K={\frac {\operatorname {E} [\mathbf {y} ^{4}]}{(\operatorname {E} [\mathbf {y} ^{2}])^{2}}}-3=\operatorname {E} [(\mathbf {w} ^{T}\mathbf {z} )^{4}]-3.}   Процесс обновления для      w    {\displaystyle \mathbf {w} }  :        w   n e w   =   w   o l d   − η E ⁡ [  z  (   w   o l d   T    z   )  3   ] .   {\displaystyle \mathbf {w} _{new}=\mathbf {w} _{old}-\eta \operatorname {E} [\mathbf {z} (\mathbf {w} _{old}^{T}\mathbf {z} )^{3}].}   где     η   {\displaystyle \eta }   является малой константой для гарантирования, что      w    {\displaystyle \mathbf {w} }   сходится к оптимальному решению. После каждого обновления мы нормализуем       w   n e w   =     w   n e w     |    w   n e w    |       {\displaystyle \mathbf {w} _{new}={\frac {\mathbf {w} _{new}}{|\mathbf {w} _{new}|}}}   и множество       w   o l d   =   w   n e w     {\displaystyle \mathbf {w} _{old}=\mathbf {w} _{new}}   и повторяем процесс обновления пока он не сойдётся. Мы можем использовать также другой алгоритм для обновления вектора весов      w    {\displaystyle \mathbf {w} }  . Другим подходом является использование негэнтропии[8] вместо коэффициента эксцесса. Негэнтропия является устойчивым методом по отношению коэффициента эксцесса, поскольку коэффициента эксцесса очень чувствителен к выбросамs. Метод негэнтропии основывается на важном свойстве распределения Гаусса — нормальная случайная величина имеет наибольшую энтропию среди всех непрерывных случайных переменных с одинаковой дисперсией. Это также является причиной, почему мы хотим найти наиболее негауссовые переменные. Простое доказательство можно найти в статье дифференциальной энтропии.      J ( x ) = S ( y ) − S ( x )    {\displaystyle J(x)=S(y)-S(x)\,}   y являются гауссовой случайной переменной некоторой ковариантной матрицы,       S ( x ) = − ∫  p  x   ( u ) log ⁡  p  x   ( u ) d u   {\displaystyle S(x)=-\int p_{x}(u)\log p_{x}(u)du}   Аппроксимация для негэнтропии равна      J ( x ) =   1 12   ( E (  x  3   )  )  2   +   1 48   ( k u r t ( x )  )  2     {\displaystyle J(x)={\frac {1}{12}}(E(x^{3}))^{2}+{\frac {1}{48}}(kurt(x))^{2}}   Доказательство можно найти на странице 131 книги «Анализ независимых компонент», которую написали Аапо Хювяринен, Юха Кархунен и Эркки Ойя[3]. Эта аппроксимация также страдает те ми же проблемами, что и коэффициент эксцесса (чувствительность к выбросам). Разрабатывались и другие подходы[9]      J ( y ) =  k  1   ( E (  G  1   ( y ) )  )  2   +  k  2   ( E (  G  2   ( y ) ) − E (  G  2   ( v )  )  2     {\displaystyle J(y)=k_{1}(E(G_{1}(y)))^{2}+k_{2}(E(G_{2}(y))-E(G_{2}(v))^{2}}   Выбор      G  1     {\displaystyle G_{1}}   и      G  2     {\displaystyle G_{2}}         G  1   =   1  a  1     log ⁡ ( cosh ⁡ (  a  1   u ) )   {\displaystyle G_{1}={\frac {1}{a_{1}}}\log(\cosh(a_{1}u))}   и      G  2   = − exp ⁡ ( −    u  2   2   )   {\displaystyle G_{2}=-\exp(-{\frac {u^{2}}{2}})}   Основанный на infomax[править | править код] АНК, по существу, является многомерной параллельной версией поиска наилучшей проекции. В то время как поиск наилучшей проекции выделяет серию сигналов по одному из смеси M сигналов, АНК выделяет M сигналов параллельно. Это приводит к большей устойчивости АНК по сравнению с поиском наилучшей проекции[2]. Метод поиска наилучшей проекции, чтобы обеспечить независимость выделяемых сигналов, использует ортогонализацию Грама ― Шмидта в то время как АНК использует метод infomax[en] и оценку максимального правдоподобия для обеспечения независимости выделяемого сигнала. Ненормальность выделяемого сигнала достигается с помощью соответствующей модели. Процесс АНК, основанный на infomax[en], коротко: если дана смесь сигналов      x    {\displaystyle \mathbf {x} }   и набор одинаковых независимых функций распределения     g   {\displaystyle g}  , мы ищем несмешивающую матрицу      W    {\displaystyle \mathbf {W} }  , которая максимизирует совместную энтропию сигналов      Y  = g (  y  )   {\displaystyle \mathbf {Y} =g(\mathbf {y} )}  , где      y  =  W x    {\displaystyle \mathbf {y} =\mathbf {Wx} }   являются сигналами, отобранными по      W    {\displaystyle \mathbf {W} }  . Если дана оптимальная      W    {\displaystyle \mathbf {W} }  , сигналы      Y    {\displaystyle \mathbf {Y} }   имеют максимальную энтропию и, потому, независимы, что гарантирует, что выделенные сигналы      y  =  g  − 1   (  Y  )   {\displaystyle \mathbf {y} =g^{-1}(\mathbf {Y} )}   также независимы. Функция     g   {\displaystyle g}   обратима и является моделью сигнала. Заметим, что если плотность вероятности модели источника сигнала      p  s     {\displaystyle p_{s}}   соответствует плотности вероятности выделенного сигнала      p   y      {\displaystyle p_{\mathbf {y} }}  , то максимизация совместной энтропии     Y   {\displaystyle Y}   также максимизирует количество взаимной информации между      x    {\displaystyle \mathbf {x} }   и      Y    {\displaystyle \mathbf {Y} }  . По этой причине, использование энтропии для выделения независимых сигналов известно как infomax[en]. Рассмотрим энтропию векторной переменной      Y  = g (  y  )   {\displaystyle \mathbf {Y} =g(\mathbf {y} )}  , где      y  =  W x    {\displaystyle \mathbf {y} =\mathbf {Wx} }   является набором сигналов, выделенных несмешивающей матрицей      W    {\displaystyle \mathbf {W} }  . Для конечного набора значений, выбранных из распределения с плотностью вероятности      p   y      {\displaystyle p_{\mathbf {y} }}  , энтропия      Y    {\displaystyle \mathbf {Y} }   может быть оценена как:      H (  Y  ) = −   1 N    ∑  t = 1   N   ln ⁡  p   Y    (   Y   t   )   {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {Y} }(\mathbf {Y} ^{t})}   Совместная плотность вероятности      p   Y      {\displaystyle p_{\mathbf {Y} }}  , как можно показать, связана с совместной плотностью вероятности      p   y      {\displaystyle p_{\mathbf {y} }}   извлечённых сигналов с помощью многомерной формы:       p   Y    ( Y ) =     p   y    (  y  )    |     ∂  Y    ∂  y      |       {\displaystyle p_{\mathbf {Y} }(Y)={\frac {p_{\mathbf {y} }(\mathbf {y} )}{|{\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}|}}}   где      J  =    ∂  Y    ∂  y       {\displaystyle \mathbf {J} ={\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}}   является матрицей Якоби. Мы имеем      |   J   |  =  g ′  (  y  )   {\displaystyle |\mathbf {J} |=g'(\mathbf {y} )}  , и      g ′    {\displaystyle g'}   является плотностью вероятности, принятых для источников сигналов      g ′  =  p  s     {\displaystyle g'=p_{s}}  , поэтому,       p   Y    ( Y ) =     p   y    (  y  )    |     ∂  Y    ∂  y      |     =     p   y    (  y  )    p   s    (  y  )      {\displaystyle p_{\mathbf {Y} }(Y)={\frac {p_{\mathbf {y} }(\mathbf {y} )}{|{\frac {\partial \mathbf {Y} }{\partial \mathbf {y} }}|}}={\frac {p_{\mathbf {y} }(\mathbf {y} )}{p_{\mathbf {s} }(\mathbf {y} )}}}   поэтому,      H (  Y  ) = −   1 N    ∑  t = 1   N   ln ⁡     p   y    (  y  )    p   s    (  y  )      {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln {\frac {p_{\mathbf {y} }(\mathbf {y} )}{p_{\mathbf {s} }(\mathbf {y} )}}}   Мы знаем, что когда      p   y    =  p  s     {\displaystyle p_{\mathbf {y} }=p_{s}}  ,      p   Y      {\displaystyle p_{\mathbf {Y} }}   является однородным распределением, а     H (   Y   )   {\displaystyle H({\mathbf {Y} })}   максимизирована. Поскольку       p   y    (  y  ) =     p   x    (  x  )    |     ∂  y    ∂  x      |     =     p   x    (  x  )    |   W   |       {\displaystyle p_{\mathbf {y} }(\mathbf {y} )={\frac {p_{\mathbf {x} }(\mathbf {x} )}{|{\frac {\partial \mathbf {y} }{\partial \mathbf {x} }}|}}={\frac {p_{\mathbf {x} }(\mathbf {x} )}{|\mathbf {W} |}}}   где      |   W   |    {\displaystyle |\mathbf {W} |}   является абсолютным значением определителя несмешивающей матрицы      W    {\displaystyle \mathbf {W} }  . Поэтому,      H (  Y  ) = −   1 N    ∑  t = 1   N   ln ⁡     p   x    (   x   t   )    |   W   |   p   s    (   y   t   )      {\displaystyle H(\mathbf {Y} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln {\frac {p_{\mathbf {x} }(\mathbf {x} ^{t})}{|\mathbf {W} |p_{\mathbf {s} }(\mathbf {y} ^{t})}}}   так что,      H (  Y  ) =   1 N    ∑  t = 1   N   ln ⁡  p   s    (   y   t   ) + ln ⁡  |   W   |  + H (  x  )   {\displaystyle H(\mathbf {Y} )={\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {s} }(\mathbf {y} ^{t})+\ln |\mathbf {W} |+H(\mathbf {x} )}   поскольку     H (  x  ) = −   1 N    ∑  t = 1   N   ln ⁡  p   x    (   x   t   )   {\displaystyle H(\mathbf {x} )=-{\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {x} }(\mathbf {x} ^{t})}  , и максимизация      W    {\displaystyle \mathbf {W} }   не влияет на      H   x      {\displaystyle H_{\mathbf {x} }}  , мы можем максимизировать функцию      h (  Y  ) =   1 N    ∑  t = 1   N   ln ⁡  p   s    (   y   t   ) + ln ⁡  |   W   |    {\displaystyle h(\mathbf {Y} )={\frac {1}{N}}\sum _{t=1}^{N}\ln p_{\mathbf {s} }(\mathbf {y} ^{t})+\ln |\mathbf {W} |}   чтобы получить независимость извлечённого сигнала. Если имеется M маргинальных плотностей вероятности модели, совместные плотности вероятности      p   s      {\displaystyle p_{\mathbf {s} }}   независимы и используют супергауссову модель плотности вероятности для источников сигналов      p   s    = ( 1 − tanh ⁡ (  s   )  2   )   {\displaystyle p_{\mathbf {s} }=(1-\tanh(\mathbf {s} )^{2})}  , то мы получаем      h (  Y  ) =   1 N    ∑  i = 1   M    ∑  t = 1   N   ln ⁡ ( 1 − tanh ⁡ (   w  i   T    x  t     )  2   ) + ln ⁡  |   W   |    {\displaystyle h(\mathbf {Y} )={\frac {1}{N}}\sum _{i=1}^{M}\sum _{t=1}^{N}\ln(1-\tanh(\mathbf {w_{i}^{T}x^{t}} )^{2})+\ln |\mathbf {W} |}   В сумме, если задана наблюдаемая смесь сигнала      x    {\displaystyle \mathbf {x} }  , соответствующий набор извлечённых сигналов      y    {\displaystyle \mathbf {y} }   и модель источника сигнала      p   s    =  g ′    {\displaystyle p_{\mathbf {s} }=g'}  , мы можем найти оптимальную несмешивающую матрицу      W    {\displaystyle \mathbf {W} }   и сделать извлечённые сигналы независимыми и негауссовыми. Подобно ситуации с поиском наилучшей проекции, мы можем использовать метод градиентного спуска для поиска оптимального решения несмешивающей матрицы.  На основе оценки максимального правдоподобия[править | править код] Оценка максимального правдоподобия (англ. Maximum likelihood estimation, MLE) является стандартным статистическим средством для нахождения значений параметров (например, несмешивающей матрицы      W    {\displaystyle \mathbf {W} }  ), которые обеспечивают лучшее соответствие некоторых данных (например, извлечённых сигналов     y   {\displaystyle y}  ) для данной модели (например, совместной плотности вероятности (ПВ)      p  s     {\displaystyle p_{s}}   источников сигналов) [2]. Модель максимального правдоподобия включает спецификацию плотности вероятности, которая в этом случае является плотностью вероятности      p  s     {\displaystyle p_{s}}   сигналов неизвестного источника     s   {\displaystyle s}  . При использовании максимального правдоподобия целью является нахождение несмешивающей матрицы, которая даёт извлечённые сигналы     y =  W  x   {\displaystyle y=\mathbf {W} x}   с совместной плотностью вероятности, которые максимально подобны совместной плотностью вероятности      p  s     {\displaystyle p_{s}}   сигналов неизвестного источника     s   {\displaystyle s}  . Оценка максимального правдоподобия основывается на предположении, что если модель плотности вероятности      p  s     {\displaystyle p_{s}}   и модель параметров      A    {\displaystyle \mathbf {A} }   правильны, то должно быть получена высокая вероятность для     x   {\displaystyle x}  , что эти данные действительно наблюдаемы. Обратно, если      A    {\displaystyle \mathbf {A} }   далек от верных значений параметров, то следует ожидать низкую вероятность наблюдения данных. При оценке максимального правдоподобия мы называем вероятность наблюдаемых данных для данного набора значений параметров модели (например, плотности вероятности      p  s     {\displaystyle p_{s}}   и матрицы      A    {\displaystyle \mathbf {A} }  ) правдоподобностью значений параметров модели, заданной наблюдаемыми данными. Мы определяем функцию правдоподобия      L ( W )    {\displaystyle \mathbf {L(W)} }   матрицы      W    {\displaystyle \mathbf {W} }  :      L ( W )  =  p  s   (  W  x )  |  det  W   |  .   {\displaystyle \mathbf {L(W)} =p_{s}(\mathbf {W} x)|\det \mathbf {W} |.}   Это равно плотности вероятности в     x   {\displaystyle x}  , поскольку     s =  W  x   {\displaystyle s=\mathbf {W} x}  . Тогда, если мы хотим найти      W    {\displaystyle \mathbf {W} }  , то наиболее вероятно иметь сгенерированные наблюдаемые смеси     x   {\displaystyle x}   из неизвестных источников сигналов     s   {\displaystyle s}   с плотностью вероятности      p  s     {\displaystyle p_{s}}  , то нам нужно лишь найти      W    {\displaystyle \mathbf {W} }  , которая максимизирует правдоподобность      L ( W )    {\displaystyle \mathbf {L(W)} }  . Несмешивающая матрица, которая максимизирует равенство известна как оценка максимального правдоподобия оптимальной несмешивающей матрицей. Распространённой практикой является использование логарифма правдоподобия, поскольку его проще всего вычислить. Так как логарифм является монотонной функцией, матрица      W    {\displaystyle \mathbf {W} }  , которая максимизирует функция      L ( W )    {\displaystyle \mathbf {L(W)} }  , также максимизирует его логарифм     ln ⁡  L ( W )    {\displaystyle \ln \mathbf {L(W)} }  . Это позволяет взять логарифм в равенстве выше, что даёт логарифм функции правдоподобия     ln ⁡  L ( W )  =  ∑  i    ∑  t   ln ⁡  p  s   (  w  i   T    x  t   ) + N ln ⁡  |  det  W   |    {\displaystyle \ln \mathbf {L(W)} =\sum _{i}\sum _{t}\ln p_{s}(w_{i}^{T}x_{t})+N\ln |\det \mathbf {W} |}   Если мы подставим широко используемую модель плотности вероятности с высоким коэффициентом эксцесса для источников сигналов      p  s   = ( 1 − tanh ⁡ ( s  )  2   )   {\displaystyle p_{s}=(1-\tanh(s)^{2})}  , мы получим     ln ⁡  L ( W )  =   1 N    ∑  i   M    ∑  t   N   ln ⁡ ( 1 − tanh ⁡ (  w  i   T    x  t    )  2   ) + ln ⁡  |  det  W   |    {\displaystyle \ln \mathbf {L(W)} ={1 \over N}\sum _{i}^{M}\sum _{t}^{N}\ln(1-\tanh(w_{i}^{T}x_{t})^{2})+\ln |\det \mathbf {W} |}   Матрица      W    {\displaystyle \mathbf {W} }  , максимизирующая эту функцию, является оценкой максимального правдоподобия.  История и предпосылки[править | править код] Раннюю общую основу для анализа независимых компонент предложили Дженни Эро и Бернард Анс в 1984[10], затем к ним присоединился Христиан Джуттен с 1985[11][12][13]. Наиболее ясно метод изложил Пьер Комон в 1994[14]. В 1995 Тони Белл и Терри Седжновски предложили быстрый и эффективный алгоритм АНК, основанный на infomax[en], принципе, введённом Ральфом Линскером в 1987. Есть много алгоритмов, доступных в литературе, которые реализуют АНК. Широко используется, включая производственные приложения, алгоритм FastICA, который разработали Аапо Хювяринен и Эркки Ойя. Алгоритм использует коэффициент эксцесса в качестве функции цены. Другие примеры скорее связаны со слепым разделением сигнала, где используется более общий подход. Например, можно опустить допущение независимости и разделить попарно коррелирующие сигналы, а следовательно, статистически «зависимых» сигналов. Сепп Хохрайтер и Юрген Шмидхубер показали, как получить нелинейный АНК или разделение источников как побочный продукт регуляризации (1999)[15]. Их метод не требует априорного знания о числе независимых источников.  Приложения[править | править код] АНК может быть расширен на анализ нефизических сигналов. Например, АНК был применён для обнаружения тем обсуждения в архивах новостей. Некоторые из приложений АНК перечислены ниже[2]:   Анализ независимых компонент в EEGLAB[en] оптическое изображение нейронов[16] сортировка импульсов нейронов sorting[17] распознавание лица[18] моделирование рецепторного поля главных зрительных нейронов[19] предсказание цен на рынке бумаг[20] мобильная телефонная связь[21] определение зрелости помидор на основе цвета[22] удаление объектов, таких как мигание глаза, из данных электроэнцефалограммы[23] анализ изменений в экспрессии гена со временем в экспериментах по секвенированию РНК в отдельной клетке cell[24] изучение функциональной МРТ мозга в состоянии покоя[en][25] См. также[править | править код]  Слепая деконволюция Факторный анализ Спектр Гилберта[en] Обработка цифрового изображения[en] Мультилинейный анализ главных компонент[en] Полилинейное обучение подпространств[en] Неотрицательное матричное разложение Нелинейное снижение размерности[en] Поиск наилучшей проекции Вращение по методу Varimax[en]  Примечания[править | править код]   ↑ Isomura, Toyoizumi, 2016.  ↑ 1 2 3 4 5 Stone, 2004.  ↑ 1 2 Hyvärinen, Karhunen, Oja, 2001.  ↑ Himbergand, Hyvärinen, 2001.  ↑ Nguyen, Zheng, 2011, с. 3168-3181.  ↑ Painsky, Rosset, Feder, 2014, с. 1326–1330.  ↑ Kruskal, 1969, с. 427–440.  ↑ Hyvärinen, Oja, 2000, с. 411–430.  ↑ Hyvärinen, 1998, с. 273–279.  ↑ Hérault, Ans, 1984, с. 525–528.  ↑ Ans, Hérault, Jutten, 1985, с. 593-597.  ↑ Hérault, Jutten, Ans, 1985, с. 1017-1022.  ↑ Hérault, Jutten, 1986, с. 206-211.  ↑ Comon, 1994.  ↑ Hochreiter, Schmidhuber, 1999, с. 679–714.  ↑ Brown, Yamada, Sejnowski, 2001, с. 54–63.  ↑ Lewicki, 1998, с. 53–78.  ↑ Barlett, 2001.  ↑ Bell, Sejnowski, 1997, с. 3327–3338.  ↑ Back, Weigend, 1997, с. 473–484.  ↑ Hyvarinen, Karhunen, Oja, 2001.  ↑ Polder, van der Heijen, 2003, с. 57–64.  ↑ Delorme, Sejnowski, Makeig, 2007, с. 1443–1449.  ↑ Trapnell, Cacchiarelli, Grimsby, 2014, с. 381–386.  ↑ Kiviniemi, Kantola, Jauhiainen, Hyvärinen, Tervonen, 2003, с. 253–260.   Литература[править | править код]  Takuya Isomura, Taro Toyoizumi. A local learning rule for independent component analysis // Scientific Reports. — 2016. — DOI:10.1038/srep28073. Aapo Hyvärinen, Juha Karhunen, Erkki Oja. Independent component analysis. — 1st. — New York: John Wiley & Sons, 2001. — ISBN 0-471-22131-7. Aapo Hyvärinen, Juha Karhunen, Erkki Oja. Independent component analysis. — Reprint. — New York, NY: Wiley, 2001. — ISBN 0-471-40540-X. Aapo Hyvärinen, Erkki Oja. Independent Component Analysis:Algorithms and Applications // Neural Networks. — 2000. — Т. 13, вып. 4–5. — DOI:10.1016/s0893-6080(00)00026-5. — PMID 10946390. Johan Himbergand, Aapo Hyvärinen. Independent Component Analysis For Binary Data: An Experimental Study // Proc. Int. Workshop on Independent Component Analysis and Blind Signal Separation (ICA2001). — San Diego, California, 2001. Aapo Hyvärinen. New approximations of differential entropy for independent component analysis and projection pursuit. // Advances in Neural Information Processing Systems. — 1998. — Т. 10. Huy Nguyen, Rong Zheng. Binary Independent Component Analysis With or Mixtures // IEEE Transactions on Signal Processing. — 2011. — Июль (т. 59, вып. 7). Amichai Painsky, Saharon Rosset, Meir Feder. Generalized Binary Independent Component Analysis // IEEE International Symposium on Information Theory (ISIT), 2014. — 2014. James V. Stone. Independent Component Analysis: A Tutorial Introduction. — Cambridge, Massachusetts, London, England: The MIT Press, 2004. — ISBN 0-262-69315-1. Kruskal J. B. Toward a practical method which helps uncover the structure of a set of observations by finding the line transformation which optimizes a new "index of condensation" // Statistical computation / Milton R. C., Nelder J. A.. — New York: Academic Press, 1969. Pierre Comon. Independent Component Analysis: a new concept? // Signal Processing. — 1994. — Т. 36, вып. 3. — С. 287–314. (Оригинальная статья, описывающая концепцию ICA) Comon P., Jutten C. Handbook of Blind Source Separation, Independent Component Analysis and Applications. — Oxford UK: Academic Press, 2010. — ISBN 978-0-12-374726-6. Lee T.-W. Independent component analysis: Theory and applications. — Boston, Mass: Kluwer Academic Publishers, 1998. — ISBN 0-7923-8261-7. Ranjan Acharyya. A New Approach for Blind Source Separation of Convolutive Sources - Wavelet Based Separation Using Shrinkage Function. — 2008. — ISBN 3-639-07797-0. (книга фокусируется на обучении без учителя с помощью слепого выделения источника) Hérault J., Ans B. Réseau de neurones à synapses modifiables : Décodage de messages sensoriels composites par apprentissage non supervisé et permanent // Comptes Rendus de l'Académie des Sciences, Série III. — 1984. — Т. 299. — С. 525–528. Ans B., Hérault J., Jutten C. Architectures neuromimétiques adaptatives: Détection de primitives. // Cognitiva 85, Paris 4-7 Juin 1985. — Paris, 1985. — Т. 2. Hérault J., Jutten C., Ans B. Détection de grandeurs primitives dans un message composite par une architecture de calcul neuromimétique en apprentissage non supervise // Proceedings of the 10th Workshop Traitement du signal et ses applications. — Nice (France): GRETSI, 1985. — Т. 2. Hérault J., Jutten C. Space or time adaptive signal processing by neural networks models // Intern. Conf. on Neural Networks for Computing. — Utah, USA: Snowbird, 1986. Sepp Hochreiter, Jürgen Schmidhuber. Feature Extraction Through LOCOCODE // Neural Computation. — 1999. — Т. 11, вып. 3. — ISSN 0899-7667. — DOI:10.1162/089976699300016629. Brown G. D., Yamada S., Sejnowski T. J. Independent components analysis at the neural cocktail party // Trends in Neurosciences. — 2001. — Т. 24, вып. 1. — DOI:10.1016/s0166-2236(00)01683-0. Lewicki M. S. Areview of methods for spike sorting: detection and classification of neural action potentials // Network: Computation in Neural Systems. — 1998. — Т. 9. Barlett M. S. Face image analysis by unsupervised learning. — Boston: Kluwer International Series on Engineering and Computer Science, 2001. — Т. 612. — (SECS). — ISBN 978-1-4613-5653-0. Bell A. J., Sejnowski T. J. The independent components of natural scenes are edge filters // Vision Research. — 1997. — Т. 37, вып. 23. — DOI:10.1016/s0042-6989(97)00121-1. — PMID 9425547. Back A. D., Weigend A. S. A first application of independent component analysis to extracting structure from stock returns // International Journal of Neural Systems. — 1997. — Т. 8, вып. 4. — DOI:10.1142/s0129065797000458. — PMID 9730022. Hyvarinen A., Karhunen J., Oja E. Independent component analysis / Symon Haykin. — New York: John Wiley and Sons, 2001. — (Adaptive and Learning System for Signal Processing, Communications, and Control). — ISBN 0-471-40540-X. Polder G., van der Heijen F.W.A.M. Estimation of compound distribution in spectral images of tomatoes using independent component analysis // Austrian Computer Society. — 2003. Delorme A., Sejnowski T., Makeig S. Enhanced detection of artifacts in EEG data using higher-order statistics and independent component analysis // NeuroImage. — 2007. — Т. 34, вып. 4. — DOI:10.1016/j.neuroimage.2006.11.004. — PMID 17188898. Trapnell C., Cacchiarelli D., Grimsby J. The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells // Nature Biotechnology. — 2014. — Т. 32, вып. 4. — DOI:10.1038/nbt.2859. — PMID 24658644. Vesa J. Kiviniemi, Juha-Heikki Kantola, Jukka Jauhiainen, Aapo Hyvärinen, Osmo Tervonen. Independent component analysis of nondeterministic fMRI signal sources // NeuroImage. — 2003. — Т. 19. — DOI:10.1016/S1053-8119(03)00097-1. — PMID 12814576.  Ссылки[править | править код] Что есть анализ независимых компонент? (Аапо Хювяринен) Обучающий курс: Анализ независимых компонент (Аапо Хювяринен) Обучающий курс: Анализ независимых компонент FastICA как пакет для Matlab, на языках R и C++ Инструментальные средства ICALAB для Matlab, разработанный в RIKEN Инструментальные средства высокоэффективного анализа сигналов даёт реализацию на языке C++ методов FastICA и Infomax Инструментальные средства ICA - Инструментальные средства Matlab для ICA с Bell-Sejnowski, Molgedey-Schuster и mean field ICA. Разработано в DTU. Демонстрация задачи о шумной вечеринке Инструментальное средство EEGLAB ICA для электроэнцефалограмм для Matlab, разработанное в UCSD. Инструментальные средства FMRLAB ICA функциональной магнитно-резонансной томографии для Matlab, разработанное в UCSD MELODIC, часть библиотеки FMRIB[en]. Обсуждение ICA, используемой в контексте биометрических представлений объектов Алгоритмы FastICA, CuBICA, JADE и TDSEP для языка Python Инструментальные средства Group ICA и Fusion ICA Обучающий курс: Использование ICA для очищения сигналов энцефалограммы Для улучшения этой статьи желательно: Проверить качество перевода с иностранного языка.Исправить статью согласно стилистическим правилам Википедии.Проверить статью на грамматические и орфографические ошибки.    