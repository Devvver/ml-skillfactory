Авторегрессионная модель Авторегрессионная (AR-) модель (англ. autoregressive model) — модель временных рядов, в которой значения временного ряда в данный момент линейно зависят от предыдущих значений этого же ряда. Авторегрессионный процесс порядка p (AR(p)-процесс) определяется следующим образом       X  t   = c +  ∑  i = 1   p    a  i    X  t − i   +  ε  t   ,   {\displaystyle X_{t}=c+\sum _{i=1}^{p}a_{i}X_{t-i}+\varepsilon _{t},}   где      a  1   , … ,  a  p     {\displaystyle a_{1},\ldots ,a_{p}}   — параметры модели (коэффициенты авторегрессии),     c   {\displaystyle c}   — постоянная (часто для упрощения предполагается равной нулю), а      ε  t     {\displaystyle \varepsilon _{t}}   — белый шум. Простейшим примером является авторегрессионный процесс первого порядка AR(1)-процесс:       X  t   = c + r  X  t − 1   +  ε  t     {\displaystyle X_{t}=c+rX_{t-1}+\varepsilon _{t}}   Для данного процесса коэффициент авторегрессии совпадает с коэффициентом автокорреляции первого порядка. Другой простой процесс — процесс Юла — AR(2)-процесс:       X  t   = c +  a  1    X  t − 1   +  a  2    X  t − 2   +  ε  t     {\displaystyle X_{t}=c+a_{1}X_{t-1}+a_{2}X_{t-2}+\varepsilon _{t}}   Содержание  1 Операторное представление 2 Автокорреляционная функция 3 Оценка параметров модели 4 Сезонные модели авторегрессии 5 См. также 6 Примечания   Операторное представление[править | править код] Если ввести лаговый оператор     L : L  x  t   =  x  t − 1     {\displaystyle L:Lx_{t}=x_{t-1}}  , то авторегрессионную модель можно представить следующим образом       X  t   = c +  ∑  i = 1   p    a  i    L  i    X  t   +  ε  t   ,   {\displaystyle X_{t}=c+\sum _{i=1}^{p}a_{i}L^{i}X_{t}+\varepsilon _{t},}   или      a ( L )  X  t   = ( 1 −  ∑  i = 1   p    a  i    L  i   )  X  t   = c +  ε  t     {\displaystyle a(L)X_{t}=(1-\sum _{i=1}^{p}a_{i}L^{i})X_{t}=c+\varepsilon _{t}}   Стационарность авторегрессионного процесса зависит от корней характеристического полинома     a ( z ) = 1 −  ∑  i = 1   n    a  i    z  i     {\displaystyle a(z)=1-\sum _{i=1}^{n}a_{i}z^{i}}  . Для того чтобы процесс был стационарным[1], достаточно, чтобы все корни характеристичекого полинома лежали вне единичного круга в комплексной плоскости      |  z  |  > 1   {\displaystyle |z|>1}  . В частности, для AR(1)-процесса     a ( z ) = 1 − r z   {\displaystyle a(z)=1-rz}  , следовательно корень этого полинома     z = 1  /  r   {\displaystyle z=1/r}  , поэтому условие стационарности можно записать в виде      |  r  |  < 1   {\displaystyle |r|<1}  , то есть коэффициент авторегрессии (он же в данном случае коэффициент автокорреляции) должен быть строго меньше 1 по модулю. Для AR(2)-процесса можно показать, что условия стационарности имеют вид:      |   a  2    |  < 1 ,  a  2   ±  a  1   < 1   {\displaystyle |a_{2}|<1,a_{2}\pm a_{1}<1}  . Стационарные AR-процессы допускают разложение Вольда — представление в виде бесконечного MA-процесса:      X  t   =  a  − 1   ( L ) c +  a  − 1   ( L )  ε  t   =   c  1 −  ∑  i = 1   p    a  i      +  ∑  j = 0   ∞    b  j    ε  t − j     {\displaystyle X_{t}=a^{-1}(L)c+a^{-1}(L)\varepsilon _{t}={\frac {c}{1-\sum _{i=1}^{p}a_{i}}}+\sum _{j=0}^{\infty }b_{j}\varepsilon _{t-j}}   Первое слагаемое представляет собой математическое ожидание AR-процесса. Если c=0, то математическое ожидание процесса также равно нулю.  Автокорреляционная функция[править | править код] Можно показать, что автоковариационная и автокорреляционная функции AR(p)-процесса удовлетворяют рекуррентным соотношениям:     γ ( k ) =  ∑  j = 1   p    a  j   γ ( k − j )           r ( k ) =  ∑  j = 1   p    a  j   r ( k − j )   {\displaystyle \gamma (k)=\sum _{j=1}^{p}a_{j}\gamma (k-j)~~~~~r(k)=\sum _{j=1}^{p}a_{j}r(k-j)}   В простейшем случае AR(1)-процесса, математическое ожидание равно     μ = c  /  ( 1 − a )   {\displaystyle \mu =c/(1-a)}  , дисперсия     γ ( 0 ) =  σ  ε   2    /  ( 1 −  a  2   )   {\displaystyle \gamma (0)=\sigma _{\varepsilon }^{2}/(1-a^{2})}  , а автокорреляции     r ( k ) = r ⋅ r ( k − 1 )   ⇒   r ( k ) =  r  k     {\displaystyle r(k)=r\cdot r(k-1)~\Rightarrow ~r(k)=r^{k}}  . В общем случае выражение для математического ожидания через параметры модели было указано выше, однако, выражение для дисперсии временного ряда — существенно усложняется. Можно показать, что дисперсия ряда     γ ( 0 )   {\displaystyle \gamma (0)}   и вектор автоковариаций     γ   {\displaystyle \gamma }   выражаются через параметры следующим образом:     γ ( 0 ) = ( 1 +  a  T   ( C − a  a  T    )  − 1   a )  σ  ε   2     {\displaystyle \gamma (0)=(1+a^{T}(C-aa^{T})^{-1}a)\sigma _{\varepsilon }^{2}}  ,     γ =  σ  ε   2   ( C − a  a  T    )  − 1   a   {\displaystyle \gamma =\sigma _{\varepsilon }^{2}(C-aa^{T})^{-1}a}   где     a   {\displaystyle a}  -вектор параметров,     C   {\displaystyle C}  -матрица порядка     p   {\displaystyle p}  , элементы которой определяются следующим образом. Диагональные элементы равны      c  i i   = 1 −  a  2 i     {\displaystyle c_{ii}=1-a_{2i}}  . Элементы выше диагонали равны     −  a  2 i + j − 1     {\displaystyle -a_{2i+j-1}}  , а элементы ниже диагонали равны     − (  a  j   +  a  2 i − j   )   {\displaystyle -(a_{j}+a_{2i-j})}  . Здесь подразумевается, что если индекс превышает порядок модели     p   {\displaystyle p}  , то соответствующая величина приравнивается к нулю. В частности, для AR(1)-процесса матрица     C   {\displaystyle C}   равна просто единице, следовательно,     γ ( 0 ) = ( 1 +    a  2    1 −  a  2      )  σ  ε   2     {\displaystyle \gamma (0)=(1+{\frac {a^{2}}{1-a^{2}}})\sigma _{\varepsilon }^{2}}  , что соответствует вышеуказанной формуле. Для     A R ( 2 )   {\displaystyle AR(2)}  -процесса матрица     C   {\displaystyle C}   — второго порядка определяется следующим образом: первая строка равна (    1 −  a  2     {\displaystyle 1-a_{2}}  ;0), вторая — (    −  a  1     {\displaystyle -a_{1}}  ;1). Применив вышеуказанную формулу можно получить следующее выражение для дисперсии данного процесса:     γ ( 0 ) =    ( 1 −  a  2   )  σ  ε   2     ( 1 +  a  2   ) ( ( 1 −  a  2    )  2   −  a  1   2   )      {\displaystyle \gamma (0)={\frac {(1-a_{2})\sigma _{\varepsilon }^{2}}{(1+a_{2})((1-a_{2})^{2}-a_{1}^{2})}}}   На практике формулы для дисперсии процесса, выраженной через параметры модели обычно не применяются, а используется следующее выражение через ковариации:     γ ( 0 ) =  σ  ε   2   +  ∑  k = 1   p    a  k   γ ( k )   {\displaystyle \gamma (0)=\sigma _{\varepsilon }^{2}+\sum _{k=1}^{p}a_{k}\gamma (k)}   Автокорреляционная функция авторегрессионого процесса экспоненциально затухает с возможной осцилляцией (осцилляции зависят от наличия комплексных корней у характеристического полинома). При этом частная автокорреляционная функция при k>p равна нулю. Это свойство используется для идентификации порядка AR-модели по выборочной частной автокорреляционной функции временного ряда. Для AR(1)-процесса автокорреляционная функция — экспоненциально затухающая функция (без осцилляций), если выполнено условие стационарности. Частная автокорреляционная функция первого порядка равна r, а для более высоких порядков равна 0.  Оценка параметров модели[править | править код] Учитывая чётность автокорреляционной функции и используя рекуррентное соотношение для первых p автокорреляций, получаем систему уравнений Юла — Уокера[2]:     1 ⩽ k ⩽ p   ,    ∑  j = 1   p    a  j   r (  |  k − j  |  ) = r ( k )   {\displaystyle 1\leqslant k\leqslant p~,~\sum _{j=1}^{p}a_{j}r(|k-j|)=r(k)}   или в матричной форме     R a = r   ,   ⇒ a =  R  − 1   r   ,     R =   (    1    r  1      r  2     . . .    r  p − 1        r  1     1    r  1     . . .    r  p − 2        r  2      r  1     1   . . .    r  p − 3       . . .      r  p − 1      r  p − 2      r  p − 3     . . .   1    )     {\displaystyle Ra=r~,~\Rightarrow a=R^{-1}r~,~~R={\begin{pmatrix}1&r_{1}&r_{2}&...&r_{p-1}\\r_{1}&1&r_{1}&...&r_{p-2}\\r_{2}&r_{1}&1&...&r_{p-3}\\...\\r_{p-1}&r_{p-2}&r_{p-3}&...&1\\\end{pmatrix}}}   Если использовать вместо истинных автокорреляций (неизвестных) выборочные автокорреляции, получим оценки неизвестных коэффициентов авторегрессии. Можно показать, что этот метод оценки эквивалентен обычному методу наименьших квадратов (МНК). Если случайные ошибки модели имеют нормальное распределение, то данный метод также эквивалентен условному методу максимального правдоподобия. Для получения более точных оценок в последнем случае можно использовать полный метод максимального правдоподобия, в котором используется информация о распределении первых членов ряда. Например, в случае AR(1)-процесса распределение первого члена принимается равным безусловному распределению временного ряда (нормальное распределение с математическим ожиданием и безусловной дисперсией ряда).  Сезонные модели авторегрессии[править | править код] С помощью AR-моделей можно моделировать сезонность. Такие модели обозначают SAR (Seasonal AR). Например, при наличии квартальных данных и предположении о квартальной сезонности можно построить следующую модель SAR(4):      y  t   =  a  4    y  t − 4   +  ε  t     {\displaystyle y_{t}=a_{4}y_{t-4}+\varepsilon _{t}}   Фактически это обычная AR-модель с ограничением на параметры модели (равенство нулю параметров при лагах менее 4). На практике сезонность может сочетаться с обычной авторегрессией, например:      y  t   =  a  1    y  t − 1   +  a  4    y  t − 4   +  ε  t     {\displaystyle y_{t}=a_{1}y_{t-1}+a_{4}y_{t-4}+\varepsilon _{t}}   В некоторых случаях оказываются полезными сезонные модели, у которых случайная ошибка подчиняется некоторому AR-процессу:      y  t   =  a  4    y  t − 4   +  ε  t     ,    ε  t   =  a  1    ε  t − 1   +  u  t     {\displaystyle y_{t}=a_{4}y_{t-4}+\varepsilon _{t}~,~\varepsilon _{t}=a_{1}\varepsilon _{t-1}+u_{t}}   Нетрудно увидеть, что такую модель в операторной форме можно записать как:     ( 1 −  a  1   L ) ( 1 −  a  4    L  4   )  y  t   =  u  t     {\displaystyle (1-a_{1}L)(1-a_{4}L^{4})y_{t}=u_{t}}   Такую модель обозначают     A R ( 1 ) × S A R ( 4 )   {\displaystyle AR(1)\times SAR(4)}  .  См. также[править | править код] Модель авторегрессии и распределённого лага Модель авторегрессии — скользящего среднего Модель скользящего среднего Векторная авторегрессия Лаговый оператор Примечания[править | править код]   ↑ Разностное уравнение и рекуррентная последовательность (неопр.).  ↑ Марковские последовательности (неопр.).      