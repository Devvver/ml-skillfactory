Автокодировщик Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning      Архитектура автокодировщика: при обучении стремятся получить выходной вектор x' наиболее близким к входному вектору x Автокодировщик (англ. autoencoder, также — автоассоциатор)[1] — специальная архитектура искусственных нейронных сетей, позволяющая применять обучение без учителя[2] при использовании метода обратного распространения ошибки. Простейшая архитектура автокодировщика — сеть прямого распространения, без обратных связей, наиболее схожая с перцептроном и содержащая входной слой, промежуточный слой и выходной слой. В отличие от перцептрона, выходной слой автокодировщика должен содержать столько же нейронов, сколько и входной слой. Основной принцип работы и обучения сети автокодировщика — получить на выходном слое отклик, наиболее близкий к входному. Чтобы решение не оказалось тривиальным, на промежуточный слой автокодировщика накладывают ограничения: промежуточный слой должен быть или меньшей размерности, чем входной и выходной слои, или искусственно ограничивается количество одновременно активных нейронов промежуточного слоя — разрежённая активация. Эти ограничения заставляют нейросеть искать обобщения и корреляцию в поступающих на вход данных, выполнять их сжатие. Таким образом, нейросеть автоматически обучается выделять из входных данных общие признаки, которые кодируются в значениях весов сети. Так, при обучении сети на наборе различных входных изображений, нейросеть может самостоятельно обучиться распознавать линии и полосы под различными углами.   Предобучение многослойного персептрона без учителя с помощью автокодировщиков. Чаще всего автокодировщики применяют каскадно для обучения глубоких (многослойных) сетей. Автокодировщики применяют для предварительного обучения глубокой сети без учителя. Для этого слои обучаются друг за другом, начиная с первых. К каждому новому необученному слою на время обучения подключается дополнительный выходной слой, дополняющий сеть до архитектуры автокодировщика, после чего на вход сети подается набор данных для обучения. Веса необученного слоя и дополнительного слоя автокодировщика обучаются при помощи метода обратного распространения ошибки. Затем слой автокодировщика отключается и создается новый, соответствующий следующему необученному слою сети. На вход сети снова подается тот же набор данных, обученные первые слои сети остаются без изменений и работают в качестве входных для очередного обучаемого автокодировщика слоя. Так обучение продолжается для всех слоев сети за исключением последних. Последние слои сети обычно обучаются без использования автокодировщика при помощи того же метода обратного распространения ошибки и на маркированных данных (обучение с учителем).  Применения автокодировщика[править | править код] В последнее время автокодировщики мало используются для описанного «жадного» послойного предобучения глубоких нейронных сетей. После того, как этот метод был предложен в 2006 г Джеффри Хинтоном и Русланом Салахутдиновым[3][4], достаточно быстро оказалось, что новых методов инициализации случайными весами оказывается достаточно для дальнейшего обучения глубоких сетей[5]. Предложенная в 2014 г.[6] пакетная нормализация позволила обучать ещё более глубокие сети, предложенный же в конце 2015 г.[7] метод остаточного обучения позволил обучать сети произвольной глубины[5]. Основными практическими приложениями автокодировщиков остаются уменьшение шума в данных, а также уменьшение размерности многомерных данных для визуализации. С определёнными оговорками, касающимися размерности и разрежённости данных, автокодировщики могут позволять получать проекции многомерных данных, которые оказываются лучше тех, что даёт метод главных компонент либо какой-либо другой классический метод[5].  Примечания[править | править код]   ↑ Autoencoder for Words, Liou, C.-Y., Cheng, C.-W., Liou, J.-W., and Liou, D.-R., Neurocomputing, Volume 139, 84-96 (2014), DOI:10.1016/j.neucom.2013.09.055  ↑ Обучение многослойного разреженного автокодировщика на изображениях большого масштаба, Хуршудов А. А., Вестник компьютерных и информационных технологий 02.2014 DOI:10.14489/vkit.2014.02.pp.027-030  ↑ G. E. Hinton, R. R. Salakhutdinov. Reducing the Dimensionality of Data with Neural Networks (англ.) // Science. — 2006-07-28. — Vol. 313, iss. 5786. — P. 504–507. — ISSN 1095-9203 0036-8075, 1095-9203. — DOI:10.1126/science.1127647.  ↑ Why does unsupervised pre-training help deep learning?.  ↑ 1 2 3 Building Autoencoders in Keras (неопр.).  blog.keras.io. Проверено 25 июня 2016.  ↑ Sergey Ioffe, Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift // arXiv:1502.03167 [cs]. — 2015-02-10.  ↑ Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition // arXiv:1512.03385 [cs]. — 2015-12-10.   Ссылки[править | править код] Ng Andrew Y. CS294A. Lecture Notes: Sparse Autoencoder.. stanford wiki: Stacked Autoencoders Типы искусственных нейронных сетей Сеть прямого распространения[en] (Сеть радиально-базисных функций) Однослойный перцептрон Многослойный перцептрон (Розенблата • Румельхарта) Сеть Хопфилда Цепь Маркова Машина Больцмана Ограниченная машина Больцмана Автокодировщик (Шумоподавляющий автокодировщик[en] • Разреженный автокодировщик[en] • Вариационный автокодировщик[en]) Глубокая сеть доверия Свёрточная нейронная сеть Глубинная свёрточная нейронная сеть Развёртывающая нейронная сеть Глубинная свёрточная обратная графическая сеть Генеративно-состязательная сеть Рекуррентная нейронная сеть Рекурсивные нейронные сети Долгая краткосрочная память Управляемый рекуррентный блок Нейронные машины Тьюринга[en] Двунаправленная сеть (Двунаправленная рекуррентная нейросеть[en] • Двунаправленная сеть с долгой краткосрочной памятью • Двунаправленные управляемые рекуррентные нейроны) Глубинная остаточная сеть Нейронная эхо-сеть[en] Метод экстремального обучения[en] Метод неустойчивых состояний[en] Метод опорных векторов Сеть Кохонена Самоорганизующаяся карта Кохонена Капсульная нейронная сеть Ассоциативная память на нейронных сетях     