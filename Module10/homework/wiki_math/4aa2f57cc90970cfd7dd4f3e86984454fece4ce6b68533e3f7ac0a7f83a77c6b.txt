Метод k-ближайших соседей Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning        Пример классификации k-ближайших соседей. Тестовый образец (зелёный круг) должен быть классифицирован как синий квадрат (класс 1) или как красный треугольник (класс 2). Если k = 3, то он классифицируется как 2-й класс, потому что внутри меньшего круга 2 треугольника и только 1 квадрат. Если k = 5, то он будет классифицирован как 1-й класс (3 квадрата против 2 треугольников внутри большего круга) Метод k-ближайших соседей (англ. k-nearest neighbors algorithm, k-NN) — метрический алгоритм для автоматической классификации объектов или регрессии.  В случае использования метода для классификации объект присваивается тому классу, который является наиболее распространённым среди     k   {\displaystyle k}   соседей данного элемента, классы которых уже известны. В случае использования метода для регрессии, объекту присваивается среднее значение по     k   {\displaystyle k}   ближайшим к нему объектам, значения которых уже известны. Содержание  1 Определение дистанции  1.1 Нормализация 1.2 Выделение значимых атрибутов   2 Взвешенный способ  2.1 Определение класса 2.2 Определение непрерывной величины   3 Ссылки   Определение дистанции[править | править код] Алгоритм может быть применим к выборкам с большим количеством атрибутов (многомерным). Для этого перед применением нужно определить функцию дистанции. Классический вариант определения дистанции — дистанция в евклидовом пространстве.  Нормализация[править | править код] Однако разные атрибуты могут иметь разный диапазон представленных значений в выборке (например атрибут А представлен в диапазоне от 0.1 до 0.5, а атрибут Б представлен в диапазоне от 1000 до 5000), то значения дистанции могут сильно зависеть от атрибутов с бо́льшими диапазонами. Поэтому данные обычно подлежат нормализации. При кластерном анализе есть два основных способа нормализации данных:  Мини-макс нормализация.      x ′  = ( x − M I N [ X ] )  /  ( M A X [ X ] − M I N [ X ] )   {\displaystyle x'=(x-MIN[X])/(MAX[X]-MIN[X])}   В этом случае все значения будут лежать в диапазоне от 0 до 1. Дискретные бинарные значения определяются как 0 и 1.  Z-нормализация.      x ′  = ( x − M [ X ] )  /  σ [ X ]   {\displaystyle x'=(x-M[X])/\sigma [X]}   где σ — среднеквадратическое отклонение. В этом случае большинство значений попадет в диапазон (-3σ;3σ)  Выделение значимых атрибутов[править | править код] Некоторые значимые атрибуты могут быть важнее остальных, поэтому для каждого атрибута может быть задан в соответствие определенный вес (например вычисленный с помощью тестовой выборки и оптимизации ошибки отклонения). Таким образом, каждому атрибуту     k   {\displaystyle k}   будет задан в соответствие вес      z  k     {\displaystyle z_{k}}  , так что значение атрибута будет попадать в диапазон     [ 0 ;  z  k   m a x ( k ) ]   {\displaystyle [0;z_{k}max(k)]}   (для нормализованных значений по методу мини-макс). Например, если атрибуту присвоен вес 2.7, то его нормализованно-взвешенное значение будет лежать в диапазоне     [ 0 ; 2.7 ]   {\displaystyle [0;2.7]}    Взвешенный способ[править | править код] Определение класса[править | править код] При таком способе во внимание принимается не только количество попавших в область определенных классов, но и их удаленность от нового значения. Для каждого класса j определяется оценка близости:      Q  j   =  ∑  i = 1   n     1  d ( x ,  a  i    )  2        {\displaystyle Q_{j}=\sum _{i=1}^{n}{\frac {1}{d(x,a_{i})^{2}}}}  , где d(x, a) — дистанция от нового значения x до объекта а. У какого класса выше значение близости, тот класс и присваивается новому объекту.  Определение непрерывной величины[править | править код] С помощью этого метода можно вычислять значение одного из атрибутов классифицируемого объекта на основании дистанций от попавших в область объектов и соответствующих значений этого же атрибута у объектов.      x  k   =     ∑  i = 1   n     k  i   d ( x ,  a  i    )  2       ∑  i = 1   n    d ( x ,  a  i    )  2         {\displaystyle x_{k}={\frac {\sum _{i=1}^{n}{k_{i}d(x,a_{i})^{2}}}{\sum _{i=1}^{n}{d(x,a_{i})^{2}}}}}  , где      a  i     {\displaystyle a_{i}}   — i-ый объект, попавший в область      k  i     {\displaystyle k_{i}}   — это значение атрибута     k   {\displaystyle k}   у заданного объекта      a  i     {\displaystyle a_{i}}   (примечание для корректоров: было бы неплохо заменить      k  i     {\displaystyle k_{i}}   на значение вложенного индекса (a_k_i))     x   {\displaystyle x}   — новый объект      x  k     {\displaystyle x_{k}}   — k-ый атрибут нового объекта  Ссылки[править | править код] kNN и Потенциальная энергия (апплет), Е. М. Миркес и университет Лейстера. Апплет позволяет сравнивать два метода классификации. Daniel T. Larose, Discovering Knowledge in Data: An Introduction to Data Mining (http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471666572.html)    