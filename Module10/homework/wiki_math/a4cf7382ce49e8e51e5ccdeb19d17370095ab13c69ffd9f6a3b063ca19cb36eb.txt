Размерность Вапника — Червоненкиса Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning     Размерность Вапника — Червоненкиса или VC-размерность — это характеристика семейства алгоритмов для решения задачи классификации с двумя классами, характеризующая сложность или ёмкость этого семейства. Это одно из ключевых понятий в теории Вапника-Червоненкиса о статистическом машинном обучении, названное в честь Владимира Вапника и Алексея Червоненкиса. Сами Вапник и Червоненкис предпочитают называть эту величину комбинаторной размерностью, так как выяснилось, она была известна алгебраистам еще до открытия их теории машинного обучения.  Содержание  1 Определение 2 Примеры 3 См. также 4 Ссылки 5 Примечания   Определение[править | править код] Пусть задано множество     X   {\displaystyle X}   и некоторое семейство индикаторных функций (алгоритмов классификации, решающих правил)       F   = { f ( x , α ) }   {\displaystyle {\mathcal {F}}=\{f(x,\alpha )\}}  , где     x ∈ X   {\displaystyle x\in X}   — аргумент функций,     α   {\displaystyle \alpha }   — вектор параметров, задающий функцию. Каждая такая функция     f ( x , α )   {\displaystyle f(x,\alpha )}   сопоставляет каждому элементу множества     X   {\displaystyle X}   один из двух заданных классов. VC-размерностью семейства       F     {\displaystyle {\mathcal {F}}}   называется наибольшее число     h   {\displaystyle h}  , такое, что существует подмножество из     h   {\displaystyle h}   элементов множества     X   {\displaystyle X}  , которые функции из       F     {\displaystyle {\mathcal {F}}}   могут разбить на два класса всеми возможными способами. Если же такие подмножества существуют для сколь угодно большого     h   {\displaystyle h}  , то VC-размерность полагается равной бесконечности. VC-размерность можно обобщить и на случай семейства функций     { g ( x , α ) }   {\displaystyle \{g(x,\alpha )\}}  , принимающих действительные значения. Его VC-размерность определяется как VC-размерность семейства индикаторных функций     { I ( g ( x , α ) > β ) }   {\displaystyle \{I(g(x,\alpha )>\beta )\}}  , где     β   {\displaystyle \beta }   пробегает область значений функций     g   {\displaystyle g}  .[1]  Примеры[править | править код] Как пример, рассмотрим задачу о разбиении точек на плоскости на два класса прямой линией — это так называемый линейный классификатор. Множество из любых трёх точек, не лежащих на одной прямой, может быть разделено прямой линией на два класса всеми возможными способами (     2  3   = 8   {\displaystyle 2^{3}=8}   способами, на рисунке ниже показаны три из них), но множества из четырёх и более точек — уже нет. Поэтому VC-размерность линейного классификатора на плоскости равна трём.             Примеры разделения трёхточек на два класса  Разделение невозможнодля этих четырёх точек  В общем случае, VC-размерность линейных классификаторов в     n   {\displaystyle n}  -мерном пространстве равна     n + 1   {\displaystyle n+1}  .  См. также[править | править код] Метод опорных векторов Ссылки[править | править код] Информация с сайта www.machinelearning.ru Примечания[править | править код]   ↑ Hastie, T., Tibshirani R., Friedman J. Chapter 7.9. Vapnik–Chervonenkis Dimension // The Elements of Statistical Learning: Data Mining, Inference, and Prediction. — 2nd ed. — Springer-Verlag, 2009. — 746 p. — ISBN 978-0-387-84857-0..      