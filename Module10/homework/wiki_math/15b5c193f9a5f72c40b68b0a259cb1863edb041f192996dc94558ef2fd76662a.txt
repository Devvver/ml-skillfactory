EM-алгоритм Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning     EM-алгоритм (англ. Expectation-maximization (EM) algorithm) — алгоритм, используемый в математической статистике для нахождения оценок максимального правдоподобия параметров вероятностных моделей, в случае, когда модель зависит от некоторых скрытых переменных. Каждая итерация алгоритма состоит из двух шагов. На E-шаге (expectation) вычисляется ожидаемое значение функции правдоподобия, при этом скрытые переменные рассматриваются как наблюдаемые. На M-шаге (maximization) вычисляется оценка максимального правдоподобия, таким образом увеличивается ожидаемое правдоподобие, вычисляемое на E-шаге. Затем это значение используется для E-шага на следующей итерации. Алгоритм выполняется до сходимости. Часто EM-алгоритм используют для разделения смеси гауссиан.  Содержание  1 Описание алгоритма 2 Альтернативное описание 3 Примеры использования 4 Примечания 5 Ссылки   Описание алгоритма[править | править код] Пусть       X     {\displaystyle {\textbf {X}}}   — некоторые из значений наблюдаемых переменных, а       T     {\displaystyle {\textbf {T}}}   — скрытые переменные. Вместе       X     {\displaystyle {\textbf {X}}}   и       T     {\displaystyle {\textbf {T}}}   образуют полный набор данных. Вообще,       T     {\displaystyle {\textbf {T}}}   может быть некоторой подсказкой, которая облегчает решение проблемы в случае, если она известна. Например, если имеется смесь распределений, функция правдоподобия легко выражается через параметры отдельных распределений смеси. Положим     p   {\displaystyle p}   — плотность вероятности (в непрерывном случае) или функция вероятности (в дискретном случае) полного набора данных с параметрами     Θ   {\displaystyle \Theta }  :     p (  X  ,  T   |  Θ ) .   {\displaystyle p(\mathbf {X} ,\mathbf {T} |\Theta ).}   Эту функцию можно понимать как правдоподобие всей модели, если рассматривать её как функцию параметров     Θ   {\displaystyle \Theta }  . Заметим, что условное распределение скрытой компоненты при некотором наблюдении и фиксированном наборе параметров может быть выражено так:      p (  T   |   X  , Θ ) =    p (  X  ,  T   |  Θ )   p (  X   |  Θ )    =    p (  X   |   T  , Θ ) p (  T   |  Θ )   ∫ p (  X   |     T ^    , Θ ) p (    T ^     |  Θ ) d    T ^         {\displaystyle p(\mathbf {T} |\mathbf {X} ,\Theta )={\frac {p(\mathbf {X} ,\mathbf {T} |\Theta )}{p(\mathbf {X} |\Theta )}}={\frac {p(\mathbf {X} |\mathbf {T} ,\Theta )p(\mathbf {T} |\Theta )}{\int p(\mathbf {X} |\mathbf {\hat {T}} ,\Theta )p(\mathbf {\hat {T}} |\Theta )d\mathbf {\hat {T}} }}}  , используя расширенную формулу Байеса и формулу полной вероятности. Таким образом, нам необходимо знать только распределение наблюдаемой компоненты при фиксированной скрытой     p (  X   |   T  , Θ )   {\displaystyle p(\mathbf {X} |\mathbf {T} ,\Theta )}   и вероятности скрытых данных     p (  T   |  Θ )   {\displaystyle p(\mathbf {T} |\Theta )}  . EM-алгоритм итеративно улучшает начальную оценку      Θ  0     {\displaystyle \Theta _{0}}  , вычисляя новые значения оценок      Θ  1   ,  Θ  2   ,   {\displaystyle \Theta _{1},\Theta _{2},}   и так далее. На каждом шаге переход к      Θ  n + 1     {\displaystyle \Theta _{n+1}}   от      Θ  n     {\displaystyle \Theta _{n}}   выполняется следующим образом:       Θ  n + 1   = arg ⁡  max  Θ   Q ( Θ )   {\displaystyle \Theta _{n+1}=\arg \max _{\Theta }Q(\Theta )}   где     Q ( Θ )   {\displaystyle Q(\Theta )}   — матожидание логарифма правдоподобия. Другими словами, мы не можем сразу вычислить точное правдоподобие, но по известным данным (    X   {\displaystyle X}  ) мы можем найти апостериорную оценку вероятностей для различных значений скрытых переменных     T   {\displaystyle T}  . Для каждого набора значений     T   {\displaystyle T}   и параметров     Θ   {\displaystyle \Theta }   мы можем вычислить матожидание функции правдоподобия по данному набору     X   {\displaystyle X}  . Оно зависит от предыдущего значения     Θ   {\displaystyle \Theta }  , потому что это значение влияет на вероятности скрытых переменных     T   {\displaystyle T}  .     Q ( Θ )   {\displaystyle Q(\Theta )}   вычисляется следующим образом:      Q ( Θ ) =  E   T       [  log ⁡ p  (   X  ,  T    |   Θ  )    |    X   ]    {\displaystyle Q(\Theta )=E_{\mathbf {T} }\!\!\left[\log p\left(\mathbf {X} ,\mathbf {T} \,|\,\Theta \right){\Big |}\mathbf {X} \right]}   то есть это условное матожидание     log ⁡ p  (   X  ,  T    |   Θ  )    {\displaystyle \log p\left(\mathbf {X} ,\mathbf {T} \,|\,\Theta \right)}   при условии      X    {\displaystyle \mathbf {X} }  . Другими словами,      Θ  n + 1     {\displaystyle \Theta _{n+1}}   — это значение, максимизирующее (M) условное матожидание (E) логарифма правдоподобия при данных значениях наблюдаемых переменных и предыдущем значении параметров. В непрерывном случае значение     Q ( Θ )   {\displaystyle Q(\Theta )}   вычисляется так:      Q ( Θ ) =  E   T       [  log ⁡ p  (   X  ,  T    |   Θ  )    |    X   ]  =  ∫  − ∞   ∞   p  (   T    |    X  ,  Θ  n    )  log ⁡ p  (   X  ,  T    |   Θ  )  d  T    {\displaystyle Q(\Theta )=E_{\mathbf {T} }\!\!\left[\log p\left(\mathbf {X} ,\mathbf {T} \,|\,\Theta \right){\Big |}\mathbf {X} \right]=\int _{-\infty }^{\infty }p\left(\mathbf {T} \,|\,\mathbf {X} ,\Theta _{n}\right)\log p\left(\mathbf {X} ,\mathbf {T} \,|\,\Theta \right)d\mathbf {T} }   Альтернативное описание[править | править код] При определенных обстоятельствах удобно рассматривать EM-алгоритм как два чередующихся шага максимизации.[1][2] Рассмотрим функцию:      F ( q , θ ) =  E  q   ⁡ [ log ⁡ L ( θ ; x , Z ) ] + H ( q ) = −  D  KL     (   q   ‖    p  Z  |  X   ( ⋅  |  x ; θ )   )   + log ⁡ L ( θ ; x )   {\displaystyle F(q,\theta )=\operatorname {E} _{q}[\log L(\theta ;x,Z)]+H(q)=-D_{\text{KL}}{\big (}q{\big \|}p_{Z|X}(\cdot |x;\theta ){\big )}+\log L(\theta ;x)}   где q — распределение вероятностей ненаблюдаемых переменных Z; pZ|X(· |x;θ) — условное распределение ненаблюдаемых переменных при фиксированных наблюдаемых x и параметрах θ; H — энтропия и DKL — расстояние Кульбака-Лейблера. Тогда шаги EM-алгоритма можно представить как:  E(xpectation) шаг: Выбираем q, чтобы максимизировать F:      q  ( t )   =  *  ⁡   arg  max   q     F ( q ,  θ  ( t )   )   {\displaystyle q^{(t)}=\operatorname {*} {\arg \,\max }_{q}\ F(q,\theta ^{(t)})}   M(aximization) шаг: Выбираем θ, чтобы максимизировать F:      θ  ( t + 1 )   =  *  ⁡   arg  max   θ     F (  q  ( t )   , θ )   {\displaystyle \theta ^{(t+1)}=\operatorname {*} {\arg \,\max }_{\theta }\ F(q^{(t)},\theta )}   Примеры использования[править | править код] k-means — алгоритм кластеризации, построенный на идее EM-алгоритма Метод упругих карт для нелинейного сокращения размерности данных Алгоритм Баума-Велша — алгоритм для оценки параметров скрытых марковских моделей Примечания[править | править код]   ↑ Neal, Radford; Hinton, Geoffrey (1999).  Michael I. Jordan, ed. “A view of the EM algorithm that justifies incremental, sparse, and other variants” (PDF). Learning in Graphical Models. Cambridge, MA: MIT Press: 355—368. ISBN 0262600323. Проверено 2009-03-22..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output q{quotes:"\"""\"""'""'"}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-limited a,.mw-parser-output .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}  ↑ Hastie, Trevor. 8.5 The EM algorithm // The Elements of Statistical Learning / Trevor Hastie, Tibshirani, Friedman. — New York : Springer, 2001. — P. 236–243.   Ссылки[править | править код] Демонстрация разделения смеси гауссиан с помощью EM-алгоритма Реализация на Java    