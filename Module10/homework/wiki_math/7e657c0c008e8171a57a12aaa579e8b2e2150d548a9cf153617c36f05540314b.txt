Тематическое моделирование  Построение тематической модели документа: :     p ( w  |  t )   {\displaystyle p(w|t)}   — матрица искомых условных распределений слов по темам :     p ( t  |  d )   {\displaystyle p(t|d)}   матрица искомых условных распределений тем по документам :     d   {\displaystyle d}   — документ :     w   {\displaystyle w}   — слово :     d , w   {\displaystyle d,w}   — наблюдаемые переменные :     t   {\displaystyle t}   — тема (скрытая переменная) Тематическое моделирование — способ построения модели коллекции текстовых документов, которая определяет, к каким темам относится каждый из документов[1]. Тематическая модель (англ. topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова (термины) образуют каждую тему[2]. Переход из пространства терминов в пространство найденных тематик помогает разрешать синонимию и полисемию терминов, а также эффективнее решать такие задачи, как тематический поиск, классификация, суммаризация и аннотация коллекций документов и новостных потоков. Тематическое моделирование как вид статистических моделей для нахождения скрытых тем встреченных в коллекции документов, нашло своё применение в таких областях как машинное обучение и обработка естественного языка. Исследователи используют различные тематические модели для анализа текстов, текстовых архивов документов, для анализа изменения тем в наборах документов.mw-parser-output .ts-Переход img{margin-left:.285714em}[⇨]. Интуитивно понимая, что документ относится к определенной теме, в документах посвященных одной теме можно встретить некоторые слова чаще других. Например: «собака» и «кость» встречаются чаще в документах про собак, «кошки» и «молоко» будут встречаться в документах о котятах, предлоги «и» и «в» будут встречаться в обеих тематиках. Обычно документ касается нескольких тем в разных пропорциях, таким образом, документ в котором 10 % темы составляют кошки, а 90 % темы про собак, можно предположить, что слов про собак в 9 раз больше. Тематическое моделирование отражает эту интуицию в математическую структуру, которая позволяет на основании изучения коллекции документов и исследования частотных характеристик слов в каждом документе, сделать вывод, что каждый документ это некоторый баланс тем. Наибольшее применение в современных приложениях находят подходы, основанные на Байесовских сетях — вероятностных моделях на ориентированных графах. Вероятностные тематические модели — это относительно молодая область исследований в теории самообучения[⇨]. Одним из первых был предложен вероятностный латентно-семантический анализ[⇨] (PLSA), основанный на принципе максимума правдоподобия, как альтернатива классическим методам кластеризации, основанным на вычислении функций расстояния. Вслед за PLSA был предложен метод латентного размещения Дирихле и его многочисленные обобщения[3][⇨]. Вероятностные тематические модели осуществляют «мягкую» кластеризацию, позволяя документу или термину относиться сразу к нескольким темам с различными вероятностями. Вероятностные тематические модели описывает каждую тему дискретным распределением на множестве терминов, каждый документ — дискретным распределением на множестве тем. Предполагается, что коллекция документов — это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке[4][⇨]. Хотя тематическое моделирование традиционно описывалось и применялось в обработке естественного языка, оно нашло своё применение и в других областях, например, таких как биоинформатика.  Содержание  1 История  1.1 Тематические исследования   2 Алгоритмы тематического моделирования  2.1 Вероятностный латентно-семантический анализ 2.2 Латентное размещение Дирихле   3 См. также 4 Примечания 5 Литература 6 Ссылки  6.1 Программное обеспечение и программные библиотеки     История[править | править код] Первое описание тематического моделирования появилось в работе Рагавана, Пападимитриу, Томаки и Вемполы 1998 году[5]. Томас Хофманн в 1999 году[6] предложил вероятностное скрытое семантическое индексирование (PLSI). Одна из самых распространенных тематических моделей — это латентное размещение Дирихле (LDA), эта модель является обобщением вероятностного семантического индексирования и разработана Дэвидом Блейем (англ. David Blei), Эндрю Ыном и Майклом Джорданом (англ. Michael I. Jordan) в 2002 году[7]. Другие тематические модели как правило являются расширением LDA, например, размещение патинко улучшает LDA за счёт введения дополнительных корреляционных коэффициентов для каждого слова, которое составляет тему.  Тематические исследования[править | править код] Тэмплтон сделал обзор работ по тематическому моделированию в гуманитарных науках, сгруппированных по синхронному и диахроническому подходу[8]. Синхронные подходы выделяют темы в некоторый момент времени, например, Джокерс с помощью тематической модели исследовал, о чём писали блогеры в День Цифровых Гуманитарных наук в 2010 году[9]. Диахронические подходы, включая определение Блока и Ньюмана о временной динамике тем в Пенсильванской газете 1728—1800 года[10]. Грифитс и Стейверс использовали тематическое моделирование для обзоров журнала PNAS, определяли изменения популярности тем с 1991 по 2001 год[11]. Блевин создал тематическую модель дневника Марты Балладс[12]. Мимно использовал тематическое моделирование для анализа 24 журналов по классической филологии и археологии за 150 лет, чтобы определить изменения популярности тем и узнать, насколько сильно изменились журналы за это время[13].  Алгоритмы тематического моделирования[править | править код] В работе Дэвида Блея «Введение в тематическое моделирование» рассмотрен наиболее популярный алгоритм Латентное размещение Дирихле[⇨][14]. На практике исследователи используют одну из эвристик метода максимального правдоподобия, методы сингулярного разложения (SVD), метод моментов, алгоритм, основанный на неотрицательной матрице факторизации (NMF), вероятностные тематические модели, вероятностный латентно-семантический анализ, латентное размещение Дирихле. В работе Воронцова К. В. рассмотрены вариации основных алгоритмов тематического моделирования: робастная тематическая модель, тематические модели классификации, динамические тематические модели, иерархические тематические модели, многоязычные тематические модели, модели текста как последовательности слов, многомодальные тематические модели [2]. Вероятностные тематические модели основаны на следующих предположениях [15] [16] [17] [18]:  Порядок документов в коллекции не имеет значения Порядок слов в документе не имеет значения, документ — мешок слов Слова, встречающиеся часто в большинстве документов, не важны для определения тематики Коллекцию документов можно представить как выборку пар документ-слово     ( d , w )   {\displaystyle (d,w)}   ,     d ∈ D   {\displaystyle d\in D}  ,     w ∈    W    d     {\displaystyle w\in {\mathit {W}}_{d}}   Каждая тема     t ∈ T   {\displaystyle t\in T}   описывается неизвестным распределением     p (   W    |  t )   {\displaystyle p({\mathit {W}}|t)}   на множестве слов     w ∈   W     {\displaystyle w\in {\mathit {W}}}   Каждый документ     d ∈ D   {\displaystyle d\in D}   описывается неизвестным распределением     p ( t  |  d )   {\displaystyle p(t|d)}   на множестве тем     t ∈ T   {\displaystyle t\in T}   Гипотеза условной независимости     p ( w  |  t , d ) = p ( w  |  t )   {\displaystyle p(w|t,d)=p(w|t)}   Построить тематическую модель — значит, найти матрицы     Φ =  |   |  p ( w  |  t )  |   |    {\displaystyle \Phi =||p(w|t)||}   и     Θ =  |   |  p ( t  |  d )  |   |    {\displaystyle \Theta =||p(t|d)||}   по коллекции       D     {\displaystyle {\mathit {D}}}   В более сложных вероятностных тематических моделях некоторые из этих предположений заменяются более реалистичными.  Вероятностный латентно-семантический анализ[править | править код]  Вероятностный латентно-семантический анализ (PLSA).     d   {\displaystyle d}   — документ,     w   {\displaystyle w}   — слово,     d , w   {\displaystyle d,w}   — наблюдаемые переменные,     t   {\displaystyle t}   — тема (скрытая переменная),     p ( d )   {\displaystyle p(d)}   — априорное распределение на множестве документов,     p ( w  |  t ) , p ( t  |  d )   {\displaystyle p(w|t),p(t|d)}   — искомые условные распределения,     D   {\displaystyle D}   — коллекция документов,     N   {\displaystyle N}   — длина документа в словах Вероятностный латентно-семантический анализ (PLSA) предложен Томасом Хофманном в 1999 году. Вероятностная модель появления пары «документ-слово» может быть записана тремя эквивалентными способами:      p ( d , w ) =  ∑  t ∈ T   p ( t ) p ( w  |  t ) p ( d  |  t ) =  ∑  t ∈ T   p ( d ) p ( w  |  t ) p ( t  |  d ) =  ∑  t ∈ T   p ( w ) p ( t  |  w ) p ( d  |  t )   {\displaystyle p(d,w)=\sum _{t\in T}p(t)p(w|t)p(d|t)=\sum _{t\in T}p(d)p(w|t)p(t|d)=\sum _{t\in T}p(w)p(t|w)p(d|t)}   где     T   {\displaystyle T}   — множество тем;      p ( t )   {\displaystyle p(t)}   — неизвестное априорное распределение тем во всей коллекции;     p ( d )   {\displaystyle p(d)}   — априорное распределение на множестве документов, эмпирическая оценка     p ( d ) =  n  d    /  n   {\displaystyle p(d)=n_{d}/n}   , где     n =  ∑  d    n  d     {\displaystyle n=\sum _{d}n_{d}}   — суммарная длина всех документов;     p ( w )   {\displaystyle p(w)}   — априорное распределение на множестве слов, эмпирическая оценка     p ( w ) =  n  w    /  n   {\displaystyle p(w)=n_{w}/n}  , где      n  w     {\displaystyle n_{w}}   — число вхождений слова     w   {\displaystyle w}   во все документы; Искомые условные распределения     p ( w  |  t ) , p ( t  |  d )   {\displaystyle p(w|t),p(t|d)}   выражаются через     p ( t  |  w ) , p ( d  |  t )   {\displaystyle p(t|w),p(d|t)}   по формуле Байеса:     p ( w  |  t ) =    p ( t  |  w ) p ( w )    ∑   w ′    p ( t  |   w ′  ) p (  w ′  )    ;  p ( t  |  d ) =    p ( d  |  t ) p ( t )    ∑   t ′    p ( d  |   t ′  ) p (  t ′  )    .   {\displaystyle p(w|t)={\frac {p(t|w)p(w)}{\sum _{w'}p(t|w')p(w')}};\qquad p(t|d)={\frac {p(d|t)p(t)}{\sum _{t'}p(d|t')p(t')}}.}   Для идентификации параметров тематической модели по коллекции документов применяется принцип максимума правдоподобия, который приводит к задаче минимизации функционала      ∑  d ∈ D    ∑  w ∈ d    n  d w   log ⁡ p ( d , w ) →  min  Φ , Θ   ,   {\displaystyle \sum _{d\in D}\sum _{w\in d}n_{dw}\log p(d,w)\to \min _{\Phi ,\Theta },}   при ограничениях нормировки      ∑  w   p ( w  |  t ) = 1 ,   ∑  t   p ( t  |  d ) = 1 ,   ∑  t   p ( t ) = 1 ,   {\displaystyle \sum _{w}p(w|t)=1,\;\sum _{t}p(t|d)=1,\;\sum _{t}p(t)=1,}   где      n  d w     {\displaystyle n_{dw}}   — число вхождений слова     w   {\displaystyle w}   в документ     d   {\displaystyle d}  . Для решения данной оптимизационной задачи обычно применяется EM-алгоритм. Основные недостатки PLSA:  Число параметров растёт линейно по числу документов в коллекции, что может приводить к переобучению модели. При добавлении нового документа     d   {\displaystyle d}   в коллекцию распределение     p ( t  |  d )   {\displaystyle p(t|d)}   невозможно вычислить по тем же формулам, что и для остальных документов, не перестраивая всю модель заново. Латентное размещение Дирихле[править | править код]  Латентное размещение Дирихле LDA. :    w   {\displaystyle w}   — слово (наблюдаемая переменная) :    t   {\displaystyle t}   — тема (скрытая переменная) :    D   {\displaystyle D}   — коллекция документов :    N   {\displaystyle N}   — длина документа в словах :    K   {\displaystyle K}   — количество тем в коллекции :    θ   {\displaystyle \theta }   — распределение тем в документе :    ϕ   {\displaystyle \phi }   — распределение слов в теме Метод латентного размещения Дирихле (LDA) предложен Дэвидом Блеем в 2003 году. В этом методе устранены основные недостатки PLSA. Метод LDA основан на той же вероятностной модели     p ( d , w ) =  ∑  t ∈ T   p ( d ) p ( w  |  t ) p ( t  |  d ) ,   {\displaystyle p(d,w)=\sum _{t\in T}p(d)p(w|t)p(t|d),}   при дополнительных предположениях:  вектора документов      θ  d   =   (   p ( t  |  d ) : t ∈ T   )     {\displaystyle \theta _{d}={\bigl (}p(t|d):t\in T{\bigr )}}   порождаются одним и тем же вероятностным распределением на нормированных      |  T  |    {\displaystyle |T|}  -мерных векторах; это распределение удобно взять из параметрического семейства распределений Дирихле      D i r  ( θ , α ) ,  α ∈   R    |  T  |      {\displaystyle \mathrm {Dir} (\theta ,\alpha ),\;\alpha \in \mathbb {R} ^{|T|}}  ; вектора тем      ϕ  t   =   (   p ( w  |  t ) : w ∈ W   )     {\displaystyle \phi _{t}={\bigl (}p(w|t):w\in W{\bigr )}}   порождаются одним и тем же вероятностным распределением на нормированных векторах размерности      |  W  |    {\displaystyle |W|}  ; это распределение удобно взять из параметрического семейства распределений Дирихле      D i r  ( θ , β ) ,  β ∈   R    |  W  |      {\displaystyle \mathrm {Dir} (\theta ,\beta ),\;\beta \in \mathbb {R} ^{|W|}}  . Для идентификации параметров модели LDA по коллекции документов применяется семплирование Гиббса, вариационный байесовский вывод или метод распространения ожидания (англ.)русск. (Expectation propagation).  См. также[править | править код] Explicit semantic analysis (англ.) Иерархический процесс Дирихле (англ.) Примечания[править | править код]   ↑ Коршунов, 2012.  ↑ 1 2 Воронцов, 2013.  ↑ Ali10, 2010.  ↑ Воронцов12, 2012.  ↑ Пападимитриу, 1998.  ↑ Хофманн, 1999.  ↑ Блей2003, 2003.  ↑ Тэмплтон, 2011.  ↑ Джокерс, 2010.  ↑ НьюманБлок, 2006.  ↑ Грифитс, 2004.  ↑ Блевин, 2010.  ↑ Мимно, 2012.  ↑ Блей2012, 2012.  ↑ Коршунов, 2012, с. 229.  ↑ Воронцов, 2013, с. 6.  ↑ Воронцов13, 2013, с. 5.  ↑ ВоронцовМЛ, 2013, с. 5.   Литература[править | править код] Коршунов Антон, Гомзин Андрей. Тематическое моделирование текстов на естественном языке // Труды Института системного программирования РАН : журнал. — 2012. Воронцов К.В. Вероятностное тематическое моделирование // www.machinelearning.ru : web. — 2013. Воронцов К.В., Потапенко А.А. Регуляризация, робастность и разреженность вероятностных тематических моделей // Компьютерные исследования и моделирование : журнал. — 2012. — С. 693-706. Воронцов К.В. Аддитивная регуляризация вероятностных тематических моделей Презентация // www.machinelearning.ru : web. — 2013. Воронцов К.В. Вероятностные тематические модели коллекции текстовых документов Презентация // www.machinelearning.ru : web. — 2013. Марк Стейверс, Tom Griffiths. Вероятностная тематическая модель. // Справочник скрытого семантического анализа / T. Landauer, D. McNamara, S. Dennis, W. Kintsch. — Psychology Press, 2007. — ISBN 978-0-8058-5418-3. Daud Ali, Li Juanzi, Zhou Lizhu, Muhammad Faqir. Knowledge discovery through directed probabilistic topic models: a survey. In Proceedings of Frontiers of Computer Science in China. // www.researchgate.net : web. — 2010. Christos Papadimitriou, Prabhakar Raghavan, Hisao Tamaki, Santosh Vempala. Latent Semantic Indexing: A probabilistic analysis // Proceedings of ACM PODS. — 1998. Thomas Hoffman. Probabilistic Latent Semantic Indexing // Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval. — 1999. Архивировано 14 декабря 2010 года. David M. Blei, Andrew Y. Ng, Michael I. Jordan. Latent Dirichlet Allocation // Journal of Machine Learning Research. — 2003. David Blei. Introduction to Probabilistic Topic Models // Communications of the ACM. — 2012. — С. 77–84. David Blei, J.D. Lafferty. Topic models : web. — 2009. David Blei, J.D. Lafferty. Introduction to Probabilistic Topic Models // Annals of Applied Statistics. — 2007. — С. 17–35. — DOI:10.1214/07-AOAS114. David Mimno. Computational Historiography: Data Mining in a Century of Classics Journals // Journal on Computing and Cultural Heritag : журнал. — 2012. — DOI:10.1145/2160165.2160168. Matthew L. Jockers. Who's your DH Blog Mate: Match-Making the Day of DH Bloggers with Topic Modeling : web. — 2010. E. Микс. Понимание цифровых гуманитарных наук : web. — 2011. C. Тэмплтон. Тематическое моделирование в гуманитарных науках: обзор. // Maryland Institute for Technology in the Humanities Blog : web. — 2011. T. Гифитс, М. Стейверс. Нахождение научных тем // Proceedings of the National Academy of Sciences : журнал. — 2004. — DOI:10.1073/pnas.0307752101. — PMID 14872004. T. Янг, A Торгет и Р. Mihalcea. Тематическое моделирование в исторических газетах // Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities. The Association for Computational Linguistics, Madison : журнал. — 2011. — С. 96–104. С. Блок. Делаем больше с оцифровкой- введение в тематическое моделирование в ранних американских источниках // Common-place The Interactive Journal of Early American Life : журнал. — 2006. Д. Ньюман, С. Блок. Вероятностное тематическое разложение в газетах 18 века // Journal of the American Society for Information Science and Technology : журнал. — 2006. — DOI:10.1002/asi.20342. C. Блевин. Тематическое моделирование дневника Марты Баллардс // historying : web. — 2010. Ссылки[править | править код] Лекция: Тематическое моделирование — К. В. Воронцов // Школа анализа данных (видео-лекции). Лекция 2: Тематическое моделирование — К. В. Воронцов // Школа анализа данных (видео-лекции). Тематическое моделирование (неопр.). Коллекции документов для тематического моделирования (неопр.). Полностью разреженные тематические модели (перевод) / Fully Sparse Topic Models (неопр.). Обзор по вероятностным тематическим моделям (неопр.). Тематические модели для коллекции текстов (неопр.). Байесовские методы машинного обучения (курс лекций, Д. П. Ветров, Д. А. Кропотов) (неопр.). Тепллтон, Клай Тематическое моделирование в гуманитарных науках. Общий обзор. (неопр.).  Maryland Institute for Technology in the Humanities. Применение тематического моделирования для анализа новостей и ревю. Video of a Google Tech Talk presentation by Alice Oh on topic modeling with Latent Dirichlet allocation Моделирование науки: Динамическое тематическое моделирование научных исследований. Video of a Google Tech Talk presentation by David M. Blei Автоматизированная тематическая модель в политической науке. Video of a presentation by Brandon Stewart at the Tools for Text Workshop, 14 June 2010 Лекция: Тематическое моделирование — Дэвид Блей 2009 г. Видео лекция Принстонский университет Регуляризация вероятностных тематических моделей для повышения интерпретируемости и определения числа тем Диалог 2014 Parsimonious Topic Models with Salient Word Discovery Программное обеспечение и программные библиотеки[править | править код] Малет (программа) Инструментарий Стэндфордского университета по тематическому моделированию GenSim — «тематическое моделирование для людей» LDA C# LDA in Infer.NET Обработка естественного языкаОбщие определения Корпус текстов Речевой корпус Стоп-слова Мешок слов[en] AI-полнота N-грамма Биграммный шифр Триграмма[en] Анализ текста Сегментация текста[en] Частеречная разметка Поверхностный синтаксический анализ[en] Обработка сложных слов[en] Извлечение коллокаций[en] Стемминг Лемматизация Распознавание именованных сущностей[en] Разрешение кореферентности Анализ тональности текста Извлечение концептов[en] Синтаксический анализ Разрешение лексической многозначности Извлечение терминологии[en] Извлечение информации Идентификация языка Определение регистра[en] Реферирование[en] Извлечение предложений[en] Генерация реферата Многодокументное реферирование[en] Упрощение текста[en] Машинный перевод Автоматизированный Гибридный Интерлингвальный[en] На основе правил На основе примеров На основе словаря[en] Статистический Синхронный Трансферный[en] Идентификация и сбор данных Распознавание речи Синтез речи Оптическое распознавание символов Генерация текста Тематическая модель Размещение патинко Латентное размещение Дирихле Латентно-семантический анализ Рецензирование[en] Автоматизированная оценка сочинений[en] Конкордансер[en] Предективный ввод текста Система проверки грамматики[en] Система проверки правописания Угадывание синтаксиса[en] Интерфейс на естественном языке[en] Автоматизированный онлайн-помощник[en] Виртуальный собеседник Вопросно-ответная система Интерактивная литература     