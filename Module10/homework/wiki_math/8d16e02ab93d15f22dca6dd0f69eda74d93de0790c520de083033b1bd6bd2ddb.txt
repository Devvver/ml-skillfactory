Гауссовский процесс В теории вероятностей и статистике гауссовский процесс - это стохастический процесс (совокупность случайных величин, индексированных некоторым параметром, чаще всего временем или координатами), такой что любой конечный набор этих случайных величин имеет многомерное нормальное распределение, то есть любая конечная линейная комбинация из них нормально распределена. Распределение гауссовского процесса – это совместное распределение всех его случайных величин и, в силу чего, является распределением функций с непрерывной областью определения. Если рассматривать гауссовский процесс как способ решения задач машинного обучения, то используется ленивое обучение и мера подобия между точками (функция ядра) для получения прогноза значения невидимой точки из обучающей выборки. В понятие прогноза, помимо самой оценки точки, входит информация о неопределенности — одномерное гауссовское распределение.[1] Для вычисления прогнозов некоторых функций ядра используют метод матричной алгебры, кригинг.  Гауссовский процесс назван так в честь Карла Фридриха Гаусса, поскольку в его основе лежит понятие гауссовского распределения (нормального распределения). Гауссовский процесс может рассматриваться как бесконечномерное обобщение многомерных нормальных распределений. Эти процессы применяются в статистическом моделировании; в частности используются свойства нормальности. Например, если случайный процесс моделируется как гауссовский, то распределения различных производных величин, такие как среднее значение процесса в течение определенного промежутка времени и погрешность его оценки с использованием выборки значений, могут быть получены явно.   Содержание  1 Определение 2 Ковариационные функции  2.1 Обычные ковариационные функции   3 Броуновское движение как интеграл гауссовских процессов 4 Приложения  4.1 Прогноз гауссовского процесса или кригинг   5 См. также 6 Примечания 7 Внешние ссылки  7.1 Программное обеспечение     Определение[править | править код] Случайный процесс с непрерывным временем является гауссовским тогда и только тогда, когда для любого конечного множества индексов      t  1   , … ,  t  k     {\displaystyle t_{1},\ldots ,t_{k}}   из множества индексов     T   {\displaystyle T}          X    t  1   , … ,  t  k     = (   X    t  1     , … ,   X    t  k     )   {\displaystyle \mathbf {X} _{t_{1},\ldots ,t_{k}}=(\mathbf {X} _{t_{1}},\ldots ,\mathbf {X} _{t_{k}})}   - многомерная гауссовская случайная величина.[2] То же самое, что и всякая линейная комбинация     (   X    t  1     , … ,   X    t  k     )   {\displaystyle (\mathbf {X} _{t_{1}},\ldots ,\mathbf {X} _{t_{k}})}   имеет одномерное нормальное (гауссовское) распределение. Используя характеристические функции случайных величин, свойство Гаусса можно сформулировать следующим образом:      {   X  t   ; t ∈ T  }    {\displaystyle \left\{X_{t};t\in T\right\}}   - гауссовское тогда и только тогда, когда для любого конечного множества индексов      t  1   , … ,  t  k     {\displaystyle t_{1},\ldots ,t_{k}}  , существуют вещественные значения      σ  ℓ j     {\displaystyle \sigma _{\ell j}}  ,      μ  ℓ     {\displaystyle \mu _{\ell }}   где      σ  j j   > 0   {\displaystyle \sigma _{jj}>0}   такие, что для всех       s  1   ,  s  2   , … ,  s  k   ∈  R    {\displaystyle s_{1},s_{2},\ldots ,s_{k}\in \mathbb {R} }   выполнено равенство      E ⁡  (  exp ⁡  (  i    ∑  ℓ = 1   k    s  ℓ       X    t  ℓ      )   )  = exp ⁡  (  −   1 2     ∑  ℓ , j    σ  ℓ j    s  ℓ    s  j   + i  ∑  ℓ    μ  ℓ    s  ℓ    )  .   {\displaystyle \operatorname {E} \left(\exp \left(i\ \sum _{\ell =1}^{k}s_{\ell }\ \mathbf {X} _{t_{\ell }}\right)\right)=\exp \left(-{\frac {1}{2}}\,\sum _{\ell ,j}\sigma _{\ell j}s_{\ell }s_{j}+i\sum _{\ell }\mu _{\ell }s_{\ell }\right).}   Где     i   {\displaystyle i}   - мнимая единица. Числа      σ  ℓ j     {\displaystyle \sigma _{\ell j}}   и      μ  ℓ     {\displaystyle \mu _{\ell }}   - ковариации и средние значения переменных в процессах соответственно.[3]  Ковариационные функции[править | править код] Главная особенность гауссовских процессов - они могут быть полностью определены второй порядковой статистикой.[4] Следовательно, ковариационная функция полностью определяет поведение процесса, если математическое ожидание гауссовского процесса равно нулю. Важно отметить, что неотрицательная определенность функции делает возможным ее спектральное разложение при помощи разложения Карунена - Лоэва. Через ковариационную функцию можно определить стационарность, изотропию, гладкость и периодичность процесса.[4][5] Стационарность выражает поведение процесса относительно расстояния между любыми двумя точками     x   {\displaystyle x}   и      x ′    {\displaystyle x'}  . Если процесс стационарный, то он зависит от взаимного расположения своих точек, расстояния между ними,     x −  x ′    {\displaystyle x-x'}  , в ином случае, он нестационарный, то есть зависит от фактического положения точек     x   {\displaystyle x}   и      x ′    {\displaystyle x'}  . Примером может послужить частный случай процесса Орнштейна-Уленбека, процесс броуновского движения: он является стационарным. Если процесс зависит только от      |  x −  x ′   |    {\displaystyle |x-x'|}  , евклидова расстояния (не направления) между     x   {\displaystyle x}   и      x ′    {\displaystyle x'}  , то процесс считается изотропным. Стационарный и изотропный процесс называют однородным;[6] на практике свойства стационарности и изотропии отражают различия(или, скорее, их отсутствие) в поведении процесса с учетом положения наблюдателя. Суть гауссовских процессов заключается в получении априорных распределений вероятности, гладкость которых зависит от взятой ковариационной функции.[4] Если мы ожидаем, что для "лежащих близко" входных точек     x   {\displaystyle x}   и      x ′    {\displaystyle x'}   соответствующие им выходные точки     y   {\displaystyle y}   и      y ′    {\displaystyle y'}   также "лежат близко", тогда присутствует предположение о непрерывности функции. Если мы хотим допустить значительное смещение, то нужно выбрать более грубую ковариационную функцию. В качестве примеров крайнего поведения можно привести ковариационную функцию Орнштейна-Уленбека и квадратичную экспоненциальную функцию, где первая не дифференцируема нигде, а последняя бесконечно дифференцируема. Под периодичностью понимается индуцирование периодических закономерностей в поведении процесса. Формально это достигается путем отображения входного значения     x   {\displaystyle x}   на двумерный вектор     u ( x ) = ( c o s ( x ) , s i n ( x ) ) .   {\displaystyle u(x)=(cos(x),sin(x)).}    Обычные ковариационные функции[править | править код]  Влияние выбора различных ядер на функцию распределения гауссовского процесса. Слева направо: квадратичное экспоненциальное ядро, броуновское, квадратичное. Существует ряд общих ковариационных функций:[5]  Константа:      K  C   ( x ,  x ′  ) = C   {\displaystyle K_{\operatorname {C} }(x,x')=C}   Линейная функция:      K  L   ( x ,  x ′  ) =  x  T    x ′    {\displaystyle K_{\operatorname {L} }(x,x')=x^{T}x'}   Гауссовский шум:      K  GN   ( x ,  x ′  ) =  σ  2    δ  x ,  x ′      {\displaystyle K_{\operatorname {GN} }(x,x')=\sigma ^{2}\delta _{x,x'}}   Квадратичная экспоненциальная функция:      K  SE   ( x ,  x ′  ) = exp ⁡   (   −    ‖ d  ‖  2     2  ℓ  2        )     {\displaystyle K_{\operatorname {SE} }(x,x')=\exp {\Big (}-{\frac {\|d\|^{2}}{2\ell ^{2}}}{\Big )}}   Функция Орнштейна-Уленбека:      K  OU   ( x ,  x ′  ) = exp ⁡  (  −     |  d  |   ℓ    )    {\displaystyle K_{\operatorname {OU} }(x,x')=\exp \left(-{\frac {|d|}{\ell }}\right)}   Matérn:      K  Matern   ( x ,  x ′  ) =    2  1 − ν    Γ ( ν )      (        2 ν    |  d  |   ℓ      )    ν    K  ν     (        2 ν    |  d  |   ℓ     )     {\displaystyle K_{\operatorname {Matern} }(x,x')={\frac {2^{1-\nu }}{\Gamma (\nu )}}{\Big (}{\frac {{\sqrt {2\nu }}|d|}{\ell }}{\Big )}^{\nu }K_{\nu }{\Big (}{\frac {{\sqrt {2\nu }}|d|}{\ell }}{\Big )}}   Периодическая функция:      K  P   ( x ,  x ′  ) = exp ⁡  (  −    2  sin  2   ⁡  (   d 2   )    ℓ  2      )    {\displaystyle K_{\operatorname {P} }(x,x')=\exp \left(-{\frac {2\sin ^{2}\left({\frac {d}{2}}\right)}{\ell ^{2}}}\right)}   Рациональная квадратичная функция:      K  RQ   ( x ,  x ′  ) = ( 1 +  |  d   |   2    )  − α   ,  α ≥ 0   {\displaystyle K_{\operatorname {RQ} }(x,x')=(1+|d|^{2})^{-\alpha },\quad \alpha \geq 0}   Здесь     d = x −  x ′    {\displaystyle d=x-x'}  . Параметр     ℓ   {\displaystyle \ell }   является характеристикой масштаба длины процесса (практически, «насколько близко» две точки     x   {\displaystyle x}   и      x ′    {\displaystyle x'}   должны быть, чтобы значительно влиять друг на друга),     δ   {\displaystyle \delta }   - это символ Кронекера и     σ   {\displaystyle \sigma }   - среднеквадратическое отклонение колебаний шума. Кроме того,      K  ν     {\displaystyle K_{\nu }}   является модифицированной функцией Бесселя     ν   {\displaystyle \nu }   и     Γ ( ν )   {\displaystyle \Gamma (\nu )}   - это гамма-функция, вычисленная по     ν   {\displaystyle \nu }  . Важно отметить, что сложную ковариационную функцию можно определить как линейную комбинацию других более простых ковариационных функций затем, чтобы объединить различную информацию о имеющихся наборах данных. Очевидно, что полученные результаты зависят от значений гиперпараметров     θ   {\displaystyle \theta }   (например,     ℓ   {\displaystyle \ell }   и     σ   {\displaystyle \sigma }  ), определяющих поведение модели.  Броуновское движение как интеграл гауссовских процессов[править | править код] Винеровский процесс (так называемое броуновское движение) является интегралом гауссовского процесса белого шума. Он не стационарен, однако имеет стационарные приращения. Процесс Орнштейна-Уленбека - это стационарный гауссовский процесс. Броуновский мост (подобный процессу Орнштейна-Уленбека) является примером гауссовского процесса, приращения которого не являются независимыми. Дробное броуновское движение является гауссовским процессом, ковариационная функция которого является обобщением функции винеровского процесса.  Приложения[править | править код]  Пример регрессии на основе гауссовского процесса по сравнению с другими регрессионными моделями.[7] Гауссовский процесс может быть использован как априорное распределение вероятностей функций в байесовском выводе.[5][8] Для любого множества из N точек в нужной области функций возьмите многомерное гауссовское распределение, ковариационный матричный параметр которого является определителем Грама взятых N точек с некоторым желаемым ядром, и выборку из этого распределения. Вывод непрерывных значений на основе гауссовского процесса, определяемого предыдущими ковариациями, известен как кригинг (регрессия на основе гауссовского процесса). Поэтому, гауссовские процессы полезны в качестве мощного нелинейного многомерного инструмента интерполяции. Регрессия на основе гауссовского процесса может быть дополнительно расширена для решения задач обучения как с учителем, так и без (самообучение).  Прогноз гауссовского процесса или кригинг[править | править код]  Регрессия на основе гауссовского процесса (прогнозирование) с квадратичным экспоненциальным ядром. Когда речь идёт об основной проблеме регрессии на основе гауссовского процесса (кригинге), предполагается, что для гауссовского процесса     f   {\displaystyle f}  , наблюдаемого в координатах     x   {\displaystyle x}  , вектор значений     f ( x )   {\displaystyle f(x)}   является всего лишь одной из выборок многомерного гауссовского распределения, размерность которого равна числу наблюдаемых координат      |  x  |    {\displaystyle |x|}  . Следовательно, согласно допущению о нулевом распределении,     f ( x ) ∼ N ( 0 , K ( θ , x ,  x ′  ) )   {\displaystyle f(x)\sim N(0,K(\theta ,x,x'))}  , где     K ( θ , x ,  x ′  )   {\displaystyle K(\theta ,x,x')}   - ковариационная матрица между всеми возможными парами     ( x ,  x ′  )   {\displaystyle (x,x')}   для заданного множества гиперпараметров     θ   {\displaystyle \theta }  .[5] Таким образом, логарифм предельной вероятности равен:      log ⁡ p ( f ( x )  |  θ , x ) = −   1 2   f ( x  )  T   K ( θ , x ,  x ′   )  − 1   f ( x ) −   1 2   log ⁡ det ( K ( θ , x ,  x ′  ) ) −     |  x  |   2   log ⁡ 2 π   {\displaystyle \log p(f(x)|\theta ,x)=-{\frac {1}{2}}f(x)^{T}K(\theta ,x,x')^{-1}f(x)-{\frac {1}{2}}\log \det(K(\theta ,x,x'))-{\frac {|x|}{2}}\log 2\pi }   и максимизация этой предельной вероятности по отношению к     θ   {\displaystyle \theta }   даёт полную характеристику гауссовского процесса     f   {\displaystyle f}  . Можно отметить, что первое выражение зависит от неспособности модели соответствовать наблюдаемым значениям, а второе выражение прямо пропорционально сложности модели. Указав     θ   {\displaystyle \theta }   и сделав прогноз о ненаблюдаемых значениях     f (  x  ∗   )   {\displaystyle f(x^{*})}   в координатах      x  ∗     {\displaystyle x^{*}}  , останется сделать график выборок из прогностического распределения     p (  y  ∗   ∣  x  ∗   , f ( x ) , x ) = N (  y  ∗   ∣ A , B )   {\displaystyle p(y^{*}\mid x^{*},f(x),x)=N(y^{*}\mid A,B)}  , где последующая средняя оценка     A   {\displaystyle A}   определяется как      A = K ( θ ,  x  ∗   , x ) K ( θ , x ,  x ′   )  − 1   f ( x )   {\displaystyle A=K(\theta ,x^{*},x)K(\theta ,x,x')^{-1}f(x)}   и последующая оценка дисперсии B определяется как      B = K ( θ ,  x  ∗   ,  x  ∗   ) − K ( θ ,  x  ∗   , x ) K ( θ , x ,  x ′   )  − 1   K ( θ ,  x  ∗   , x  )  T     {\displaystyle B=K(\theta ,x^{*},x^{*})-K(\theta ,x^{*},x)K(\theta ,x,x')^{-1}K(\theta ,x^{*},x)^{T}}   где     K ( θ ,  x  ∗   , x )   {\displaystyle K(\theta ,x^{*},x)}   - ковариация между новой оценкой координаты      x  ∗     {\displaystyle x^{*}}   и всеми другими наблюдаемыми координатами     x   {\displaystyle x}   для данного гиперпараметрического вектора     θ   {\displaystyle \theta }  ,     K ( θ , x ,  x ′  )   {\displaystyle K(\theta ,x,x')}   и     f ( x )   {\displaystyle f(x)}   определены как и прежде, а     K ( θ ,  x  ∗   ,  x  ∗   )   {\displaystyle K(\theta ,x^{*},x^{*})}   является дисперсией в точке      x  ∗     {\displaystyle x^{*}}  , продиктованной вектором     θ   {\displaystyle \theta }  . Важно отметить, что последующая средняя оценка     f (  x  ∗   )   {\displaystyle f(x^{*})}   ("точечная оценка") является линейной комбинацией наблюдений     f ( x )   {\displaystyle f(x)}  ; аналогичным образом дисперсия     f (  x  ∗   )   {\displaystyle f(x^{*})}   фактически не зависит от наблюдений     f ( x )   {\displaystyle f(x)}  . Известным узким местом в прогнозировании гауссовского процесса является то, что вычислительная сложность прогнозирования является кубической по числу точек      |  x  |    {\displaystyle |x|}  , то есть вычисление может быть невозможным для больших наборов данных.[4] Чтобы обойти эту проблему, ведутся работы по разреженным гауссовским процессам, которые обычно основаны на идее построения репрезентативного набора для данного процесса     f   {\displaystyle f}  .[9][10]  См. также[править | править код] Среднеквадратическое отклонение Кригинг Априорная вероятность Примечания[править | править код]   ↑ Platypus Innovation: A Simple Intro to Gaussian Processes (a great data modelling tool) (неопр.).  ↑ MacKay, David, J.C. Information Theory, Inference, and Learning Algorithms. — Cambridge University Press, 2003. — P. 540. — «"The probability distribution of a function     y (  x  )   {\displaystyle y(\mathbf {x} )}   is a Gaussian processes if for any finite selection of points       x   ( 1 )   ,   x   ( 2 )   , … ,   x   ( N )     {\displaystyle \mathbf {x} ^{(1)},\mathbf {x} ^{(2)},\ldots ,\mathbf {x} ^{(N)}}  , the density     P ( y (   x   ( 1 )   ) , y (   x   ( 2 )   ) , … , y (   x   ( N )   ) )   {\displaystyle P(y(\mathbf {x} ^{(1)}),y(\mathbf {x} ^{(2)}),\ldots ,y(\mathbf {x} ^{(N)}))}  is a Gaussian"». — ISBN 9780521642989.  ↑ Dudley, R.M. Real Analysis and Probability. — Wadsworth and Brooks/Cole, 1989.  ↑ 1 2 3 4 Barber, David. Bayesian Reasoning and Machine Learning. — Cambridge University Press, 2012. — ISBN 978-0-521-51814-7.  ↑ 1 2 3 4 Rasmussen, C.E. Gaussian Processes for Machine Learning. — MIT Press, 2006. — ISBN 0-262-18253-X.  ↑ Grimmett, Geoffrey. Probability and Random Processes. — Oxford University Press, 2001. — ISBN 0198572220.  ↑ The documentation for scikit-learn also has similar examples.  ↑ Liu, W. Kernel Adaptive Filtering: A Comprehensive Introduction. — John Wiley, 2010. — ISBN 0-470-44753-2.  ↑ Smola, A.J.; Schoellkopf, B. (2000). “Sparse greedy matrix approximation for machine learning”. Proceedings of the Seventeenth International Conference on Machine Learning: 911—918..mw-parser-output cite.citation{font-style:inherit}.mw-parser-output q{quotes:"\"""\"""'""'"}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-limited a,.mw-parser-output .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}  ↑ Csato, L.; Opper, M. (2002). “Sparse on-line Gaussian processes”. Neural Computation. 14 (3): 641—668. DOI:10.1162/089976602317250933.   Внешние ссылки[править | править код] The Gaussian Processes Web Site, including the text of Rasmussen and Williams' Gaussian Processes for Machine Learning A gentle introduction to Gaussian processes A Review of Gaussian Random Fields and Correlation Functions Программное обеспечение[править | править код] STK: a Small (Matlab/Octave) Toolbox for Kriging and GP modeling Kriging module in UQLab framework (Matlab) Matlab/Octave function for stationary Gaussian fields Yelp MOE – A black box optimization engine using Gaussian process learning ooDACE – A flexible object-oriented Kriging matlab toolbox. GPstuff – Gaussian process toolbox for Matlab and Octave GPy – A Gaussian processes framework in Python Interactive Gaussian process regression demo Basic Gaussian process library written in C++11 scikit-learn – A machine learning library for Python which includes Gaussian process regression and classification [1] - The Kriging toolKit (KriKit) is developed at the Institute of Bio- and Geosciences 1 (IBG-1) of Forschungszentrum Jülich (FZJ)    