Алгоритм фрактального сжатия  Треугольник Серпинского — изображение, задаваемое тремя аффинными преобразованиями Фрактальное сжатие изображений — алгоритм сжатия изображений c потерями, основанный на применении систем итерируемых функций (как правило являющимися аффинными преобразованиями) к изображениям. Данный алгоритм известен тем, что в некоторых случаях позволяет получить очень высокие коэффициенты сжатия при приемлемом визуальном качестве для реальных фотографий природных объектов. Из-за сложной ситуации с патентованием широкого распространения алгоритм не получил.  Содержание  1 Описание 2 Сложность метода 3 Патенты 4 См. также 5 Примечания 6 Ссылки   Описание[править | править код] Основа метода фрактального кодирования — это обнаружение самоподобных участков в изображении. Впервые возможность применения теории систем итерируемых функций (англ. Iterated Function System, IFS) к проблеме сжатия изображения была исследована Майклом Барнсли (англ. Michael Barnsley[1]) и Аланом Слоуном (англ. Alan Sloan). Они запатентовали свою идею в 1990 и 1991 годах (U.S. Patent 5 065 447). А. Жакен (фр. Arnaud Jacquin) представил метод фрактального кодирования, в котором используются системы доменных и ранговых блоков изображения (англ. domain and range subimage blocks), блоков квадратной формы, покрывающих всё изображение. Этот подход стал основой для большинства методов фрактального кодирования. Он был усовершенствован Ювалом Фишером (англ. Yuval Fisher) и рядом других исследователей. В соответствии с данным методом изображение разбивается на множество неперекрывающихся ранговых подизображений (англ. range subimages) и определяется множество перекрывающихся доменных подизображений (англ. domain subimages). Для каждого рангового блока алгоритм кодирования находит наиболее подходящий доменный блок и аффинное преобразование, которое переводит этот доменный блок в данный ранговый блок. Структура изображения отображается в систему ранговых блоков, доменных блоков и преобразований. Идея заключается в следующем: предположим что исходное изображение является неподвижной точкой некоего сжимающего отображения. Тогда можно вместо самого изображения запомнить каким-либо образом это отображение, а для восстановления достаточно многократно применить это отображение к любому стартовому изображению. По теореме Банаха, такие итерации всегда приводят к неподвижной точке, то есть к исходному изображению. На практике трудность заключается в отыскании по изображению наиболее подходящего сжимающего отображения и в компактном его хранении. Как правило, алгоритмы поиска отображения (то есть алгоритмы сжатия) в значительной степени переборные и требуют больших вычислительных затрат. В то же время, алгоритмы восстановления достаточно эффективны и быстры. Вкратце, метод, предложенный Барнсли, можно описать следующим образом. Изображение кодируется несколькими простыми преобразованиями (в нашем случае аффинными), то есть определяется коэффициентами этих преобразований (в нашем случае A, B, C, D, E, F). Например, изображение кривой Коха можно закодировать четырьмя аффинными преобразованиями, однозначно определив его с помощью всего 24-х коэффициентов. Далее, поставив чёрную точку в любой точке картинки, применим преобразования в случайном порядке некоторое (достаточно большое) число раз (этот метод ещё называют фрактальным пинг-понгом). В результате точка обязательно перейдёт куда-то внутрь чёрной области на исходном изображении. После применения такой операции много раз будет заполнено всё чёрное пространство, что восстановит картинку.  Сложность метода[править | править код] Основная сложность фрактального сжатия заключается в том, что для нахождения соответствующих доменных блоков требуется полный перебор. Поскольку при этом каждый раз должны сравниваться два массива, данная операция получается достаточно длительной. Сравнительно простым преобразованием её можно свести к операции скалярного произведения двух массивов, однако даже вычисление скалярного произведения требует довольно большого времени. На данный момент[когда?] известно достаточно большое количество алгоритмов оптимизации перебора, возникающего при фрактальном сжатии, поскольку большинство статей, исследовавших алгоритм, были посвящены этой проблеме и во время активных исследований (1992—1996 года) выходило до 300 статей в год. Наиболее эффективными оказались два направления исследований: метод выделения особенностей (feature extraction) и метод классификации доменов (classification of domains).  Патенты[править | править код] Информация в этой статье или некоторых её разделах устарела.Вы можете помочь проекту, обновив её и убрав после этого данный шаблон. Майклом Барнсли и другими было получено несколько патентов на фрактальное сжатие в США и других странах. Например, 4,941,193, 5,065,447, 5,384,867, 5,416,856 и 5,430,812. Эти патенты покрывают широкий спектр возможных изменений фрактального сжатия и серьёзно сдерживают его развитие. Данные патенты не ограничивают исследований в этой области, то есть можно придумывать свои алгоритмы на основе запатентованных и публиковать их. Также, можно продавать алгоритмы в страны, на которые не распространяются полученные патенты. Кроме того, срок действия большинства патентов — 17 лет с момента принятия, и он истекает для большинства патентов в ближайшее время, соответственно, использование методов, покрывавшихся этими патентами, станет гарантированно свободным.  См. также[править | править код] Фрактал JPEG 2000 Список фракталов по размерности Хаусдорфа[en] Сжатие изображений на базе дифференциального анализа Примечания[править | править код]   ↑ Домашняя страница Майкла Барнсли   Ссылки[править | править код] Resources on fractal compression (недоступная ссылка) Коллекция статей по фрактальному сжатию: Fractal Papers Leipzig Collection Fractal Image Compression for Spaceborne Transputers  (недоступная ссылка с 18-05-2013 [2050 дней] — история) Pulcini and Verrando’s Compressor Сообщество фрактального сжатия изображений Стиль этой статьи неэнциклопедичен или нарушает нормы русского языка.Статью следует исправить согласно стилистическим правилам Википедии. Методы сжатияТеорияИнформация Собственная Взаимная Энтропия Условная энтропия Сложность Избыточность Единицы измерения Бит Нат Ниббл Хартли Формула Хартли Без потерьЭнтропийное сжатие Алгоритм Хаффмана Адаптивный алгоритм Хаффмана Алгоритм Шеннона — Фано Алгоритм Шеннона Арифметическое кодирование (Интервальное) Коды Голомба Дельта Универсальный код Элиаса Фибоначчи Словарные методы RLE Deflate LZ (LZ77/LZ78 LZSS LZW LZWL LZO LZMA LZX LZRW LZJB LZT LZ4 Zstandard) Прочее RLE CTW BWT MTF PPM DMC АудиоТеория Свёртка PCM Алиасинг Дискретизация Теорема Котельникова Методы LPC LAR LSP WLPC CELP ACELP A-закон μ-закон АДИКМ МДКП Преобразование Фурье Психоакустическая модель Прочее Компрессор аудиосигнала Сжатие речи Полосное кодирование ИзображенияТермины Цветовое пространство Пиксель Субдискретизация насыщенности Артефакты сжатия Методы RLE DPCM Фрактальный Вейвлетный EZW SPIHT LP ДКП ПКЛ Прочее Битрейт Стандартное тестовое изображение PSNR Квантование ВидеоТермины Характеристики видео Кадр Типы кадров Качество видео Методы Компенсация движения ДКП Квантование Вейвлетный Прочее Видеокодек Rate distortion theory CBR ABR VBR     