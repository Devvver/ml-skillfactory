Обучение без учителя Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning     Обучение без учителя (самообучение, спонтанное обучение, англ. Unsupervised learning) — один из способов машинного обучения, при котором испытуемая система спонтанно обучается выполнять поставленную задачу без вмешательства со стороны экспериментатора. С точки зрения кибернетики, это является одним из видов кибернетического эксперимента. Как правило, это пригодно только для задач, в которых известны описания множества объектов (обучающей выборки), и требуется обнаружить внутренние взаимосвязи, зависимости, закономерности, существующие между объектами. Обучение без учителя часто противопоставляется обучению с учителем, когда для каждого обучающего объекта принудительно задаётся «правильный ответ», и требуется найти зависимость между стимулами и реакциями системы.  Содержание  1 Связь с физиологией 2 Понятие «обучения без учителя» в теории распознавания образов 3 Типы входных данных 4 Решаемые задачи  4.1 Задачи кластеризации 4.2 Задачи обобщения 4.3 Задачи поиска правил ассоциации 4.4 Задачи сокращения размерности 4.5 Задачи визуализации данных 4.6 Некоторые приложения   5 См. также 6 Примечания 7 Литература 8 Ссылки   Связь с физиологией[править | править код] Несмотря на многочисленные прикладные достижения, обучение с учителем критиковалось за свою биологическую неправдоподобность. Трудно вообразить обучающий механизм в мозге, который бы сравнивал желаемые и действительные значения выходов, выполняя коррекцию с помощью обратной связи. Если допустить подобный механизм в мозге, то откуда тогда возникают желаемые выходы? Обучение без учителя является намного более правдоподобной моделью обучения в биологической системе. Развитая Кохоненом и многими другими, она не нуждается в целевом векторе для выходов и, следовательно, не требует сравнения с предопределенными идеальными ответами[1].  Понятие «обучения без учителя» в теории распознавания образов[править | править код] Для построения теории и отхода от кибернетического эксперимента в различных теориях эксперимент с обучением без учителя пытаются формализовать математически. Существует много различных подвидов постановки и определения данной формализации, одна из которых отражена в теории распознавания образов. Такой отход от эксперимента и построение теории связаны с различным мнением специалистов во взглядах. Различия, в частности, возникают при ответе на вопрос: «Возможны ли единые принципы адекватного описания образов различной природы, или же такое описание каждый раз есть задача для специалистов конкретных знаний?». В первом случае постановка должна быть нацелена на выявление общих принципов использования априорной информации при составлении адекватного описания образов. Важно, что здесь априорные сведения об образах различной природы разные, а принцип их учета один и тот же. Во втором случае проблема получения описания выносится за рамки общей постановки, и теория обучения машин распознаванию образов с точки зрения статистической теории обучения распознаванию образов может быть сведена к проблеме минимизации среднего риска в специальном классе решающих правил[2]. В теории распознавания образов различают в основном три подхода к данной проблеме[3]:  Эвристические методы; Математические методы; Лингвистические (синтаксические) методы. Типы входных данных[править | править код] Признаковое описание объектов. Каждый объект описывается набором своих характеристик, называемых признаками. Признаки могут быть числовыми или нечисловыми. Матрица расстояний между объектами. Каждый объект описывается расстояниями до всех остальных объектов обучающей выборки. Решаемые задачи[править | править код] Экспериментальная схема обучения без учителя часто используется в теории распознавания образов, машинном обучении. При этом в зависимости от подхода формализуется в ту или иную математическую концепцию. И только в теории искусственных нейронных сетей задача решается экспериментально, применяя тот или иной вид нейросетей. При этом, как правило, полученная модель может не иметь интерпретации, что иногда относят к минусам нейросетей. Но тем не менее, результаты получаются ничем не хуже, и при желании могут быть интерпретированы при применении специальных методов.  Задачи кластеризации[править | править код] Основная статья: Кластерный анализ Эксперимент обучения без учителя при решении задачи распознавания образов можно сформулировать как задачу кластерного анализа. Выборка объектов разбивается на непересекающиеся подмножества, называемые кластерами, так, чтобы каждый кластер состоял из схожих объектов, а объекты разных кластеров существенно отличались. Исходная информация представляется в виде матрицы расстояний.  Методы решения  Графовые алгоритмы кластеризации Статистические алгоритмы кластеризации Иерархическая кластеризация или таксономия Нейронная сеть Кохонена Метод ближайших соседей (k-means) Автоассоциатор Глубокая сеть доверия Кластеризация может играть вспомогательную роль при решении задач классификации и регрессии. Для этого нужно сначала разбить выборку на кластеры, затем к каждому кластеру применить какой-нибудь совсем простой метод, например, приблизить целевую зависимость константой.  Методы решения  Нейронная сеть встречного распространения Метод радиальных базисных функций Задачи обобщения[править | править код] Основная статья: Обобщение понятий Так же как и в случае экспериментов по различению, что математически может быть сформулированно как кластеризация, при обобщении понятий можно исследовать спонтанное обобщение, при котором критерии подобия не вводятся извне или не навязываются экспериментатором. При этом в эксперименте по «чистому обобщению» от модели мозга или перцептрона требуется перейти от избирательной реакции на один стимул (допустим, квадрат, находящийся в левой части сетчатки) к подобному ему стимулу, который не активизирует ни одного из тех же сенсорных окончаний (квадрат в правой части сетчатки). К обобщению более слабого вида относится, например, требование, чтобы реакции системы распространялись на элементы класса подобных стимулов, которые не обязательно отделены от уже показанного ранее (или услышанного, или воспринятого на ощупь) стимула.  Задачи поиска правил ассоциации[править | править код] В этом разделе не хватает ссылок на источники информации.Информация должна быть проверяема, иначе она может быть поставлена под сомнение и удалена.Вы можете отредактировать эту статью, добавив ссылки на авторитетные источники.Эта отметка установлена  12 мая 2011 года. Исходная информация представляется в виде признаковых описаний. Задача состоит в том, чтобы найти такие наборы признаков, и такие значения этих признаков, которые особенно часто (неслучайно часто) встречаются в признаковых описаниях объектов.  Задачи сокращения размерности[править | править код] Исходная информация представляется в виде признаковых описаний, причём число признаков может быть достаточно большим. Задача состоит в том, чтобы представить эти данные в пространстве меньшей размерности, по возможности, минимизировав потери информации.  Методы решения  Метод главных компонент Метод независимых компонент Многомерное шкалирование Задачи визуализации данных[править | править код] Некоторые методы кластеризации и снижения размерности строят представления выборки в пространстве размерности два. Это позволяет отображать многомерные данные в виде плоских графиков и анализировать их визуально, что способствует лучшему пониманию данных и самой сути решаемой задачи.  Методы решения  Дендрограмма Самоорганизующаяся карта Кохонена Generative topographic map Карта сходства Некоторые приложения[править | править код] Социологические исследования: формирование представительных подвыборок при организации социологических опросов. Маркетинговые исследования: разбиение множества всех клиентов на кластеры для выявления типичных предпочтений. Анализ рыночных корзин: выявление сочетаний товаров, часто встречающихся вместе в покупках клиентов. См. также[править | править код] EM-алгоритм Обобщение понятий Квазибиологическая парадигма Примечания[править | править код]   ↑ Уоссермен, Ф. Нейрокомпьютерная техника: Теория и практика. — М.: Мир, 1992  ↑ Вапник В. Н., Червоненкис А. Я., Теория распознавания образов. Статистические проблемы обучения, 1974  ↑ Ту Дж., Гонсалес Р. Принципы распознавания образов, М. 1978   Литература[править | править код] Айвазян С. А., Енюков И. С., Мешалкин Л. Д. Прикладная статистика: основы моделирования и первичная обработка данных. — М.: Финансы и статистика, 1983. Айвазян С. А., Енюков И. С., Мешалкин Л. Д. Прикладная статистика: исследование зависимостей. — М.: Финансы и статистика, 1985. Айвазян С. А., Бухштабер В. М., Енюков И. С., Мешалкин Л. Д. Прикладная статистика: классификация и снижение размерности. — М.: Финансы и статистика, 1989. Журавлев Ю. И., Рязанов В. В., Сенько О. В. «Распознавание». Математические методы. Программная система. Практические применения. — М.: Фазис, 2006. ISBN 5-7036-0108-8. Загоруйко Н. Г. Прикладные методы анализа данных и знаний. — Новосибирск: ИМ СО РАН, 1999. ISBN 5-86134-060-9. Мандель И. Д. Кластерный анализ. — М.: Финансы и статистика, 1988. ISBN 5-279-00050-7. Шлезингер М., Главач В. Десять лекций по статистическому и структурному распознаванию. — Киев: Наукова думка, 2004. ISBN 966-00-0341-2. Hastie, T., Tibshirani R., Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. — 2nd ed. — Springer-Verlag, 2009. — 746 p. — ISBN 978-0-387-84857-0.. Розенблатт, Ф. Принципы нейродинамики: Перцептроны и теория механизмов мозга = Principles of Neurodynamic: Perceptrons and the Theory of Brain Mechanisms. — М.: Мир, 1965. — 480 с. Уоссермен, Ф. Нейрокомпьютерная техника: Теория и практика = Neural Computing. Theory and Practice. — М.: Мир, 1992. — 240 с. — ISBN 5-03-002115-9. Л. Б. Емельянов-Ярославский, Интеллектуальная квазибиологическая система, М., «НАУКА», 1990 — книга о одном подходе самообучения в соответствии с квазибиологической парадигмой Ссылки[править | править код] Профессиональный вики-ресурс, посвященный машинному обучению и интеллектуальному анализу данных Для улучшения этой статьи желательно: Викифицировать список литературы.Викифицировать статью.Проставив сноски, внести более точные указания на источники.    