BIRCH Машинное обучение иdata mining Задачи  Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение    Обучение с учителем  Метод k-ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамбли моделей (Бэггинг, Бустинг, Random forest) Метод релевантных векторов    Кластерный анализ  Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift    Снижение размерности  Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE    Структурное прогнозирование  Графовая вероятностная модель (Байесовская сеть, Скрытая марковская модель, CRF)    Выявление аномалий  Метод k-ближайших соседей Локальный уровень выброса    Графовые вероятностные модели  Байесовская сеть Марковская сеть Скрытая марковская модель    Искусственные нейронные сети  Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации (Сигмоида, Softmax, Радиально-базисная функция) Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть (Долгая краткосрочная память, Управляемый рекуррентный блок) Свёрточная нейронная сеть (U-Net) Автокодировщик    Обучение с подкреплением  Марковский процесс Уравнение Беллмана Жадный алгоритм Q-learning SARSA Temporal difference (TD)    Теория  Теория Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения    Журналы и конференции  NIPS ICML ML JMLR ArXiv:cs.LG    Глоссарий  Глоссарий по машинному обучению    Статьи по теме  List of datasets for machine-learning research Outline of machine learning     Сбалансированное итеративное сокращение и кластеризация с помощью иерархий (BIRCH, англ. balanced iterative reducing and clustering using hierarchies) — это алгоритм интеллектуального анализа данных без учителя, используемый для осуществления иерархической кластеризации на наборах данных большого размера[1]. Преимуществом BIRCH является возможность метода динамически кластеризовать по мере поступления многомерных метрических точек данных[en] в попытке получить кластеризацию лучшего качества для имеющегося набора ресурсов (памяти и временных рамок[en]). В большинстве случаев алгоритм BIRCH требует одного прохода по базе данных. Разработчики BIRCH утверждали, что это был «первым алгоритмом кластеризации, предлагающим в базах данных эффективно обрабатывать 'шум' (точки данных, которые не являются частью схемы)»[1] побивший DBSCAN за два месяца. Алгоритм получил в 2006 году приз SIGMOD[en] после 10 лет тестирования[2].  Содержание  1 Проблема с предыдущими методами 2 Преимущества BIRCH 3 Алгоритм 4 Вычисление признаков кластеров 5 Примечания 6 Литература   Проблема с предыдущими методами[править | править код] Предыдущие алгоритмы кластеризации работали менее эффективно на больших базах данных и неадекватно вели себя в случае, когда данные были слишком большие, чтобы поместиться в оперативную память. Как результат имелось много затрат для получения высокого качества кластеризации при минимизации цены дополнительных операций ввода/вывода. Более того, большинство предшественников BIRCH просматривали все точки данных (или всех выделенных кластеров на текущий момент) одинаково для каждого 'решения кластеризации' и не делали эвристического взвешивания на базе расстояний между этими точками данных.  Преимущества BIRCH[править | править код] Каждое решение кластеризации локально и осуществляется без просмотра всех точек данных и существующих на текущий момент кластеров. Метод работает на наблюдениях, пространство данных которых обычно не однородно заполнено и не каждая точка данных одинаково важна. Метод позволяет использовать всю доступную память для получения наиболее точных возможных подкластеров при минимизации цены ввода/вывода. Метод является инкрементальным и не требует наличия полного набора данных[en] сразу.  Алгоритм[править | править код] Алгоритм BIRCH берёт в качестве входа набор из N точек данных, представленный как вещественные вектора, и желаемое число кластеров K. Алгоритм разбит на четыре фазы, вторая из которых не обязательна. Первая фаза строит CF дерево точек данных, высоко сбалансированную древесную структуру, определённую следующим образом:  Если дан набор N d-мерных точек данных, признак кластеризации (англ. Clustering Feature)     C F   {\displaystyle CF}   набора определяется как тройка     C F = ( N , L S , S S )   {\displaystyle CF=(N,LS,SS)}  , где        L S  →   =  ∑  i = 1   N      X  i   →     {\displaystyle {\overrightarrow {LS}}=\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}   является линейной суммой, а        S S  →   =  ∑  i = 1   N   (    X  i   →    )  2     {\displaystyle {\overrightarrow {SS}}=\sum _{i=1}^{N}({\overrightarrow {X_{i}}})^{2}}   является суммой квадратов точек данных. Признаки кластеризации организаются в CF-дерево, высоко сбалансированное дерево с двумя параметрами: коэффициентом ветвления     B   {\displaystyle B}   и порогом     T   {\displaystyle T}  . Каждый нелистовой узел состоит максимум из     B   {\displaystyle B}   входов вида     [ C  F  i   , c h i l  d  i   ]   {\displaystyle [CF_{i},child_{i}]}  , где     c h i l  d  i     {\displaystyle child_{i}}   является указателем на его     i   {\displaystyle i}  -ого потомка, а     C  F  i     {\displaystyle CF_{i}}   является признаком кластеризации, представляющим связанный подкластер. Лист содержит не более     L   {\displaystyle L}   входов, каждый вида     [ C  F  i   ]   {\displaystyle [CF_{i}]}  . Он также имеет два указателя, prev и next, которые используются для соединения в цепь все листы. Размер дерева зависит от параметра T. Требуется, чтобы узел A вмещался на страницу размера P. B и L определяются значением P. Таким образом, P может меняться для настройки производительности[en]. Это очень компактное представление набора данных, поскольку каждый лист не является отдельной точкой данных, а является подкластером. На втором шаге алгоритм просматривает все листья в начальном CF-дереве, чтобы построить меньшее CF-дерево путём удаления выпадений и группирования переполненных подклассов в бо́льшие подклассы. Этот шаг в исходном представлении BIRCH помечен как необязательный. На третьем шаге используется существующий алгоритм для кластеризации всех листов. Здесь применяется агломерирующий иерархический алгоритм кластеризации непосредственно к подкластерам, представленным их CF-векторами. Это также обеспечивает гибкость, позволяющую пользователю указать либо желаемое число кластеров, либо желаемый порог диаметра кластеров. После этого шага получаем набор кластеров, которые содержат главные схемы распределения в данных. Однако могут существовать небольшие локальные неточности, которые могут быть обработаны необязательным шагом 4. На шаге 4 центры тяжести кластеров, полученных на шаге 3, используются как зародыши и точки перераспределения точек данных для получения нового набора кластеров. Шаг 4 обеспечивает также возможность отбрасывания выбросов. То есть точка, которая слишком далека от ближайшего зародыша, может считаться выбросом.  Вычисление признаков кластеров[править | править код] Если дано только     C F = [ N ,    L S  →   ,    S S  →   ]   {\displaystyle CF=[N,{\overrightarrow {LS}},{\overrightarrow {SS}}]}  , те же измерения могут быть получены без знания истинных значений.  Центроид:       C →   =     ∑  i = 1   N      X  i   →    N   =     L S  →  N     {\displaystyle {\overrightarrow {C}}={\frac {\sum _{i=1}^{N}{\overrightarrow {X_{i}}}}{N}}={\frac {\overrightarrow {LS}}{N}}}   Радиус:     R =      ∑  i = 1   N   (    X  i   →   −   C →    )  2    N    =     N ⋅    C →    2   +    S S  →   − 2 ⋅   C →   ⋅    L S  →    N      {\displaystyle R={\sqrt {\frac {\sum _{i=1}^{N}({\overrightarrow {X_{i}}}-{\overrightarrow {C}})^{2}}{N}}}={\sqrt {\frac {N\cdot {\overrightarrow {C}}^{2}+{\overrightarrow {SS}}-2\cdot {\overrightarrow {C}}\cdot {\overrightarrow {LS}}}{N}}}}   Среднее расстояние между кластерами     C  F  1   = [  N  1   ,    L  S  1    →   ,    S  S  1    →   ]   {\displaystyle CF_{1}=[N_{1},{\overrightarrow {LS_{1}}},{\overrightarrow {SS_{1}}}]}   и     C  F  2   = [  N  2   ,    L  S  2    →   ,    S  S  2    →   ]   {\displaystyle CF_{2}=[N_{2},{\overrightarrow {LS_{2}}},{\overrightarrow {SS_{2}}}]}  :     D  2   =      ∑  i = 1    N  1      ∑  j = 1    N  2     (    X  i   →   −    Y  j   →    )  2      N  1   ⋅  N  2       =      N  1   ⋅    S  S  2    →   +  N  2   ⋅    S  S  1    →   − 2 ⋅    L  S  1    →   ⋅    L  S  2    →      N  1   ⋅  N  2         {\displaystyle D_{2}={\sqrt {\frac {\sum _{i=1}^{N_{1}}\sum _{j=1}^{N_{2}}({\overrightarrow {X_{i}}}-{\overrightarrow {Y_{j}}})^{2}}{N_{1}\cdot N_{2}}}}={\sqrt {\frac {N_{1}\cdot {\overrightarrow {SS_{2}}}+N_{2}\cdot {\overrightarrow {SS_{1}}}-2\cdot {\overrightarrow {LS_{1}}}\cdot {\overrightarrow {LS_{2}}}}{N_{1}\cdot N_{2}}}}}   В мультифакторных случаях квадратный корень может быть заменён подходящей нормой.  Примечания[править | править код]   ↑ 1 2 Zhang, Ramakrishnan, Livny, 1996, с. 103–114.  ↑ 2006 SIGMOD Test of Time Award (неопр.). Архивировано 23 мая 2010 года.   Литература[править | править код]  Zhang T., Ramakrishnan R., Livny M. BIRCH: an efficient data clustering method for very large databases // Proceedings of the 1996 ACM SIGMOD international conference on Management of data  - SIGMOD '96. — 1996. — DOI:10.1145/233269.233324.  Для улучшения этой статьи желательно: Проверить качество перевода с иностранного языка.Исправить статью согласно стилистическим правилам Википедии.    