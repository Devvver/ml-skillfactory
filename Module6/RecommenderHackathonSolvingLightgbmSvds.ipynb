{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CceQuduDl-ie"
   },
   "source": [
    "В рамках этого домашнего задания мы организовали для вас соревнование на площадке kaggle. <br><br>\n",
    "Вам предстоит создать модель, которая делает максимально релевантные рекомендации фильмов для пользователей онлайн-кинотеатра. Вам доступна история просмотров, а также некоторая мета-информация о фильмах.  <br>\n",
    "Наше соревнования на kaggle закрытое, к нему могут присоединиться только участники курса. Если вдруг вы еще не зарегистрировались на kaggle - самое время это сделать (kaggle.com). <br>\n",
    "<br>\n",
    "В рамках этого ноутбука вам предоставлен код для работы с данными, реализация целевой метрики (NDCG@10), а также реализована модель, рекомендующая топ фильмов из истории. <br>\n",
    "Желаем успехов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "facB-LR7l-ig"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mmh3\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "import copy\n",
    "seaborn.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvtI7rvrl-ij"
   },
   "source": [
    "Загрузим обучающие данные. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeEOKZ7Gl-ik"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NBxdEiaXkdd"
   },
   "source": [
    "shape —  это свойство объекта DataFrame, которое позволяет узнать его размерность. Первое число —  количество строк в DataFrame, второе —  количеств столбцов (признаков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgdZtaaoXkdn",
    "outputId": "95a923f3-6a20-4c15-ba8f-8acfc97fe745"
   },
   "outputs": [],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEHcaXGaXkeM"
   },
   "source": [
    "head() - это функция объекта DataFrame, которая позволяет посмотреть на первые строчки датасета. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnBH7Q4xl-ir",
    "outputId": "94e28442-bc0c-498b-ca84-ba3c0dbf6e15"
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KmyqRTvl-iu"
   },
   "source": [
    "Также вам предоставлены файлы с мета-информацией о фильмах и соответствующих тегах (data/recommendations/movies.csv, data/recommendations/tags.csv). Их можно использовать для вашей модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoV1Ns1Yl-iu",
    "outputId": "6b33949b-d6b6-41e0-cfe5-df4d1ae0ec2c"
   },
   "outputs": [],
   "source": [
    "ll data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQIk6qtGXkft"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_GZPb9xl-ix"
   },
   "source": [
    "Тестовое множество в данном соревновании —  набор идентификаторов пользователей, для которых нужно сделать предсказания. Сохраним их в лист test_user_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gn4Jdvw3l-iy"
   },
   "outputs": [],
   "source": [
    "with open('data/test_user_id.list', 'r') as file:\n",
    "    test_user_id = file.read()\n",
    "test_user_id = [int(user_id) for user_id in test_user_id.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R-0FEJNXkg2"
   },
   "source": [
    "Выведем на экран первые 10 идентификаторов пользователей из тестового множества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6BCswRYl-i1",
    "outputId": "b21f280c-ccb2-46aa-d6f0-259a4afc0c0d"
   },
   "outputs": [],
   "source": [
    "test_user_id[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWnloluzl-i3"
   },
   "source": [
    "### Разделим данные на обучение и валидацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWrPHJyiXkhd"
   },
   "source": [
    "Важная деталь! Так как данные в нашем датасете на самом деле отсортированы по времени, то корректное разбиение на обучение и валидацию будет следующим: необходимо в качестве обучающего множества брать наблюдения более ранние по времени, а для валидации —  наиболее поздние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C24Js9BPl-i5"
   },
   "outputs": [],
   "source": [
    "train = ratings[:int(ratings.shape[0] * 0.75)]\n",
    "validation = ratings[int(ratings.shape[0] * 0.75):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDZdYuCNXkh0"
   },
   "source": [
    "Посмотрим на размерности получившихся множеств для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vN49rZ1XXkh6",
    "outputId": "2eee25e1-467c-49ed-b104-47354d18623e"
   },
   "outputs": [],
   "source": [
    "train.shape, validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjQWwWJxl-i7"
   },
   "source": [
    "### Реализация целевой метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adD2H2gxXkip"
   },
   "source": [
    "Ваша задача —  предсказать **релевантность** фильмов тому или иному пользователю. Релевантность — это функция, характеризующая, насколько объект (в нашем случае —  фильм) подходит пользователю. Чем выше значение функции релевантности, тем больше фильм подходит пользователю. \n",
    "\n",
    "При этом, так как мы строим рекомендательную систему, наша задача — найти самые релевантные  для пользователя фильмы. Поэтому нам не важно абсолютное значение релевантности, а важен порядок, в соответствии с которым мы выстроили фильмы согласно функции релевантности. \n",
    "\n",
    "Такая задача (в которой нам важно упорядочить элементы) называется **задачей ранжировани**. Кроме рекомендательной системы, задачу ранжирования решает, например, поисковая система:  ее функция — упорядочить web-страницы в порядке релевантности запросу пользователя. \n",
    "\n",
    "Для того, чтобы измерить качество ранжирования, нам понадобится специальная метрика NDCG (Normalized Discounted Cummulative Gain). \n",
    "Она сравнивает порядок, в котором мы упорядочили фильмы, с идеальным порядком (при котором сначала стоят самые релевантные фильмы, а в конце — самые нерелевантные). Если элемент стоит дальше того места, где ему положено стоять, NDCG уменьшает свое значение. При этом она учитывает номер, на котором произошло нарушение порядка: понятно, что поменять местами 1-й и 10й фильмы намного хуже чем 101-й и 110-й. На первую рекомендацию пользователь посмотрит гарантированно (и там окажется элемент, который должен был быть 10м), а до 101-го с большой вероятностью просто не дойдет. \n",
    "\n",
    "Важность позиции в NDCG убывает в  соответствии с обратным логарифмическим законом. \n",
    "\n",
    "При использовании метрики NDCG@K, важность позиций с номером большим, чем K, полагается равной нулю.\n",
    "\n",
    "Более подробно про метрику NDCG (с формулами!) можете почитать по ссылке: https://habr.com/company/econtenta/blog/303458/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4c68bZFXkit",
    "outputId": "ce125310-3859-4d3a-c4bb-ee50b0af6805"
   },
   "outputs": [],
   "source": [
    "K = 30\n",
    "max_n = 35\n",
    "\n",
    "x = [i for i in range(1, max_n)]\n",
    "y = [(i <= K) * 1/math.log2(i + 1) for i in range(1, max_n)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Относительная важность ошибки на i-й позиции метрики NDCG@{}\".format(K))\n",
    "plt.xlabel(\"Номер позиции\")\n",
    "plt.ylabel(\"Относительная важность ошибки\")\n",
    "plt.text(5, 0.1, \"\"\"после {}й позиции происходит резкий скачок в ноль,\n",
    "так как метрика NDCG@{} считает все позиции > {} неважными\"\"\".format(K, K, K), bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.plot(x, y);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRaCOwsfl-i8"
   },
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAjzjsuoXkje"
   },
   "source": [
    "Примеры использования метрики. r — это лист, содержащий значение релевантности объекта, находящегося на этой позиции. Чем выше значение, тем больше релеватность. 0 — объект не релевантен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbPFyRSgXkjj",
    "outputId": "3a4fdac0-3900-4d80-945e-002159bd2730"
   },
   "outputs": [],
   "source": [
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "ndcg_at_k(r, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2u-9bJ-XkkL",
    "outputId": "206c46fb-0501-4da1-c7d2-9a50c2cac24c"
   },
   "outputs": [],
   "source": [
    "r = [2, 1, 2, 0]\n",
    "ndcg_at_k(r, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-r4TjeNsXkkb",
    "outputId": "af4f145f-22bc-465f-d7ad-fccf940fc2b3"
   },
   "outputs": [],
   "source": [
    "ndcg_at_k([0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgDuJ9oZXklB",
    "outputId": "6844cbde-7956-4ce1-8c59-0b7401fb5346"
   },
   "outputs": [],
   "source": [
    "ndcg_at_k([1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4cGCKNyXkli"
   },
   "source": [
    "Помните, что в задаче ранжирования важен сам факт того, что пользователь посмотрит фильм, и не важна его конкретная оценка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXpv0avrl-i-"
   },
   "source": [
    "### Реализация топ-рекоммендера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIYCnF3cl-i_"
   },
   "outputs": [],
   "source": [
    "class TopRecommender(object):\n",
    "    def fit(self, train_data):\n",
    "        counts = Counter(train_data['movieId'])\n",
    "        self.predictions = counts.most_common()\n",
    "        \n",
    "    def predict(self, user_id, n_recommendations=10):\n",
    "        return [movie_id for movie_id, frequency in self.predictions[:n_recommendations]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPjbsXmml-jB"
   },
   "source": [
    "Построим модель и посмотрим на качество по метрике NDCG@10 на валидационном множестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SN7-RAI0l-jC"
   },
   "outputs": [],
   "source": [
    "recommender_train = TopRecommender()\n",
    "recommender_train.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMGhjjTbl-jE",
    "outputId": "9a7c9086-239c-4c9a-b76b-45b3716c6790",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "num_to_print = 10\n",
    "total_ndcg = 0\n",
    "\n",
    "for user_id, group in validation.groupby('userId'):\n",
    "    ground_truth_films = [int(data.movieId) for row, data in group.iterrows()]\n",
    "    recommendations = recommender_train.predict(user_id, n_recommendations=10)\n",
    "    relevance_scores = []\n",
    "    for rec in recommendations:\n",
    "        if rec in ground_truth_films:\n",
    "            relevance_scores.append(len(ground_truth_films) - ground_truth_films.index(rec))\n",
    "        else:\n",
    "            relevance_scores.append(0)\n",
    "    total_ndcg += ndcg_at_k(relevance_scores, k=10)\n",
    "    \n",
    "    if verbose and np.random.random() > 0.999:\n",
    "        user_films_train = train[train.userId == user_id].movieId.values\n",
    "        print('Идентификатор пользователя: ', user_id)\n",
    "        print(\n",
    "            'Фильмы в обучающей выборке для этого пользователя:',\n",
    "            [movies[movies.movieId == movie_id].title.values[0] for movie_id in user_films_train[:num_to_print]],\n",
    "            '\\n'\n",
    "        )\n",
    "        print(\n",
    "            'Просмотренные на самом деле фильмы: ', \n",
    "            [movies[movies.movieId == movie_id].title.values[0] for movie_id in ground_truth_films[:num_to_print]],\n",
    "            '\\n'\n",
    "        )\n",
    "        print(\n",
    "            'Рекомендации топ-рекомендера: ', \n",
    "            [movies[movies.movieId == rec_id].title.values[0] for rec_id in recommendations],\n",
    "            '\\n'\n",
    "        )\n",
    "        print('Значение NDCG@10 = ', ndcg_at_k(relevance_scores, k=10), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IM72WbqNl-jI",
    "outputId": "b2f8a16f-62da-422f-8cde-07addb895c55"
   },
   "outputs": [],
   "source": [
    "total_ndcg / validation.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWiyWQS_l-jL"
   },
   "source": [
    "### Обучим топ-рекоммендер на всем обучающем множестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdoCxplml-jM"
   },
   "outputs": [],
   "source": [
    "recommender = TopRecommender()\n",
    "recommender.fit(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dmxEuw7Xko6"
   },
   "source": [
    "Попробуем сделать предсказание для первого пользователя в тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iUt4fvM8l-jO",
    "outputId": "044c915e-558a-4528-a698-1e0df1109a32"
   },
   "outputs": [],
   "source": [
    "recommender.predict(user_id=test_user_id[0], n_recommendations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUQsDmJFl-jQ"
   },
   "source": [
    "### Создание файла с решением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUFIFl-dl-jR"
   },
   "source": [
    "Сформируем файл с решением, содержащим предсказания топ-рекоммендера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzZfmQXSl-jS",
    "outputId": "5d3a9405-5a9f-40b5-b0c7-5bad6a7129c9"
   },
   "outputs": [],
   "source": [
    "with open('submit.csv', 'w') as f:\n",
    "    f.write('userId,movieId\\n')\n",
    "    for user_id in test_user_id:\n",
    "        recommendations = recommender.predict(user_id=user_id, n_recommendations=10)\n",
    "        for rec in recommendations:\n",
    "            f.write(str(user_id) + ',' + str(int(rec)) + '\\n')\n",
    "    print('Отлично! Время загрузить файл submit.csv на kaggle!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmBkj2zCl-jV"
   },
   "source": [
    "### Наши советы: что можно пробовать в этом хакатоне\n",
    "\n",
    "1) Загрузите и отправьте на kaggle результат TopRecommender-а\n",
    "\n",
    "2) Имплементируйте SVD-рекоммендер (воспользовавшись примером из модуля)\n",
    "\n",
    "3) Попробуйте поиграть параметрами SVD-рекоммендера (например, количеством скрытых параметров)\n",
    "\n",
    "4) Подумайте, как скомбинировать вывод svd-рекоммендера с параметрами фильма\n",
    "\n",
    "5) Lightgbm поддерживает прямую оптимизацию по NDCG\n",
    "\n",
    "\n",
    "Не обязательно выполнять все эти пункты; также можно пробовать что-то другое, до чего еще никто не догадался.\n",
    "Очень много материалов про обучение рекомендательных систем можно найти в интернете. \n",
    "\n",
    "Успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разбор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала поймем, нужно ли нам решать проблему холодного старта. Для этого проверим, есть ли в списке контрольных пользователей пользователи, которых не было в наборе, выданном нам для обучения. \n",
    "\n",
    "Для этого создадим множества для каждого из наборов пользователей и проверим, есть ли пользователи в первом, которых нет во втором. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Множество пользоватей из контрольного набора: \n",
    "control_users_set = set(test_user_id)\n",
    "\n",
    "#Множество пользователей из выданного нам тренировочного набора:\n",
    "ratings_user_set = set(ratings.userId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_users_set - ratings_user_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, значит в контрольном множестве содержатся только те же пользователи, что и в тренировочном. Это означает, что нам не надо заниматься решением проблемы \"холодного старта\" — пытаться предсказывать что-либо для пользователей, которых не было в тренировочном сете. \n",
    "\n",
    "Что ж, давайте для начала удалим из валидационного набора пользователей, которые не встречались в тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Количество пользователей в валидационной выборке до фильтрации\", len(set(validation.userId)))\n",
    "\n",
    "train_users_set = set(train.userId)\n",
    "validation = validation[validation.userId.apply(lambda userId: userId in train_users_set)]\n",
    "print(\"Количество пользователей в валидационной выборке после фильтрации\", len(set(validation.userId)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства давайте обернем наши вызовы для рассчета NDCG для рекоммендера и для формирования сабмита в специальные функции get_ndcg и make_submit. Это пригодится нам, когда мы будем перебирать гиперпараметры алгоритмов. Обратите внимание, что в отличие от примера выше,  мы делим total_ndcg на количество пользователей, а не на общее количество примеров в тестовой выборке. Данные значения отличаются только в константное количество раз, и при переборе гиперпараметров это не важно, однако NDCG — это метрика набора рекомендаций, поэтому правильно усреднять именно по количеству таких наборов, которое равно количеству пользователей в тестовом сете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_users(dataset, fraction):\n",
    "    all_users = list(set(dataset.userId))\n",
    "    filtered_users = set(filter(lambda x: random.random() <= fraction, all_users))\n",
    "    return dataset[dataset.userId.apply (lambda x: x in filtered_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg(recommender, validation_fraction=1.0):\n",
    "    recommender.fit(train)\n",
    "    total_ndcg = 0\n",
    "    \n",
    "    sampled_validation = sample_users(validation, validation_fraction)\n",
    "    \n",
    "    validation_by_user = sampled_validation.groupby('userId')\n",
    "    for user_id, group in validation_by_user:\n",
    "        ground_truth_films = [int(data.movieId) for row, data in group.iterrows()]\n",
    "        recommendations = recommender.predict(user_id, n_recommendations=10)\n",
    "        relevance_scores = []\n",
    "        for rec in recommendations:\n",
    "            if rec in ground_truth_films:\n",
    "                relevance_scores.append(len(ground_truth_films) - ground_truth_films.index(rec))\n",
    "            else:\n",
    "                relevance_scores.append(0)\n",
    "        total_ndcg += ndcg_at_k(relevance_scores, k=10)\n",
    "    return total_ndcg/len(validation_by_user)\n",
    "\n",
    "\n",
    "def make_submit(recommender, filename):\n",
    "    recommender.fit(ratings)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('userId,movieId\\n')\n",
    "        for user_id in test_user_id:\n",
    "            recommendations = recommender.predict(user_id=user_id, n_recommendations=10)\n",
    "            for rec in recommendations:\n",
    "                f.write(str(user_id) + ',' + str(int(rec)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = TopRecommender()\n",
    "print(\"topRecommender NDCG: \", get_ndcg(recommender))\n",
    "make_submit(recommender, \"top_recommender.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На Kaggle TopRecommender набирает 0.02410 балла. Разница в показателях связана с тем, что мы отдали в \"тест\" много данных (25%), за это время \"вкусы\" пользователя могут поменяться и сдвинуться ближе к \"среднему\". Тем не менее, мы можем пользоваться данной оценкой для понимания \"направления\" правильности наших предсказаний. Теперь давайте попробуем улучшить качество предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала воспользуемся примером SVD-рекоммендера из лекций. Отличие будет заключаться в том, что в лекциях мы возвращали и фильм и его предсказанное значение релевантности; в данном случае будем возвращать только ID фильма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "class SVDRecommender(object):\n",
    "    def fit(self, data, n_components = 30):     \n",
    "        self.users = defaultdict(lambda: len(self.users))\n",
    "        self.movies = defaultdict(lambda: len(self.movies))\n",
    "        rows = data.userId.apply(lambda userId: self.users[userId])\n",
    "        cols = data.movieId.apply(lambda movieId: self.movies[movieId])\n",
    "        vals = [1.0]* len(cols)\n",
    "        self.interactions_matrix = csr_matrix((vals, (rows, cols)))\n",
    "        self.model = TruncatedSVD(n_components = n_components, algorithm='arpack')\n",
    "        self.model.fit(self.interactions_matrix)\n",
    "        self.movies_reverse = {}\n",
    "        for movie_id in self.movies:\n",
    "            movie_idx = self.movies[movie_id]\n",
    "            self.movies_reverse[movie_idx] = movie_id     \n",
    "\n",
    "        \n",
    "    def predict(self, user_id, n_recommendations=10):        \n",
    "        user_interactions = self.interactions_matrix.getrow(self.users[user_id])    \n",
    "        user_low_dimensions = self.model.transform(user_interactions)  \n",
    "        return self.predict_low_dimension(user_low_dimensions, user_interactions, n_recommendations)\n",
    "    \n",
    "    def predict_low_dimension(self, user_low_dimensions, user_interactions, max_n=10):\n",
    "        user_predictions = self.model.inverse_transform(user_low_dimensions)[0]\n",
    "        recommendations = []\n",
    "        \n",
    "        for movie_idx in reversed(np.argsort(user_predictions)):\n",
    "            #Добавляем фильм к рекомендациям только если пользователь его еще не смотрел\n",
    "            if user_interactions[0, movie_idx] == 0.0:\n",
    "                movie = self.movies_reverse[movie_idx]\n",
    "                score = user_predictions[movie_idx]\n",
    "                \n",
    "                #Эта строчка отличается из примера из лекций\n",
    "                recommendations.append(movie)\n",
    "                \n",
    "            if (len(recommendations) >= max_n):\n",
    "                return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ndcg(SVDRecommender())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ух ты, в 2 раза лучше! Давайте сделаем сабмит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit(SVDRecommender(), \"svd_recommender.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто SVD (скопированный из лекциии без изменений) набирает 0.0468 в kaggle. Этого достаточно, чтобы получить \"Зачет\". \n",
    "\n",
    "Давайте попробуем улучшить результат. Для этого воспользуемся поиском гиперпараметров для svd.\n",
    "Посмотрим на описание SVD из sklearn(https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) и увидим, что по сути мы можем влиять на 2 параметра: n_components и algorithm. \n",
    "\n",
    "Давайте для начала просто попробуем подобрать идеальное значение n_components. \n",
    "\n",
    "Для того, чтобы нам было удобнее выполнять перебор, вынесем параметры n_components и algorithm в конструктор объекта(функцию ```__init__```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDRecommender(object):\n",
    "    def __init__(self, n_components = 30, algorithm='arpack'):\n",
    "        self.n_components = n_components\n",
    "        self.algorithm = algorithm\n",
    "    \n",
    "    def fit(self, data):     \n",
    "        self.users = defaultdict(lambda: len(self.users))\n",
    "        self.movies = defaultdict(lambda: len(self.movies))\n",
    "        rows = data.userId.apply(lambda userId: self.users[userId])\n",
    "        cols = data.movieId.apply(lambda movieId: self.movies[movieId])\n",
    "        vals = [1.0]* len(cols)\n",
    "        self.interactions_matrix = csr_matrix((vals, (rows, cols)))\n",
    "        self.model = TruncatedSVD(n_components=self.n_components, algorithm=self.algorithm)\n",
    "        self.model.fit(self.interactions_matrix)\n",
    "        self.movies_reverse = {}\n",
    "        for movie_id in self.movies:\n",
    "            movie_idx = self.movies[movie_id]\n",
    "            self.movies_reverse[movie_idx] = movie_id     \n",
    "\n",
    "        \n",
    "    def predict(self, user_id, n_recommendations=10):        \n",
    "        user_interactions = self.interactions_matrix.getrow(self.users[user_id])    \n",
    "        user_low_dimensions = self.model.transform(user_interactions)  \n",
    "        return self.predict_low_dimension(user_low_dimensions, user_interactions, n_recommendations)\n",
    "    \n",
    "    def predict_low_dimension(self, user_low_dimensions, user_interactions, max_n=10):\n",
    "        user_predictions = self.model.inverse_transform(user_low_dimensions)[0]\n",
    "        recommendations = []\n",
    "        for movie_idx in reversed(np.argsort(user_predictions)):\n",
    "            if user_interactions[0, movie_idx] == 0.0:\n",
    "                movie = self.movies_reverse[movie_idx]\n",
    "                score = user_predictions[movie_idx]\n",
    "                recommendations.append(movie)\n",
    "            if (len(recommendations) >= max_n):\n",
    "                return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала пробежимся \"грубо\", проверяя каждый 10й элемент, чтобы примерно представить, где нужно искать максимум качества. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "y = []\n",
    "\n",
    "max_ndcg = 0.0\n",
    "iteration = 0\n",
    "for n_components in range(2, 200, 10):\n",
    "    iteration += 1\n",
    "    recommender = SVDRecommender(n_components=n_components)\n",
    "    ndcg = get_ndcg(recommender)\n",
    "    x.append(n_components)\n",
    "    y.append(ndcg)\n",
    "    \n",
    "    if ndcg > max_ndcg: max_ndcg = ndcg\n",
    "    print(\"iteration: {}, n_components: {}, NDCG: {}, max_ndcg:  {}\".format(iteration, n_components, ndcg, max_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что у параметра n_components есть 2 оптимума: в районе 22 и в районе 72 компонент. Давайте пробежимся в окрестностях каждого из них и попробуем найти максимумы. \n",
    "Попробуем отправить оба в kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit(SVDRecommender(n_components=22), \"svd22.txt\")\n",
    "make_submit(SVDRecommender(n_components=72), \"svd72.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD c 22 компонентами позволяет набрать 0.048 в kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте подумаем вот о чем: интересы пользователя со временем могут меняться. Поэтому разумно взять не всю историю пользователя, \n",
    "а только N последних его фильмов. Давайте посмотрим, что из этого выйдет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDRecommenderKeepN(object):\n",
    "    def __init__(self, n_components = 30, algorithm='arpack', keep_last_n=20):\n",
    "        self.n_components = n_components\n",
    "        self.algorithm = algorithm\n",
    "        self.keep_last_n = keep_last_n\n",
    "        self.user_seen = {}\n",
    "    \n",
    "    def fit(self, data):     \n",
    "        self.users = defaultdict(lambda: len(self.users))\n",
    "        self.movies = defaultdict(lambda: len(self.movies))\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for user_id, group in data.groupby('userId'):\n",
    "            all_user_movies = [self.movies[int(d.movieId)] for row, d in group.iterrows()]\n",
    "            self.user_seen[user_id] = set(all_user_movies)\n",
    "            user_keep_movies = all_user_movies[-self.keep_last_n:]\n",
    "            rows += [self.users[user_id]] * len(user_keep_movies)\n",
    "            cols += user_keep_movies\n",
    "        vals = [1.0]* len(cols)\n",
    "        self.interactions_matrix = csr_matrix((vals, (rows, cols)))\n",
    "        self.model = TruncatedSVD(n_components=self.n_components, algorithm=self.algorithm)\n",
    "        self.model.fit(self.interactions_matrix)\n",
    "        self.movies_reverse = {}\n",
    "        for movie_id in self.movies:\n",
    "            movie_idx = self.movies[movie_id]\n",
    "            self.movies_reverse[movie_idx] = movie_id     \n",
    "\n",
    "        \n",
    "    def predict(self, user_id, n_recommendations=10):        \n",
    "        user_interactions = self.interactions_matrix.getrow(self.users[user_id])    \n",
    "        user_low_dimensions = self.model.transform(user_interactions)  \n",
    "        return self.predict_low_dimension(user_low_dimensions, user_interactions, user_id, n_recommendations)\n",
    "    \n",
    "    def predict_low_dimension(self, user_low_dimensions, user_interactions, user_id, max_n=10):\n",
    "        user_predictions = self.model.inverse_transform(user_low_dimensions)[0]\n",
    "        recommendations = []\n",
    "        for movie_idx in reversed(np.argsort(user_predictions)):\n",
    "            if movie_idx in self.user_seen[user_id]:\n",
    "                continue\n",
    "            movie = self.movies_reverse[movie_idx]\n",
    "            score = user_predictions[movie_idx]\n",
    "            recommendations.append(movie)\n",
    "            if (len(recommendations) >= max_n):\n",
    "                return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = SVDRecommenderKeepN(n_components=22, keep_last_n=100)\n",
    "get_ndcg(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "y = []\n",
    "\n",
    "max_ndcg = 0.0\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "for keep_last_n in range(1, 25):\n",
    "    iteration += 1\n",
    "    recommender = SVDRecommenderKeepN(n_components=22, keep_last_n=keep_last_n)\n",
    "    ndcg = get_ndcg(recommender)\n",
    "    if ndcg > max_ndcg: max_ndcg = ndcg\n",
    "    x.append(keep_last_n)\n",
    "    y.append(ndcg)\n",
    "    print(\"iteration: {}, keep_last_n: {}, NDCG: {}, max_ndcg:  {}\".format(iteration, keep_last_n, ndcg, max_ndcg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень интересно! Видим, что очень неплохо работает предсказание всего лишь по одному последнему фильму, дальше происходит резкий провал, а потом качество возвращается на исходную позицию только в районе 20 фильмов. Скорее всего так получается из-за того, что пользователи часто смотрят сиквел фильма после первой части (например, \"Терминатор 2\" после \"Терминатор 1\"), и в этом случае последовательность оказывается очень важна. \n",
    "\n",
    "Давайте отправим в kaggle оба варианта и посмотрим на результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit(SVDRecommenderKeepN(n_components=22, keep_last_n=1), \"svd_keep_1.csv\")\n",
    "make_submit(SVDRecommenderKeepN(n_components=22, keep_last_n=19), \"svd_keep_19.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый из этих рекоммендеров по отдельности не позволяет нам получить результат лучше просто svd. Однако мы можем увидеть,\n",
    "что для рекомендаций важны как долгосрочные, так и краткосрочные действия пользователя. Давайте попробуем восопользоваться ML-ем, чтобы скомбинировать их оптимальным образом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class MLRecommender(object):\n",
    "    def __init__(self, target_movies = 3, \n",
    "                 svd_at=22,\n",
    "                 ranking_samples=150,\n",
    "                 keep_movies=[2, 8, 32, 128]):\n",
    "        self.target_movies = target_movies\n",
    "        self.keep_movies = keep_movies\n",
    "        self.svd_at=22\n",
    "        self.svd_algorithm = 'arpack'\n",
    "        self.ranking_samples = ranking_samples\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.svd_rank_cache = {}\n",
    "        self.users = defaultdict(lambda: int(len(self.users)))\n",
    "        self.movies = defaultdict(lambda: int(len(self.movies)))\n",
    "        \n",
    "        self.extract_target(data)\n",
    "        \n",
    "        self.movie_reverse = {}\n",
    "        for movie in self.movies:\n",
    "            self.movie_reverse[self.movies[movie]] = movie\n",
    "            \n",
    "        self.build_svds()\n",
    "        self.build_top()\n",
    "        self.build_ranking_dataset()\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        df = copy.deepcopy(self.ranking_dataset)\n",
    "        user_ids = df['user_id']\n",
    "        del(df['user_id'])\n",
    "        target = df['target']\n",
    "        del(df['target'])\n",
    "        \n",
    "        prev_id = None\n",
    "        group = []\n",
    "        cnt = 0\n",
    "        for uid in user_ids:\n",
    "            if prev_id is not None and uid != prev_id:\n",
    "                group.append(cnt)\n",
    "                cnt = 0 \n",
    "            cnt += 1 \n",
    "            prev_id = uid\n",
    "        group.append(cnt)\n",
    "        self.lgb = LGBMRanker()\n",
    "        self.lgb.fit(df, target, group=group)\n",
    "      \n",
    "    def build_ranking_dataset(self):\n",
    "        print(\"building dataset...\")\n",
    "        result = []\n",
    "        for i in tqdm_notebook(range(self.ranking_samples)):\n",
    "            while True:\n",
    "                user = random.randint(0, len(self.train_data) - 1)\n",
    "                if len(self.train_data[user]) != 0:\n",
    "                    break\n",
    "                    \n",
    "            movies_seq = self.train_data[user]\n",
    "            movies_seq_hash = self.get_seq_hash(movies_seq)\n",
    "            target_movies = self.target_data[user]\n",
    "            candidates_set = self.get_candidates(movies_seq, target_movies, svd_tops=0)\n",
    "            for candidate in candidates_set:\n",
    "                doc = self.get_features(movies_seq, movies_seq_hash, candidate)\n",
    "                doc[\"user_id\"] = user\n",
    "                doc[\"target\"] = candidate in target_movies\n",
    "                result.append(doc)\n",
    "            self.del_from_cache(movies_seq_hash)\n",
    "        self.ranking_dataset = pd.DataFrame(result)\n",
    "          \n",
    "    def del_from_cache(self, key):\n",
    "        for n in self.keep_movies:\n",
    "            del(self.svd_rank_cache[(key, n)])\n",
    "\n",
    "        \n",
    "    def get_seq_hash(self, seq):\n",
    "        return mmh3.hash(\",\".join([str(elem) for elem in seq]))\n",
    "    \n",
    "    def get_candidates(self, movies_seq, include_movies=[], random_samples=50, svd_tops=20):\n",
    "        result = []\n",
    "        if svd_tops > 0:\n",
    "            for n in self.keep_movies:\n",
    "                result += self.get_n_svd_rank(movies_seq, n)[:svd_tops]\n",
    "                \n",
    "        for i in range(random_samples):\n",
    "            result += [random.randint(0, len(self.movies) - 1)]\n",
    "        \n",
    "        result += include_movies          \n",
    "        return(set(result) - set(movies_seq))\n",
    "    \n",
    "    def get_n_svd_rank(self, movies_seq, n):\n",
    "        low_dim = self.get_low_dimension(movies_seq, n)\n",
    "        model = self.svds[n]\n",
    "        predictions = model.inverse_transform(low_dim)[0]\n",
    "        rank = list(reversed(np.argsort(predictions)))\n",
    "        seq_set = set(movies_seq)\n",
    "        rank = filter(lambda x: x not in seq_set, rank)\n",
    "        return list(rank)\n",
    "        \n",
    "    def build_top(self):\n",
    "        print(\"building tops..\")\n",
    "        movie_cnt = Counter()\n",
    "        cnt = 0\n",
    "        for user_id in self.train_data:\n",
    "            for movie in self.train_data[user_id]:\n",
    "                movie_cnt[movie] += 1\n",
    "                cnt += 1\n",
    "        self.top_list = []\n",
    "        self.movie_pop = {}\n",
    "        for (movie, count) in movie_cnt.most_common():\n",
    "            self.top_list.append(movie)\n",
    "            self.movie_pop[movie] = count / cnt\n",
    "        \n",
    "    def extract_target(self, data):\n",
    "        self.full_data = {}\n",
    "        self.train_data = {}\n",
    "        self.target_data = {}\n",
    "        print(\"extracting target...\")\n",
    "        for ext_user_id, group in data.groupby('userId'):\n",
    "            user_id = self.users[ext_user_id]\n",
    "            self.full_data[user_id] = [self.movies[int(d.movieId)] for row, d in group.iterrows()]\n",
    "            self.target_data[user_id] = self.full_data[user_id][-self.target_movies:]\n",
    "            self.train_data[user_id] = self.full_data[user_id][:-self.target_movies]  \n",
    "        \n",
    "    def build_svds(self):\n",
    "        print(\"building svds..\")\n",
    "        self.svds = {}\n",
    "        for n in self.keep_movies:\n",
    "            self.svds[n] = self.build_svd(n)\n",
    "            \n",
    "    def build_svd(self, n):\n",
    "        print(\"building svd using {} last movies for user\".format(n))  \n",
    "        rows = []\n",
    "        cols = []\n",
    "        \n",
    "        for user_id in self.train_data:\n",
    "            user_keep_movies = self.train_data[user_id][-n:]\n",
    "            cols += user_keep_movies\n",
    "            rows += [self.users[user_id]] * len(user_keep_movies)\n",
    "        vals = [1.0]* len(cols)\n",
    "        interactions_matrix = csr_matrix((vals, (rows, cols)), shape=(len(self.users), len(self.movies)))\n",
    "        model = TruncatedSVD(n_components=self.svd_at, algorithm=self.svd_algorithm, )\n",
    "        model.fit(interactions_matrix)\n",
    "        return model\n",
    "    \n",
    "    def get_features(self, movies_seq, movies_seq_hash, movie):\n",
    "        features = {}\n",
    "        for n in self.keep_movies:\n",
    "            features[\"svd_keep_{}\".format(n)] = self.svd_movie_rank(movies_seq, movies_seq_hash, movie, n)\n",
    "        movie_pop = self.movie_pop.get(movie, 0.0)\n",
    "        features[\"movie_pop\"] = movie_pop\n",
    "        features[\"seq_len\"] = len(movies_seq)\n",
    "        return features\n",
    "    \n",
    "    def svd_movie_rank(self, movies_seq, movies_seq_hash, movie, n):\n",
    "        if (movies_seq_hash, n) in self.svd_rank_cache:\n",
    "            return self.svd_rank_cache[(movies_seq_hash,n)][movie]\n",
    "        else:\n",
    "            low_dimension = self.get_low_dimension(movies_seq, n)\n",
    "            movies_seq_set = set(movies_seq)\n",
    "            model = self.svds[n]\n",
    "            predictions = model.inverse_transform(low_dimension)[0]\n",
    "            ranks = reversed(np.argsort(predictions))\n",
    "            ranks = filter(lambda x: x not in movies_seq_set, ranks)\n",
    "            result = {}\n",
    "            for movie_rank, movie in enumerate(ranks):\n",
    "                result[movie] = movie_rank\n",
    "            for movie in movies_seq_set:\n",
    "                result[movie] = -1\n",
    "            self.svd_rank_cache[(movies_seq_hash,n)] = result\n",
    "            return self.svd_rank_cache[(movies_seq_hash,n)][movie]\n",
    "            \n",
    "    def get_low_dimension(self, movies_seq, n):\n",
    "        n_seq = movies_seq[-n:]\n",
    "        row = np.zeros(len(self.movies))\n",
    "        model = self.svds[n]\n",
    "        for idx in n_seq:\n",
    "            row[idx] = 1.0\n",
    "        return model.transform([row])\n",
    "            \n",
    "    def get_n_score(self, low_dimension, movie, n):\n",
    "        model = self.svds[n]\n",
    "        predictions = model.inverse_transform(low_dimension)[0]\n",
    "        return predictions[movie]\n",
    "    \n",
    "    \n",
    "    def predict(self, user_id, n_recommendations=10):   \n",
    "        user = self.users[user_id]\n",
    "        seq = self.full_data[user]\n",
    "        seq_hash = self.get_seq_hash(seq)\n",
    "        candidates = list(self.get_candidates(seq))\n",
    "        docs = []\n",
    "        for candidate in candidates:\n",
    "            features=self.get_features(seq, seq_hash, candidate)\n",
    "            docs.append(features)\n",
    "        df = pd.DataFrame(docs, columns=self.ranking_dataset.columns)\n",
    "        del(df['user_id'])\n",
    "        del(df['target'])\n",
    "        predictions = self.lgb.predict(df)\n",
    "        rec_ids =  list(reversed(np.argsort(predictions)))[:n_recommendations]\n",
    "        result = []\n",
    "        for rec_id in rec_ids:\n",
    "            result.append(self.movie_reverse[candidates[rec_id]])\n",
    "        self.del_from_cache(seq_hash)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MLRecommender(ranking_samples=2000)\n",
    "get_ndcg(x, 0.1)\n",
    "make_submit(x, \"lgbm_svds_submit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим немного фичей, а именно — максимальную косинусную меру близости для N последних элементов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class MLRecommender(object):\n",
    "    def __init__(self, target_movies = 3, \n",
    "                 svd_at=22,\n",
    "                 ranking_samples=150,\n",
    "                 keep_movies=[2, 8, 32, 128], \n",
    "                 sims_at=[1, 2 , 8, 32, 128]):\n",
    "        self.target_movies = target_movies\n",
    "        self.keep_movies = keep_movies\n",
    "        self.svd_at=22\n",
    "        self.svd_algorithm = 'arpack'\n",
    "        self.sims_at = sims_at\n",
    "        self.ranking_samples = ranking_samples\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.svd_rank_cache = {}\n",
    "        self.users = defaultdict(lambda: int(len(self.users)))\n",
    "        self.movies = defaultdict(lambda: int(len(self.movies)))\n",
    "        \n",
    "        self.extract_target(data)\n",
    "        \n",
    "        self.movie_reverse = {}\n",
    "        for movie in self.movies:\n",
    "            self.movie_reverse[self.movies[movie]] = movie\n",
    "            \n",
    "        self.build_svds()\n",
    "        self.build_top()\n",
    "        self.build_movies_sims()\n",
    "        self.build_ranking_dataset()\n",
    "        self.build_model()\n",
    "        \n",
    "\n",
    "    \n",
    "    def build_model(self):\n",
    "        df = copy.deepcopy(self.ranking_dataset)\n",
    "        user_ids = df['user_id']\n",
    "        del(df['user_id'])\n",
    "        target = df['target']\n",
    "        del(df['target'])\n",
    "        \n",
    "        prev_id = None\n",
    "        group = []\n",
    "        cnt = 0\n",
    "        for uid in user_ids:\n",
    "            if prev_id is not None and uid != prev_id:\n",
    "                group.append(cnt)\n",
    "                cnt = 0 \n",
    "            cnt += 1 \n",
    "            prev_id = uid\n",
    "        group.append(cnt)\n",
    "        self.lgb = LGBMRanker()\n",
    "        self.lgb.fit(df, target, group=group)\n",
    "      \n",
    "    def build_ranking_dataset(self):\n",
    "        print(\"building dataset...\")\n",
    "        result = []\n",
    "        for i in tqdm_notebook(range(self.ranking_samples)):\n",
    "            while True:\n",
    "                user = random.randint(0, len(self.train_data) - 1)\n",
    "                if len(self.train_data[user]) != 0:\n",
    "                    break\n",
    "                    \n",
    "            movies_seq = self.train_data[user]\n",
    "            movies_seq_hash = self.get_seq_hash(movies_seq)\n",
    "            target_movies = self.target_data[user]\n",
    "            candidates_set = self.get_candidates(movies_seq, target_movies, svd_tops=0)\n",
    "            for candidate in candidates_set:\n",
    "                doc = self.get_features(movies_seq, movies_seq_hash, candidate)\n",
    "                doc[\"user_id\"] = user\n",
    "                doc[\"target\"] = candidate in target_movies\n",
    "                result.append(doc)\n",
    "            self.del_from_cache(movies_seq_hash)\n",
    "        self.ranking_dataset = pd.DataFrame(result)\n",
    "        \n",
    "    \n",
    "    def build_movies_sims(self):\n",
    "        print(\"building movie sims...\")\n",
    "        n_last_movies = 30\n",
    "        group_cnt = len(self.train_data)\n",
    "        movie_cnt = Counter()\n",
    "        pair_cnt = Counter()\n",
    "        for group in self.train_data.values():\n",
    "            for movie1 in set(group[-n_last_movies:]):\n",
    "                movie_cnt[movie1] += 1\n",
    "                for movie2 in set(group[-n_last_movies:]):\n",
    "                    if movie1 == movie2:\n",
    "                        continue\n",
    "                    pair_cnt[(movie1, movie2)] += 1\n",
    "        result = {}\n",
    "        for (movie1, movie2) in pair_cnt:\n",
    "            pair_prob = pair_cnt[(movie1, movie2)] / group_cnt\n",
    "            movie1_prob = movie_cnt[movie1] / group_cnt\n",
    "            movie2_prob = movie_cnt[movie2] / group_cnt\n",
    "            result[(movie1, movie2)] = pair_prob ** 2 / (movie1_prob * movie2_prob)\n",
    "        self.movies_sims = result\n",
    "    \n",
    "    def max_sim(self, movies_seq, movie, n_last_movies):\n",
    "        result = 0.0\n",
    "        for m in movies_seq[-n_last_movies:]:\n",
    "            result = max(result, self.movies_sims.get((m, movie), 0.0))\n",
    "        return result\n",
    "          \n",
    "    def del_from_cache(self, key):\n",
    "        for n in self.keep_movies:\n",
    "            del(self.svd_rank_cache[(key, n)])\n",
    "\n",
    "        \n",
    "    def get_seq_hash(self, seq):\n",
    "        return mmh3.hash(\",\".join([str(elem) for elem in seq]))\n",
    "    \n",
    "    def get_candidates(self, movies_seq, include_movies=[], random_samples=50, svd_tops=20):\n",
    "        result = []\n",
    "        if svd_tops > 0:\n",
    "            for n in self.keep_movies:\n",
    "                result += self.get_n_svd_rank(movies_seq, n)[:svd_tops]\n",
    "                \n",
    "        for i in range(random_samples):\n",
    "            result += [random.randint(0, len(self.movies) - 1)]\n",
    "        \n",
    "        result += include_movies          \n",
    "        return(set(result) - set(movies_seq))\n",
    "    \n",
    "    def get_n_svd_rank(self, movies_seq, n):\n",
    "        low_dim = self.get_low_dimension(movies_seq, n)\n",
    "        model = self.svds[n]\n",
    "        predictions = model.inverse_transform(low_dim)[0]\n",
    "        rank = list(reversed(np.argsort(predictions)))\n",
    "        seq_set = set(movies_seq)\n",
    "        rank = filter(lambda x: x not in seq_set, rank)\n",
    "        return list(rank)\n",
    "        \n",
    "    def build_top(self):\n",
    "        print(\"building tops..\")\n",
    "        movie_cnt = Counter()\n",
    "        cnt = 0\n",
    "        for user_id in self.train_data:\n",
    "            for movie in self.train_data[user_id]:\n",
    "                movie_cnt[movie] += 1\n",
    "                cnt += 1\n",
    "        self.top_list = []\n",
    "        self.movie_pop = {}\n",
    "        for (movie, count) in movie_cnt.most_common():\n",
    "            self.top_list.append(movie)\n",
    "            self.movie_pop[movie] = count / cnt\n",
    "        \n",
    "    def extract_target(self, data):\n",
    "        self.full_data = {}\n",
    "        self.train_data = {}\n",
    "        self.target_data = {}\n",
    "        print(\"extracting target...\")\n",
    "        for ext_user_id, group in data.groupby('userId'):\n",
    "            user_id = self.users[ext_user_id]\n",
    "            self.full_data[user_id] = [self.movies[int(d.movieId)] for row, d in group.iterrows()]\n",
    "            self.target_data[user_id] = self.full_data[user_id][-self.target_movies:]\n",
    "            self.train_data[user_id] = self.full_data[user_id][:-self.target_movies]  \n",
    "        \n",
    "    def build_svds(self):\n",
    "        print(\"building svds..\")\n",
    "        self.svds = {}\n",
    "        for n in self.keep_movies:\n",
    "            self.svds[n] = self.build_svd(n)\n",
    "            \n",
    "    def build_svd(self, n):\n",
    "        print(\"building svd using {} last movies for user\".format(n))  \n",
    "        rows = []\n",
    "        cols = []\n",
    "        \n",
    "        for user_id in self.train_data:\n",
    "            user_keep_movies = self.train_data[user_id][-n:]\n",
    "            cols += user_keep_movies\n",
    "            rows += [self.users[user_id]] * len(user_keep_movies)\n",
    "        vals = [1.0]* len(cols)\n",
    "        interactions_matrix = csr_matrix((vals, (rows, cols)), shape=(len(self.users), len(self.movies)))\n",
    "        model = TruncatedSVD(n_components=self.svd_at, algorithm=self.svd_algorithm, )\n",
    "        model.fit(interactions_matrix)\n",
    "        return model\n",
    "    \n",
    "    def get_features(self, movies_seq, movies_seq_hash, movie):\n",
    "        features = {}\n",
    "        for n in self.keep_movies:\n",
    "            features[\"svd_keep_{}\".format(n)] = self.svd_movie_rank(movies_seq, movies_seq_hash, movie, n)\n",
    "        \n",
    "        for n in self.sims_at:\n",
    "            features[\"cos_sim_{}\".format(n)] = self.max_sim(movies_seq, movie, n)\n",
    "        movie_pop = self.movie_pop.get(movie, 0.0)\n",
    "        features[\"movie_pop\"] = movie_pop\n",
    "        features[\"seq_len\"] = len(movies_seq)\n",
    "        return features\n",
    "    \n",
    "    def svd_movie_rank(self, movies_seq, movies_seq_hash, movie, n):\n",
    "        if (movies_seq_hash, n) in self.svd_rank_cache:\n",
    "            return self.svd_rank_cache[(movies_seq_hash,n)][movie]\n",
    "        else:\n",
    "            low_dimension = self.get_low_dimension(movies_seq, n)\n",
    "            movies_seq_set = set(movies_seq)\n",
    "            model = self.svds[n]\n",
    "            predictions = model.inverse_transform(low_dimension)[0]\n",
    "            ranks = reversed(np.argsort(predictions))\n",
    "            ranks = filter(lambda x: x not in movies_seq_set, ranks)\n",
    "            result = {}\n",
    "            for movie_rank, movie in enumerate(ranks):\n",
    "                result[movie] = movie_rank\n",
    "            for movie in movies_seq_set:\n",
    "                result[movie] = -1\n",
    "            self.svd_rank_cache[(movies_seq_hash,n)] = result\n",
    "            return self.svd_rank_cache[(movies_seq_hash,n)][movie]\n",
    "            \n",
    "    def get_low_dimension(self, movies_seq, n):\n",
    "        n_seq = movies_seq[-n:]\n",
    "        row = np.zeros(len(self.movies))\n",
    "        model = self.svds[n]\n",
    "        for idx in n_seq:\n",
    "            row[idx] = 1.0\n",
    "        return model.transform([row])\n",
    "            \n",
    "    def get_n_score(self, low_dimension, movie, n):\n",
    "        model = self.svds[n]\n",
    "        predictions = model.inverse_transform(low_dimension)[0]\n",
    "        return predictions[movie]\n",
    "    \n",
    "    \n",
    "    def predict(self, user_id, n_recommendations=10):   \n",
    "        user = self.users[user_id]\n",
    "        seq = self.full_data[user]\n",
    "        seq_hash = self.get_seq_hash(seq)\n",
    "        candidates = list(self.get_candidates(seq))\n",
    "        docs = []\n",
    "        for candidate in candidates:\n",
    "            features=self.get_features(seq, seq_hash, candidate)\n",
    "            docs.append(features)\n",
    "        df = pd.DataFrame(docs, columns=self.ranking_dataset.columns)\n",
    "        del(df['user_id'])\n",
    "        del(df['target'])\n",
    "        predictions = self.lgb.predict(df)\n",
    "        rec_ids =  list(reversed(np.argsort(predictions)))[:n_recommendations]\n",
    "        result = []\n",
    "        for rec_id in rec_ids:\n",
    "            result.append(self.movie_reverse[candidates[rec_id]])\n",
    "        self.del_from_cache(seq_hash)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MLRecommender(ranking_samples=2000)\n",
    "get_ndcg(x, 0.1)\n",
    "make_submit(x, \"lgbm_svds_sims_submit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход набирает 0.06856 на kaggle, но наверняка можно набрать и больше!\n",
    "\n",
    "Можно выполнить поиск гиперпараметров модели Lightgbm, поиграться со значениями N, которые мы сохраняем. \n",
    "\n",
    "Также можно добавить дополнительных фичей в наш ML-рекоммендер. Фичи теперь ограничены только вашей фантазией, например: \n",
    "\n",
    "* Кратковременная популярность фильма\n",
    "* Средняя косинусная близость\n",
    "* Близость фильмов по тэгам\n",
    "* Близость фильмов по названиям\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "RecommenderHackathon.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
